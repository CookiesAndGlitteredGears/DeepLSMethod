{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following equation\n",
    "$$- u''  = f, \\quad x\\in \\Omega$$\n",
    "$$u = 0, \\quad x \\in \\partial \\Omega.$$\n",
    "We choose the true solution $u = x(e^{-(x-\\frac{1}{3})^2/K} - e^{-\\frac{4}{9}/K})$ with $K = 0.01$ on the interval $\\Omega = [-1,1]$.\n",
    "\n",
    "By FOSLS method, we have \n",
    "$$\\sigma = - u'.$$\n",
    "And the loss function\n",
    "$$G(\\sigma, u; f) = \\|\\text{div }\\sigma + u -f \\|_{0,\\Omega}^2 + \\|\\frac{1}{\\varepsilon}\\sigma + \\varepsilon \\nabla u\\|_{0,\\Omega}^2 + \\beta \\|u\\|^2_{0,\\Omega}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def TicTocGenerator():\n",
    "    # Generator that returns time differences\n",
    "    ti = 0           # initial time\n",
    "    tf = time.time() # final time\n",
    "    while True:\n",
    "        ti = tf\n",
    "        tf = time.time()\n",
    "        yield tf-ti # returns the time difference\n",
    "\n",
    "TicToc = TicTocGenerator() # create an instance of the TicTocGen generator\n",
    "\n",
    "# This will be the main function through which we define both tic() and toc()\n",
    "def toc(tempBool=True):\n",
    "    # Prints the time difference yielded by generator instance TicToc\n",
    "    tempTimeInterval = next(TicToc)\n",
    "    if tempBool:\n",
    "        print( \"Elapsed time: %f seconds.\\n\" %tempTimeInterval )\n",
    "\n",
    "def tic():\n",
    "    # Records a time in TicToc, marks the beginning of a time interval\n",
    "    toc(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "global k, dx, beta \n",
    "k, dx, beta = .01, .002, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -(2*exp(-(x - 1/3)**2/k)*(6*k + 2*x - 27*k*x - 12*x**2 + 18*x**3))/(9*k**2)\n",
    "  \n",
    "def g(x):\n",
    "    return torch.tensor([0.], requires_grad=True)\n",
    "\n",
    "def u_exact(x):\n",
    "    return x*(exp(-1/k*(x-1/3)**2) - exp(-4/(9*k)))\n",
    "\n",
    "def sigma_exact(x):\n",
    "    return exp(-4/(9*k)) - exp(-(x - 1/3)**2/k) + (x*exp(-(x - 1/3)**2/k)*(2*x - 2/3))/k\n",
    "  \n",
    "sq = lambda x: x ** 2\n",
    "vsq = np.vectorize(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: H1 norm square: 1.500809, L2 norm square: 0.014239 \n",
      "sigma: H1 norm square: 466.257229, L2 norm square: 1.486570 \n"
     ]
    }
   ],
   "source": [
    "# compute H1 norm of true u and sigma\n",
    "L = 0.\n",
    "R = 1.\n",
    "test_set =  np.arange(L, R+dx, dx)\n",
    "u = np.vectorize(u_exact)(test_set)\n",
    "ud = -np.vectorize(sigma_exact)(test_set)\n",
    "u_h1 = np.sum(dx*(vsq(u) + vsq(ud)))\n",
    "u_l2 = np.sum(dx*vsq(u))\n",
    "\n",
    "sigma = np.vectorize(sigma_exact)(test_set)\n",
    "sigmad = np.vectorize(f)(test_set)\n",
    "sigma_h1 = np.sum(dx*(vsq(sigma) + vsq(sigmad)))\n",
    "sigma_l2 = np.sum(dx*vsq(sigma))\n",
    "\n",
    "print('u: H1 norm square: %.6f, L2 norm square: %.6f ' %(u_h1, u_l2))\n",
    "print('sigma: H1 norm square: %.6f, L2 norm square: %.6f ' %(sigma_h1, sigma_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MuSigmaPde(nn.Module):\n",
    "    def __init__(self, dimension, mesh = 24, neuron = 14):\n",
    "        super(MuSigmaPde, self).__init__()\n",
    "\n",
    "        self.xdim = dimension\n",
    "        # Layer 1\n",
    "        self.fc1mu = nn.Linear(dimension, mesh)\n",
    "        self.fc1sig = nn.Linear(dimension, mesh)\n",
    "        # Layer 2\n",
    "        self.fc2mu = nn.Linear(mesh, neuron)\n",
    "        self.fc2sig = nn.Linear(mesh, neuron)\n",
    "        # Layer 3\n",
    "        self.fc3mu = nn.Linear(neuron, neuron)\n",
    "        self.fc3sig = nn.Linear(neuron, neuron)\n",
    "        # Layer 4\n",
    "        self.fc4mu = nn.Linear(neuron, 1)\n",
    "        self.fc4sig = nn.Linear(neuron, dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert(len(x.shape) == 1 and x.shape[0] == self.xdim)\n",
    "        mu =  F.leaky_relu(self.fc2mu(F.leaky_relu(self.fc1mu(x))))\n",
    "        sig =  F.leaky_relu(self.fc2sig(F.leaky_relu(self.fc1sig(x))))\n",
    "        mu =  self.fc4mu(F.leaky_relu(self.fc3mu(mu)) + mu)\n",
    "        sig = self.fc4sig(F.leaky_relu(self.fc3sig(sig)) + sig)\n",
    "        return mu, sig\n",
    "      \n",
    "    def net_grad(self, x):\n",
    "        mu_center, sigma_center = self.forward(x)\n",
    "        mu_forward, sigma_forward = self.forward(x - dx)\n",
    "#         mu_backward, sigma_backward = self.forward(x + h)\n",
    "\n",
    "        mu_grad_forward = (mu_center - mu_forward)/(dx)\n",
    "        sigma_grad_forward = (sigma_center - sigma_forward)/(dx)\n",
    "    \n",
    "        return mu_grad_forward, sigma_grad_forward\n",
    "    \n",
    "    def loss_function_bulk(self, x):\n",
    "        mu, sigma = self.forward(x)\n",
    "        mu_grad, sigma_grad = self.net_grad(x)\n",
    "        LSE = torch.sum((mu_grad + sigma)**2) + (sigma_grad - f(x))**2\n",
    "        return LSE \n",
    "\n",
    "    def loss_function_surf(self, x):\n",
    "        mu, sigma = self.forward(x)\n",
    "        # Boundary condition penalty\n",
    "        BCP = beta * (mu - g(x))**2\n",
    "        return BCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MuSigmaPde(dimension =1, mesh = 24, neuron = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk points number 200 \n",
      "surface points number 2\n",
      "test points number 501\n",
      "dx for difference 0.002\n",
      "trainging iteration 1000\n"
     ]
    }
   ],
   "source": [
    "h = .005\n",
    "epochs = 1000\n",
    "L, R = 0., 1.\n",
    "bulk_set, surf_set =  np.arange(L, R, h), [L, R]\n",
    "loss_bulk_record, loss_surf_record = [], []\n",
    "print('bulk points number %d \\nsurface points number %d\\ntest points number %d\\ndx for difference %.3g\\ntrainging iteration %d' %(np.size(bulk_set), np.size(surf_set), np.size(test_set), dx, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Loss: 3.402245, loss bulk: 3.402020, loss surf: 0.000225\n",
      "Train Epoch: 1, Loss: 10.644883, loss bulk: 10.257454, loss surf: 0.387429\n",
      "Train Epoch: 2, Loss: 6.790787, loss bulk: 6.478257, loss surf: 0.312530\n",
      "Train Epoch: 3, Loss: 9.707782, loss bulk: 9.599524, loss surf: 0.108258\n",
      "Train Epoch: 4, Loss: 7.218844, loss bulk: 7.172197, loss surf: 0.046647\n",
      "Train Epoch: 5, Loss: 7.240502, loss bulk: 7.038644, loss surf: 0.201857\n",
      "Train Epoch: 6, Loss: 10.051948, loss bulk: 9.814232, loss surf: 0.237716\n",
      "Train Epoch: 7, Loss: 7.627132, loss bulk: 7.488103, loss surf: 0.139029\n",
      "Train Epoch: 8, Loss: 4.344244, loss bulk: 4.302250, loss surf: 0.041994\n",
      "Train Epoch: 9, Loss: 5.383499, loss bulk: 5.362757, loss surf: 0.020743\n",
      "Train Epoch: 10, Loss: 7.587119, loss bulk: 7.542858, loss surf: 0.044261\n",
      "Train Epoch: 11, Loss: 8.646218, loss bulk: 8.578008, loss surf: 0.068210\n",
      "Train Epoch: 12, Loss: 7.717729, loss bulk: 7.633121, loss surf: 0.084608\n",
      "Train Epoch: 13, Loss: 5.287778, loss bulk: 5.199959, loss surf: 0.087819\n",
      "Train Epoch: 14, Loss: 4.752848, loss bulk: 4.689108, loss surf: 0.063740\n",
      "Train Epoch: 15, Loss: 6.408365, loss bulk: 6.384749, loss surf: 0.023616\n",
      "Train Epoch: 16, Loss: 9.224874, loss bulk: 9.224273, loss surf: 0.000601\n",
      "Train Epoch: 17, Loss: 8.242485, loss bulk: 8.229891, loss surf: 0.012594\n",
      "Train Epoch: 18, Loss: 6.581563, loss bulk: 6.540834, loss surf: 0.040729\n",
      "Train Epoch: 19, Loss: 4.765687, loss bulk: 4.710701, loss surf: 0.054986\n",
      "Train Epoch: 20, Loss: 4.950707, loss bulk: 4.903552, loss surf: 0.047155\n",
      "Train Epoch: 21, Loss: 6.704211, loss bulk: 6.675927, loss surf: 0.028285\n",
      "Train Epoch: 22, Loss: 8.679903, loss bulk: 8.669395, loss surf: 0.010508\n",
      "Train Epoch: 23, Loss: 6.561365, loss bulk: 6.558752, loss surf: 0.002613\n",
      "Train Epoch: 24, Loss: 5.312483, loss bulk: 5.303118, loss surf: 0.009366\n",
      "Train Epoch: 25, Loss: 4.350953, loss bulk: 4.326266, loss surf: 0.024686\n",
      "Train Epoch: 26, Loss: 4.841126, loss bulk: 4.807817, loss surf: 0.033310\n",
      "Train Epoch: 27, Loss: 5.591608, loss bulk: 5.565152, loss surf: 0.026456\n",
      "Train Epoch: 28, Loss: 5.845280, loss bulk: 5.833684, loss surf: 0.011596\n",
      "Train Epoch: 29, Loss: 5.527236, loss bulk: 5.525170, loss surf: 0.002066\n",
      "Train Epoch: 30, Loss: 5.127287, loss bulk: 5.124511, loss surf: 0.002777\n",
      "Train Epoch: 31, Loss: 4.296476, loss bulk: 4.286755, loss surf: 0.009721\n",
      "Train Epoch: 32, Loss: 4.441023, loss bulk: 4.424906, loss surf: 0.016116\n",
      "Train Epoch: 33, Loss: 4.651076, loss bulk: 4.633913, loss surf: 0.017163\n",
      "Train Epoch: 34, Loss: 5.463037, loss bulk: 5.451202, loss surf: 0.011835\n",
      "Train Epoch: 35, Loss: 4.488145, loss bulk: 4.484257, loss surf: 0.003888\n",
      "Train Epoch: 36, Loss: 4.335346, loss bulk: 4.335318, loss surf: 0.000028\n",
      "Train Epoch: 37, Loss: 4.165969, loss bulk: 4.162941, loss surf: 0.003028\n",
      "Train Epoch: 38, Loss: 4.268545, loss bulk: 4.260361, loss surf: 0.008184\n",
      "Train Epoch: 39, Loss: 4.682958, loss bulk: 4.673619, loss surf: 0.009338\n",
      "Train Epoch: 40, Loss: 4.268817, loss bulk: 4.262208, loss surf: 0.006609\n",
      "Train Epoch: 41, Loss: 4.111587, loss bulk: 4.108306, loss surf: 0.003281\n",
      "Train Epoch: 42, Loss: 3.951958, loss bulk: 3.950349, loss surf: 0.001609\n",
      "Train Epoch: 43, Loss: 4.182661, loss bulk: 4.181475, loss surf: 0.001187\n",
      "Train Epoch: 44, Loss: 4.247269, loss bulk: 4.246027, loss surf: 0.001242\n",
      "Train Epoch: 45, Loss: 4.030818, loss bulk: 4.029059, loss surf: 0.001758\n",
      "Train Epoch: 46, Loss: 4.151468, loss bulk: 4.149243, loss surf: 0.002225\n",
      "Train Epoch: 47, Loss: 4.093779, loss bulk: 4.092212, loss surf: 0.001567\n",
      "Train Epoch: 48, Loss: 4.117150, loss bulk: 4.116918, loss surf: 0.000232\n",
      "Train Epoch: 49, Loss: 4.162241, loss bulk: 4.162030, loss surf: 0.000211\n",
      "Train Epoch: 50, Loss: 4.153249, loss bulk: 4.152134, loss surf: 0.001115\n",
      "Train Epoch: 51, Loss: 4.090702, loss bulk: 4.089578, loss surf: 0.001123\n",
      "Train Epoch: 52, Loss: 4.055038, loss bulk: 4.054440, loss surf: 0.000598\n",
      "Train Epoch: 53, Loss: 4.057670, loss bulk: 4.057083, loss surf: 0.000587\n",
      "Train Epoch: 54, Loss: 4.141818, loss bulk: 4.141155, loss surf: 0.000663\n",
      "Train Epoch: 55, Loss: 4.043892, loss bulk: 4.043635, loss surf: 0.000257\n",
      "Train Epoch: 56, Loss: 4.022595, loss bulk: 4.022594, loss surf: 0.000001\n",
      "Train Epoch: 57, Loss: 3.950277, loss bulk: 3.950069, loss surf: 0.000208\n",
      "Train Epoch: 58, Loss: 3.523744, loss bulk: 3.523460, loss surf: 0.000285\n",
      "Train Epoch: 59, Loss: 3.918598, loss bulk: 3.918404, loss surf: 0.000194\n",
      "Train Epoch: 60, Loss: 3.496250, loss bulk: 3.495883, loss surf: 0.000367\n",
      "Train Epoch: 61, Loss: 3.625090, loss bulk: 3.624621, loss surf: 0.000469\n",
      "Train Epoch: 62, Loss: 3.731184, loss bulk: 3.731017, loss surf: 0.000167\n",
      "Train Epoch: 63, Loss: 3.563602, loss bulk: 3.563583, loss surf: 0.000018\n",
      "Train Epoch: 64, Loss: 3.454378, loss bulk: 3.454030, loss surf: 0.000348\n",
      "Train Epoch: 65, Loss: 3.581422, loss bulk: 3.580820, loss surf: 0.000602\n",
      "Train Epoch: 66, Loss: 3.397475, loss bulk: 3.397055, loss surf: 0.000420\n",
      "Train Epoch: 67, Loss: 3.528918, loss bulk: 3.528780, loss surf: 0.000138\n",
      "Train Epoch: 68, Loss: 3.342949, loss bulk: 3.342904, loss surf: 0.000045\n",
      "Train Epoch: 69, Loss: 3.365993, loss bulk: 3.365906, loss surf: 0.000087\n",
      "Train Epoch: 70, Loss: 3.264885, loss bulk: 3.264741, loss surf: 0.000144\n",
      "Train Epoch: 71, Loss: 3.300151, loss bulk: 3.300047, loss surf: 0.000104\n",
      "Train Epoch: 72, Loss: 3.334833, loss bulk: 3.334817, loss surf: 0.000016\n",
      "Train Epoch: 73, Loss: 3.246220, loss bulk: 3.246209, loss surf: 0.000011\n",
      "Train Epoch: 74, Loss: 3.277732, loss bulk: 3.277663, loss surf: 0.000069\n",
      "Train Epoch: 75, Loss: 3.285603, loss bulk: 3.285512, loss surf: 0.000090\n",
      "Train Epoch: 76, Loss: 3.238130, loss bulk: 3.238086, loss surf: 0.000045\n",
      "Train Epoch: 77, Loss: 3.248544, loss bulk: 3.248539, loss surf: 0.000005\n",
      "Train Epoch: 78, Loss: 3.247521, loss bulk: 3.247510, loss surf: 0.000011\n",
      "Train Epoch: 79, Loss: 3.224860, loss bulk: 3.224831, loss surf: 0.000029\n",
      "Train Epoch: 80, Loss: 3.222517, loss bulk: 3.222497, loss surf: 0.000020\n",
      "Train Epoch: 81, Loss: 3.218159, loss bulk: 3.218136, loss surf: 0.000023\n",
      "Train Epoch: 82, Loss: 3.210587, loss bulk: 3.210544, loss surf: 0.000043\n",
      "Train Epoch: 83, Loss: 3.218167, loss bulk: 3.218142, loss surf: 0.000024\n",
      "Train Epoch: 84, Loss: 3.255806, loss bulk: 3.255804, loss surf: 0.000002\n",
      "Train Epoch: 85, Loss: 3.219615, loss bulk: 3.219582, loss surf: 0.000032\n",
      "Train Epoch: 86, Loss: 3.182079, loss bulk: 3.182017, loss surf: 0.000063\n",
      "Train Epoch: 87, Loss: 3.176194, loss bulk: 3.176144, loss surf: 0.000049\n",
      "Train Epoch: 88, Loss: 3.175627, loss bulk: 3.175592, loss surf: 0.000035\n",
      "Train Epoch: 89, Loss: 3.188998, loss bulk: 3.188974, loss surf: 0.000024\n",
      "Train Epoch: 90, Loss: 3.172388, loss bulk: 3.172382, loss surf: 0.000006\n",
      "Train Epoch: 91, Loss: 3.148664, loss bulk: 3.148646, loss surf: 0.000018\n",
      "Train Epoch: 92, Loss: 3.156794, loss bulk: 3.156752, loss surf: 0.000042\n",
      "Train Epoch: 93, Loss: 3.165301, loss bulk: 3.165267, loss surf: 0.000034\n",
      "Train Epoch: 94, Loss: 3.166900, loss bulk: 3.166891, loss surf: 0.000009\n",
      "Train Epoch: 95, Loss: 3.155589, loss bulk: 3.155587, loss surf: 0.000002\n",
      "Train Epoch: 96, Loss: 3.136622, loss bulk: 3.136616, loss surf: 0.000006\n",
      "Train Epoch: 97, Loss: 3.138058, loss bulk: 3.138051, loss surf: 0.000007\n",
      "Train Epoch: 98, Loss: 3.147955, loss bulk: 3.147952, loss surf: 0.000003\n",
      "Train Epoch: 99, Loss: 3.145684, loss bulk: 3.145683, loss surf: 0.000001\n",
      "Train Epoch: 100, Loss: 3.141654, loss bulk: 3.141649, loss surf: 0.000004\n",
      "Train Epoch: 101, Loss: 3.134830, loss bulk: 3.134823, loss surf: 0.000007\n",
      "Train Epoch: 102, Loss: 3.127467, loss bulk: 3.127462, loss surf: 0.000005\n",
      "Train Epoch: 103, Loss: 3.129730, loss bulk: 3.129728, loss surf: 0.000003\n",
      "Train Epoch: 104, Loss: 3.131451, loss bulk: 3.131450, loss surf: 0.000001\n",
      "Train Epoch: 105, Loss: 3.133169, loss bulk: 3.133168, loss surf: 0.000000\n",
      "Train Epoch: 106, Loss: 3.132384, loss bulk: 3.132383, loss surf: 0.000001\n",
      "Train Epoch: 107, Loss: 3.126392, loss bulk: 3.126391, loss surf: 0.000000\n",
      "Train Epoch: 108, Loss: 3.123454, loss bulk: 3.123454, loss surf: 0.000000\n",
      "Train Epoch: 109, Loss: 3.119637, loss bulk: 3.119636, loss surf: 0.000001\n",
      "Train Epoch: 110, Loss: 3.117705, loss bulk: 3.117703, loss surf: 0.000002\n",
      "Train Epoch: 111, Loss: 3.119301, loss bulk: 3.119297, loss surf: 0.000004\n",
      "Train Epoch: 112, Loss: 3.119031, loss bulk: 3.119026, loss surf: 0.000005\n",
      "Train Epoch: 113, Loss: 3.119857, loss bulk: 3.119853, loss surf: 0.000003\n",
      "Train Epoch: 114, Loss: 3.121701, loss bulk: 3.121699, loss surf: 0.000002\n",
      "Train Epoch: 115, Loss: 3.121638, loss bulk: 3.121637, loss surf: 0.000001\n",
      "Train Epoch: 116, Loss: 3.122332, loss bulk: 3.122331, loss surf: 0.000000\n",
      "Train Epoch: 117, Loss: 3.123163, loss bulk: 3.123162, loss surf: 0.000001\n",
      "Train Epoch: 118, Loss: 3.124062, loss bulk: 3.124061, loss surf: 0.000000\n",
      "Train Epoch: 119, Loss: 3.126523, loss bulk: 3.126523, loss surf: 0.000000\n",
      "Train Epoch: 120, Loss: 3.129440, loss bulk: 3.129438, loss surf: 0.000002\n",
      "Train Epoch: 121, Loss: 3.124543, loss bulk: 3.124541, loss surf: 0.000002\n",
      "Train Epoch: 122, Loss: 3.119477, loss bulk: 3.119475, loss surf: 0.000002\n",
      "Train Epoch: 123, Loss: 3.118300, loss bulk: 3.118299, loss surf: 0.000001\n",
      "Train Epoch: 124, Loss: 3.115946, loss bulk: 3.115946, loss surf: 0.000000\n",
      "Train Epoch: 125, Loss: 3.120236, loss bulk: 3.120235, loss surf: 0.000000\n",
      "Train Epoch: 126, Loss: 3.125905, loss bulk: 3.125905, loss surf: 0.000000\n",
      "Train Epoch: 127, Loss: 3.156869, loss bulk: 3.156869, loss surf: 0.000000\n",
      "Train Epoch: 128, Loss: 3.222339, loss bulk: 3.222339, loss surf: 0.000000\n",
      "Train Epoch: 129, Loss: 3.406564, loss bulk: 3.406564, loss surf: 0.000000\n",
      "Train Epoch: 130, Loss: 3.216495, loss bulk: 3.216494, loss surf: 0.000001\n",
      "Train Epoch: 131, Loss: 3.121053, loss bulk: 3.121052, loss surf: 0.000001\n",
      "Train Epoch: 132, Loss: 3.146966, loss bulk: 3.146965, loss surf: 0.000000\n",
      "Train Epoch: 133, Loss: 3.205084, loss bulk: 3.205084, loss surf: 0.000000\n",
      "Train Epoch: 134, Loss: 3.454745, loss bulk: 3.454745, loss surf: 0.000000\n",
      "Train Epoch: 135, Loss: 3.287154, loss bulk: 3.287154, loss surf: 0.000000\n",
      "Train Epoch: 136, Loss: 3.128434, loss bulk: 3.128433, loss surf: 0.000000\n",
      "Train Epoch: 137, Loss: 3.322165, loss bulk: 3.322165, loss surf: 0.000000\n",
      "Train Epoch: 138, Loss: 3.556981, loss bulk: 3.556981, loss surf: 0.000000\n",
      "Train Epoch: 139, Loss: 3.541392, loss bulk: 3.541392, loss surf: 0.000000\n",
      "Train Epoch: 140, Loss: 3.238256, loss bulk: 3.238256, loss surf: 0.000000\n",
      "Train Epoch: 141, Loss: 3.471185, loss bulk: 3.471184, loss surf: 0.000001\n",
      "Train Epoch: 142, Loss: 3.538930, loss bulk: 3.538929, loss surf: 0.000000\n",
      "Train Epoch: 143, Loss: 3.519098, loss bulk: 3.519098, loss surf: 0.000001\n",
      "Train Epoch: 144, Loss: 3.237657, loss bulk: 3.237654, loss surf: 0.000003\n",
      "Train Epoch: 145, Loss: 3.422219, loss bulk: 3.422214, loss surf: 0.000005\n",
      "Train Epoch: 146, Loss: 3.293288, loss bulk: 3.293286, loss surf: 0.000002\n",
      "Train Epoch: 147, Loss: 3.276804, loss bulk: 3.276804, loss surf: 0.000000\n",
      "Train Epoch: 148, Loss: 3.376380, loss bulk: 3.376380, loss surf: 0.000001\n",
      "Train Epoch: 149, Loss: 3.252594, loss bulk: 3.252593, loss surf: 0.000001\n",
      "Train Epoch: 150, Loss: 3.337585, loss bulk: 3.337584, loss surf: 0.000001\n",
      "Train Epoch: 151, Loss: 3.278220, loss bulk: 3.278219, loss surf: 0.000002\n",
      "Train Epoch: 152, Loss: 3.276773, loss bulk: 3.276770, loss surf: 0.000003\n",
      "Train Epoch: 153, Loss: 3.324849, loss bulk: 3.324845, loss surf: 0.000004\n",
      "Train Epoch: 154, Loss: 3.246341, loss bulk: 3.246336, loss surf: 0.000005\n",
      "Train Epoch: 155, Loss: 3.301760, loss bulk: 3.301759, loss surf: 0.000001\n",
      "Train Epoch: 156, Loss: 3.255364, loss bulk: 3.255362, loss surf: 0.000001\n",
      "Train Epoch: 157, Loss: 3.253545, loss bulk: 3.253544, loss surf: 0.000001\n",
      "Train Epoch: 158, Loss: 3.274359, loss bulk: 3.274359, loss surf: 0.000000\n",
      "Train Epoch: 159, Loss: 3.225678, loss bulk: 3.225677, loss surf: 0.000001\n",
      "Train Epoch: 160, Loss: 3.270577, loss bulk: 3.270576, loss surf: 0.000001\n",
      "Train Epoch: 161, Loss: 3.218698, loss bulk: 3.218695, loss surf: 0.000003\n",
      "Train Epoch: 162, Loss: 3.247016, loss bulk: 3.247013, loss surf: 0.000003\n",
      "Train Epoch: 163, Loss: 3.228202, loss bulk: 3.228201, loss surf: 0.000001\n",
      "Train Epoch: 164, Loss: 3.223777, loss bulk: 3.223776, loss surf: 0.000001\n",
      "Train Epoch: 165, Loss: 3.241074, loss bulk: 3.241074, loss surf: 0.000000\n",
      "Train Epoch: 166, Loss: 3.232262, loss bulk: 3.232261, loss surf: 0.000001\n",
      "Train Epoch: 167, Loss: 3.290465, loss bulk: 3.290464, loss surf: 0.000000\n",
      "Train Epoch: 168, Loss: 3.254105, loss bulk: 3.254103, loss surf: 0.000001\n",
      "Train Epoch: 169, Loss: 3.258246, loss bulk: 3.258245, loss surf: 0.000001\n",
      "Train Epoch: 170, Loss: 3.218692, loss bulk: 3.218690, loss surf: 0.000002\n",
      "Train Epoch: 171, Loss: 3.204998, loss bulk: 3.204995, loss surf: 0.000003\n",
      "Train Epoch: 172, Loss: 3.218520, loss bulk: 3.218518, loss surf: 0.000002\n",
      "Train Epoch: 173, Loss: 3.218736, loss bulk: 3.218735, loss surf: 0.000001\n",
      "Train Epoch: 174, Loss: 3.247598, loss bulk: 3.247598, loss surf: 0.000000\n",
      "Train Epoch: 175, Loss: 3.238651, loss bulk: 3.238651, loss surf: 0.000000\n",
      "Train Epoch: 176, Loss: 3.242249, loss bulk: 3.242248, loss surf: 0.000001\n",
      "Train Epoch: 177, Loss: 3.240793, loss bulk: 3.240793, loss surf: 0.000000\n",
      "Train Epoch: 178, Loss: 3.231653, loss bulk: 3.231653, loss surf: 0.000000\n",
      "Train Epoch: 179, Loss: 3.224935, loss bulk: 3.224919, loss surf: 0.000016\n",
      "Train Epoch: 180, Loss: 3.208443, loss bulk: 3.208406, loss surf: 0.000037\n",
      "Train Epoch: 181, Loss: 3.203435, loss bulk: 3.203250, loss surf: 0.000186\n",
      "Train Epoch: 182, Loss: 3.190770, loss bulk: 3.190680, loss surf: 0.000090\n",
      "Train Epoch: 183, Loss: 3.192138, loss bulk: 3.192020, loss surf: 0.000118\n",
      "Train Epoch: 184, Loss: 3.188247, loss bulk: 3.187849, loss surf: 0.000398\n",
      "Train Epoch: 185, Loss: 3.192307, loss bulk: 3.192153, loss surf: 0.000155\n",
      "Train Epoch: 186, Loss: 3.201765, loss bulk: 3.201411, loss surf: 0.000354\n",
      "Train Epoch: 187, Loss: 3.227810, loss bulk: 3.227705, loss surf: 0.000105\n",
      "Train Epoch: 188, Loss: 3.302897, loss bulk: 3.302708, loss surf: 0.000189\n",
      "Train Epoch: 189, Loss: 3.421827, loss bulk: 3.421519, loss surf: 0.000308\n",
      "Train Epoch: 190, Loss: 3.445333, loss bulk: 3.445331, loss surf: 0.000002\n",
      "Train Epoch: 191, Loss: 3.356771, loss bulk: 3.356508, loss surf: 0.000263\n",
      "Train Epoch: 192, Loss: 3.216010, loss bulk: 3.215922, loss surf: 0.000088\n",
      "Train Epoch: 193, Loss: 3.240571, loss bulk: 3.240422, loss surf: 0.000148\n",
      "Train Epoch: 194, Loss: 3.337698, loss bulk: 3.337620, loss surf: 0.000078\n",
      "Train Epoch: 195, Loss: 3.329973, loss bulk: 3.329897, loss surf: 0.000076\n",
      "Train Epoch: 196, Loss: 3.216476, loss bulk: 3.216280, loss surf: 0.000197\n",
      "Train Epoch: 197, Loss: 3.224068, loss bulk: 3.224007, loss surf: 0.000061\n",
      "Train Epoch: 198, Loss: 3.356482, loss bulk: 3.356358, loss surf: 0.000124\n",
      "Train Epoch: 199, Loss: 3.336416, loss bulk: 3.336394, loss surf: 0.000022\n",
      "Train Epoch: 200, Loss: 3.232194, loss bulk: 3.232153, loss surf: 0.000041\n",
      "Train Epoch: 201, Loss: 3.187691, loss bulk: 3.187625, loss surf: 0.000067\n",
      "Train Epoch: 202, Loss: 3.233040, loss bulk: 3.233016, loss surf: 0.000024\n",
      "Train Epoch: 203, Loss: 3.235516, loss bulk: 3.235505, loss surf: 0.000011\n",
      "Train Epoch: 204, Loss: 3.221118, loss bulk: 3.221101, loss surf: 0.000017\n",
      "Train Epoch: 205, Loss: 3.206594, loss bulk: 3.206570, loss surf: 0.000024\n",
      "Train Epoch: 206, Loss: 3.185518, loss bulk: 3.185513, loss surf: 0.000004\n",
      "Train Epoch: 207, Loss: 3.170928, loss bulk: 3.170923, loss surf: 0.000006\n",
      "Train Epoch: 208, Loss: 3.170546, loss bulk: 3.170516, loss surf: 0.000029\n",
      "Train Epoch: 209, Loss: 3.181890, loss bulk: 3.181866, loss surf: 0.000023\n",
      "Train Epoch: 210, Loss: 3.198420, loss bulk: 3.198406, loss surf: 0.000015\n",
      "Train Epoch: 211, Loss: 3.208389, loss bulk: 3.208374, loss surf: 0.000015\n",
      "Train Epoch: 212, Loss: 3.234210, loss bulk: 3.234189, loss surf: 0.000021\n",
      "Train Epoch: 213, Loss: 3.249813, loss bulk: 3.249770, loss surf: 0.000043\n",
      "Train Epoch: 214, Loss: 3.275135, loss bulk: 3.275126, loss surf: 0.000009\n",
      "Train Epoch: 215, Loss: 3.255167, loss bulk: 3.255150, loss surf: 0.000017\n",
      "Train Epoch: 216, Loss: 3.235096, loss bulk: 3.235082, loss surf: 0.000015\n",
      "Train Epoch: 217, Loss: 3.200622, loss bulk: 3.200601, loss surf: 0.000021\n",
      "Train Epoch: 218, Loss: 3.167106, loss bulk: 3.167096, loss surf: 0.000010\n",
      "Train Epoch: 219, Loss: 3.157156, loss bulk: 3.157153, loss surf: 0.000004\n",
      "Train Epoch: 220, Loss: 3.150325, loss bulk: 3.150306, loss surf: 0.000019\n",
      "Train Epoch: 221, Loss: 3.145539, loss bulk: 3.145535, loss surf: 0.000004\n",
      "Train Epoch: 222, Loss: 3.145213, loss bulk: 3.145208, loss surf: 0.000005\n",
      "Train Epoch: 223, Loss: 3.154205, loss bulk: 3.154201, loss surf: 0.000004\n",
      "Train Epoch: 224, Loss: 3.221516, loss bulk: 3.221505, loss surf: 0.000011\n",
      "Train Epoch: 225, Loss: 3.564643, loss bulk: 3.564640, loss surf: 0.000004\n",
      "Train Epoch: 226, Loss: 3.959119, loss bulk: 3.959116, loss surf: 0.000003\n",
      "Train Epoch: 227, Loss: 3.647515, loss bulk: 3.647512, loss surf: 0.000003\n",
      "Train Epoch: 228, Loss: 3.801737, loss bulk: 3.801736, loss surf: 0.000001\n",
      "Train Epoch: 229, Loss: 3.619848, loss bulk: 3.619833, loss surf: 0.000015\n",
      "Train Epoch: 230, Loss: 3.728710, loss bulk: 3.728646, loss surf: 0.000064\n",
      "Train Epoch: 231, Loss: 3.717521, loss bulk: 3.717462, loss surf: 0.000059\n",
      "Train Epoch: 232, Loss: 3.611712, loss bulk: 3.611687, loss surf: 0.000025\n",
      "Train Epoch: 233, Loss: 3.739300, loss bulk: 3.739297, loss surf: 0.000003\n",
      "Train Epoch: 234, Loss: 3.644827, loss bulk: 3.644813, loss surf: 0.000014\n",
      "Train Epoch: 235, Loss: 3.680586, loss bulk: 3.680578, loss surf: 0.000008\n",
      "Train Epoch: 236, Loss: 3.666825, loss bulk: 3.666817, loss surf: 0.000007\n",
      "Train Epoch: 237, Loss: 3.630588, loss bulk: 3.630568, loss surf: 0.000020\n",
      "Train Epoch: 238, Loss: 3.636091, loss bulk: 3.636091, loss surf: 0.000000\n",
      "Train Epoch: 239, Loss: 3.627021, loss bulk: 3.626997, loss surf: 0.000024\n",
      "Train Epoch: 240, Loss: 3.602515, loss bulk: 3.602508, loss surf: 0.000007\n",
      "Train Epoch: 241, Loss: 3.630642, loss bulk: 3.630626, loss surf: 0.000016\n",
      "Train Epoch: 242, Loss: 3.606330, loss bulk: 3.606299, loss surf: 0.000031\n",
      "Train Epoch: 243, Loss: 3.618271, loss bulk: 3.618269, loss surf: 0.000002\n",
      "Train Epoch: 244, Loss: 3.616410, loss bulk: 3.616395, loss surf: 0.000015\n",
      "Train Epoch: 245, Loss: 3.592758, loss bulk: 3.592746, loss surf: 0.000012\n",
      "Train Epoch: 246, Loss: 3.600065, loss bulk: 3.600059, loss surf: 0.000006\n",
      "Train Epoch: 247, Loss: 3.590546, loss bulk: 3.590526, loss surf: 0.000020\n",
      "Train Epoch: 248, Loss: 3.592579, loss bulk: 3.592572, loss surf: 0.000007\n",
      "Train Epoch: 249, Loss: 3.596021, loss bulk: 3.596011, loss surf: 0.000010\n",
      "Train Epoch: 250, Loss: 3.591617, loss bulk: 3.591612, loss surf: 0.000006\n",
      "Train Epoch: 251, Loss: 3.589046, loss bulk: 3.589042, loss surf: 0.000004\n",
      "Train Epoch: 252, Loss: 3.587868, loss bulk: 3.587856, loss surf: 0.000013\n",
      "Train Epoch: 253, Loss: 3.575640, loss bulk: 3.575639, loss surf: 0.000000\n",
      "Train Epoch: 254, Loss: 3.580572, loss bulk: 3.580563, loss surf: 0.000008\n",
      "Train Epoch: 255, Loss: 3.574738, loss bulk: 3.574733, loss surf: 0.000005\n",
      "Train Epoch: 256, Loss: 3.574707, loss bulk: 3.574702, loss surf: 0.000005\n",
      "Train Epoch: 257, Loss: 3.578434, loss bulk: 3.578425, loss surf: 0.000009\n",
      "Train Epoch: 258, Loss: 3.572508, loss bulk: 3.572508, loss surf: 0.000001\n",
      "Train Epoch: 259, Loss: 3.572433, loss bulk: 3.572423, loss surf: 0.000010\n",
      "Train Epoch: 260, Loss: 3.568777, loss bulk: 3.568772, loss surf: 0.000005\n",
      "Train Epoch: 261, Loss: 3.565516, loss bulk: 3.565513, loss surf: 0.000003\n",
      "Train Epoch: 262, Loss: 3.566720, loss bulk: 3.566714, loss surf: 0.000006\n",
      "Train Epoch: 263, Loss: 3.563781, loss bulk: 3.563781, loss surf: 0.000000\n",
      "Train Epoch: 264, Loss: 3.564289, loss bulk: 3.564286, loss surf: 0.000003\n",
      "Train Epoch: 265, Loss: 3.563664, loss bulk: 3.563663, loss surf: 0.000001\n",
      "Train Epoch: 266, Loss: 3.560642, loss bulk: 3.560640, loss surf: 0.000002\n",
      "Train Epoch: 267, Loss: 3.560184, loss bulk: 3.560181, loss surf: 0.000002\n",
      "Train Epoch: 268, Loss: 3.557746, loss bulk: 3.557745, loss surf: 0.000001\n",
      "Train Epoch: 269, Loss: 3.555956, loss bulk: 3.555955, loss surf: 0.000001\n",
      "Train Epoch: 270, Loss: 3.555868, loss bulk: 3.555868, loss surf: 0.000000\n",
      "Train Epoch: 271, Loss: 3.553762, loss bulk: 3.553762, loss surf: 0.000000\n",
      "Train Epoch: 272, Loss: 3.554671, loss bulk: 3.554670, loss surf: 0.000001\n",
      "Train Epoch: 273, Loss: 3.552601, loss bulk: 3.552599, loss surf: 0.000001\n",
      "Train Epoch: 274, Loss: 3.551463, loss bulk: 3.551461, loss surf: 0.000001\n",
      "Train Epoch: 275, Loss: 3.550491, loss bulk: 3.550490, loss surf: 0.000001\n",
      "Train Epoch: 276, Loss: 3.548726, loss bulk: 3.548725, loss surf: 0.000000\n",
      "Train Epoch: 277, Loss: 3.548609, loss bulk: 3.548609, loss surf: 0.000000\n",
      "Train Epoch: 278, Loss: 3.547438, loss bulk: 3.547438, loss surf: 0.000000\n",
      "Train Epoch: 279, Loss: 3.546463, loss bulk: 3.546462, loss surf: 0.000001\n",
      "Train Epoch: 280, Loss: 3.546074, loss bulk: 3.546073, loss surf: 0.000001\n",
      "Train Epoch: 281, Loss: 3.544488, loss bulk: 3.544487, loss surf: 0.000001\n",
      "Train Epoch: 282, Loss: 3.544124, loss bulk: 3.544124, loss surf: 0.000000\n",
      "Train Epoch: 283, Loss: 3.542645, loss bulk: 3.542645, loss surf: 0.000000\n",
      "Train Epoch: 284, Loss: 3.541931, loss bulk: 3.541931, loss surf: 0.000000\n",
      "Train Epoch: 285, Loss: 3.541037, loss bulk: 3.541037, loss surf: 0.000000\n",
      "Train Epoch: 286, Loss: 3.540238, loss bulk: 3.540238, loss surf: 0.000000\n",
      "Train Epoch: 287, Loss: 3.539569, loss bulk: 3.539569, loss surf: 0.000000\n",
      "Train Epoch: 288, Loss: 3.538779, loss bulk: 3.538779, loss surf: 0.000000\n",
      "Train Epoch: 289, Loss: 3.537952, loss bulk: 3.537952, loss surf: 0.000000\n",
      "Train Epoch: 290, Loss: 3.537427, loss bulk: 3.537427, loss surf: 0.000000\n",
      "Train Epoch: 291, Loss: 3.535944, loss bulk: 3.535944, loss surf: 0.000000\n",
      "Train Epoch: 292, Loss: 3.535300, loss bulk: 3.535300, loss surf: 0.000000\n",
      "Train Epoch: 293, Loss: 3.534156, loss bulk: 3.534156, loss surf: 0.000000\n",
      "Train Epoch: 294, Loss: 3.533388, loss bulk: 3.533388, loss surf: 0.000000\n",
      "Train Epoch: 295, Loss: 3.532164, loss bulk: 3.532164, loss surf: 0.000000\n",
      "Train Epoch: 296, Loss: 3.531042, loss bulk: 3.531042, loss surf: 0.000000\n",
      "Train Epoch: 297, Loss: 3.530218, loss bulk: 3.530218, loss surf: 0.000000\n",
      "Train Epoch: 298, Loss: 3.529363, loss bulk: 3.529363, loss surf: 0.000000\n",
      "Train Epoch: 299, Loss: 3.528242, loss bulk: 3.528242, loss surf: 0.000000\n",
      "Train Epoch: 300, Loss: 3.527665, loss bulk: 3.527665, loss surf: 0.000000\n",
      "Train Epoch: 301, Loss: 3.526911, loss bulk: 3.526911, loss surf: 0.000000\n",
      "Train Epoch: 302, Loss: 3.526386, loss bulk: 3.526386, loss surf: 0.000000\n",
      "Train Epoch: 303, Loss: 3.525341, loss bulk: 3.525341, loss surf: 0.000000\n",
      "Train Epoch: 304, Loss: 3.525078, loss bulk: 3.525078, loss surf: 0.000000\n",
      "Train Epoch: 305, Loss: 3.524295, loss bulk: 3.524295, loss surf: 0.000000\n",
      "Train Epoch: 306, Loss: 3.523508, loss bulk: 3.523508, loss surf: 0.000000\n",
      "Train Epoch: 307, Loss: 3.523454, loss bulk: 3.523454, loss surf: 0.000000\n",
      "Train Epoch: 308, Loss: 3.522517, loss bulk: 3.522516, loss surf: 0.000000\n",
      "Train Epoch: 309, Loss: 3.521829, loss bulk: 3.521829, loss surf: 0.000000\n",
      "Train Epoch: 310, Loss: 3.521295, loss bulk: 3.521295, loss surf: 0.000000\n",
      "Train Epoch: 311, Loss: 3.520580, loss bulk: 3.520580, loss surf: 0.000000\n",
      "Train Epoch: 312, Loss: 3.520062, loss bulk: 3.520062, loss surf: 0.000000\n",
      "Train Epoch: 313, Loss: 3.519465, loss bulk: 3.519465, loss surf: 0.000000\n",
      "Train Epoch: 314, Loss: 3.519008, loss bulk: 3.519008, loss surf: 0.000000\n",
      "Train Epoch: 315, Loss: 3.518381, loss bulk: 3.518381, loss surf: 0.000000\n",
      "Train Epoch: 316, Loss: 3.517693, loss bulk: 3.517693, loss surf: 0.000000\n",
      "Train Epoch: 317, Loss: 3.517117, loss bulk: 3.517117, loss surf: 0.000000\n",
      "Train Epoch: 318, Loss: 3.516756, loss bulk: 3.516756, loss surf: 0.000000\n",
      "Train Epoch: 319, Loss: 3.516161, loss bulk: 3.516161, loss surf: 0.000000\n",
      "Train Epoch: 320, Loss: 3.515387, loss bulk: 3.515387, loss surf: 0.000000\n",
      "Train Epoch: 321, Loss: 3.514843, loss bulk: 3.514843, loss surf: 0.000000\n",
      "Train Epoch: 322, Loss: 3.514243, loss bulk: 3.514243, loss surf: 0.000000\n",
      "Train Epoch: 323, Loss: 3.513836, loss bulk: 3.513836, loss surf: 0.000000\n",
      "Train Epoch: 324, Loss: 3.513363, loss bulk: 3.513363, loss surf: 0.000000\n",
      "Train Epoch: 325, Loss: 3.512625, loss bulk: 3.512625, loss surf: 0.000000\n",
      "Train Epoch: 326, Loss: 3.512223, loss bulk: 3.512223, loss surf: 0.000000\n",
      "Train Epoch: 327, Loss: 3.511593, loss bulk: 3.511593, loss surf: 0.000000\n",
      "Train Epoch: 328, Loss: 3.511385, loss bulk: 3.511385, loss surf: 0.000000\n",
      "Train Epoch: 329, Loss: 3.510658, loss bulk: 3.510658, loss surf: 0.000000\n",
      "Train Epoch: 330, Loss: 3.510100, loss bulk: 3.510100, loss surf: 0.000000\n",
      "Train Epoch: 331, Loss: 3.509605, loss bulk: 3.509605, loss surf: 0.000000\n",
      "Train Epoch: 332, Loss: 3.509061, loss bulk: 3.509061, loss surf: 0.000000\n",
      "Train Epoch: 333, Loss: 3.508514, loss bulk: 3.508514, loss surf: 0.000000\n",
      "Train Epoch: 334, Loss: 3.507996, loss bulk: 3.507996, loss surf: 0.000000\n",
      "Train Epoch: 335, Loss: 3.507618, loss bulk: 3.507618, loss surf: 0.000000\n",
      "Train Epoch: 336, Loss: 3.507117, loss bulk: 3.507117, loss surf: 0.000000\n",
      "Train Epoch: 337, Loss: 3.506631, loss bulk: 3.506631, loss surf: 0.000000\n",
      "Train Epoch: 338, Loss: 3.506298, loss bulk: 3.506298, loss surf: 0.000000\n",
      "Train Epoch: 339, Loss: 3.513967, loss bulk: 3.513967, loss surf: 0.000000\n",
      "Train Epoch: 340, Loss: 3.489594, loss bulk: 3.489594, loss surf: 0.000001\n",
      "Train Epoch: 341, Loss: 3.508229, loss bulk: 3.508229, loss surf: 0.000000\n",
      "Train Epoch: 342, Loss: 3.512972, loss bulk: 3.512971, loss surf: 0.000001\n",
      "Train Epoch: 343, Loss: 3.485745, loss bulk: 3.485744, loss surf: 0.000002\n",
      "Train Epoch: 344, Loss: 3.490380, loss bulk: 3.490380, loss surf: 0.000001\n",
      "Train Epoch: 345, Loss: 3.508399, loss bulk: 3.508399, loss surf: 0.000000\n",
      "Train Epoch: 346, Loss: 3.499510, loss bulk: 3.499507, loss surf: 0.000002\n",
      "Train Epoch: 347, Loss: 3.481677, loss bulk: 3.481675, loss surf: 0.000002\n",
      "Train Epoch: 348, Loss: 3.485076, loss bulk: 3.485076, loss surf: 0.000000\n",
      "Train Epoch: 349, Loss: 3.496156, loss bulk: 3.496155, loss surf: 0.000000\n",
      "Train Epoch: 350, Loss: 3.486793, loss bulk: 3.486791, loss surf: 0.000002\n",
      "Train Epoch: 351, Loss: 3.502994, loss bulk: 3.502992, loss surf: 0.000002\n",
      "Train Epoch: 352, Loss: 3.507849, loss bulk: 3.507848, loss surf: 0.000001\n",
      "Train Epoch: 353, Loss: 3.504520, loss bulk: 3.504518, loss surf: 0.000002\n",
      "Train Epoch: 354, Loss: 3.465296, loss bulk: 3.465296, loss surf: 0.000001\n",
      "Train Epoch: 355, Loss: 3.498640, loss bulk: 3.498638, loss surf: 0.000002\n",
      "Train Epoch: 356, Loss: 3.546084, loss bulk: 3.546083, loss surf: 0.000001\n",
      "Train Epoch: 357, Loss: 3.489205, loss bulk: 3.489204, loss surf: 0.000001\n",
      "Train Epoch: 358, Loss: 3.780678, loss bulk: 3.780674, loss surf: 0.000004\n",
      "Train Epoch: 359, Loss: 3.544875, loss bulk: 3.544873, loss surf: 0.000003\n",
      "Train Epoch: 360, Loss: 3.587596, loss bulk: 3.587594, loss surf: 0.000002\n",
      "Train Epoch: 361, Loss: 3.488539, loss bulk: 3.488533, loss surf: 0.000006\n",
      "Train Epoch: 362, Loss: 3.571134, loss bulk: 3.571126, loss surf: 0.000007\n",
      "Train Epoch: 363, Loss: 3.484707, loss bulk: 3.484702, loss surf: 0.000006\n",
      "Train Epoch: 364, Loss: 3.520126, loss bulk: 3.520120, loss surf: 0.000006\n",
      "Train Epoch: 365, Loss: 3.494356, loss bulk: 3.494352, loss surf: 0.000005\n",
      "Train Epoch: 366, Loss: 3.452923, loss bulk: 3.452899, loss surf: 0.000024\n",
      "Train Epoch: 367, Loss: 3.492558, loss bulk: 3.492498, loss surf: 0.000060\n",
      "Train Epoch: 368, Loss: 3.413996, loss bulk: 3.413968, loss surf: 0.000028\n",
      "Train Epoch: 369, Loss: 3.442582, loss bulk: 3.442573, loss surf: 0.000008\n",
      "Train Epoch: 370, Loss: 3.506267, loss bulk: 3.506261, loss surf: 0.000006\n",
      "Train Epoch: 371, Loss: 3.385473, loss bulk: 3.385468, loss surf: 0.000005\n",
      "Train Epoch: 372, Loss: 3.393902, loss bulk: 3.393899, loss surf: 0.000003\n",
      "Train Epoch: 373, Loss: 3.515531, loss bulk: 3.515518, loss surf: 0.000012\n",
      "Train Epoch: 374, Loss: 3.751017, loss bulk: 3.751009, loss surf: 0.000008\n",
      "Train Epoch: 375, Loss: 3.448360, loss bulk: 3.448353, loss surf: 0.000007\n",
      "Train Epoch: 376, Loss: 3.476862, loss bulk: 3.476860, loss surf: 0.000003\n",
      "Train Epoch: 377, Loss: 3.472833, loss bulk: 3.472823, loss surf: 0.000010\n",
      "Train Epoch: 378, Loss: 3.488206, loss bulk: 3.488186, loss surf: 0.000021\n",
      "Train Epoch: 379, Loss: 3.464102, loss bulk: 3.464092, loss surf: 0.000009\n",
      "Train Epoch: 380, Loss: 3.455247, loss bulk: 3.455244, loss surf: 0.000003\n",
      "Train Epoch: 381, Loss: 3.421877, loss bulk: 3.421859, loss surf: 0.000018\n",
      "Train Epoch: 382, Loss: 3.416568, loss bulk: 3.416553, loss surf: 0.000014\n",
      "Train Epoch: 383, Loss: 3.435685, loss bulk: 3.435682, loss surf: 0.000003\n",
      "Train Epoch: 384, Loss: 3.430219, loss bulk: 3.430201, loss surf: 0.000018\n",
      "Train Epoch: 385, Loss: 3.428378, loss bulk: 3.428362, loss surf: 0.000016\n",
      "Train Epoch: 386, Loss: 3.432925, loss bulk: 3.432923, loss surf: 0.000002\n",
      "Train Epoch: 387, Loss: 3.462316, loss bulk: 3.462315, loss surf: 0.000001\n",
      "Train Epoch: 388, Loss: 3.443083, loss bulk: 3.443081, loss surf: 0.000002\n",
      "Train Epoch: 389, Loss: 3.458665, loss bulk: 3.458663, loss surf: 0.000002\n",
      "Train Epoch: 390, Loss: 3.448818, loss bulk: 3.448809, loss surf: 0.000010\n",
      "Train Epoch: 391, Loss: 3.456029, loss bulk: 3.456022, loss surf: 0.000008\n",
      "Train Epoch: 392, Loss: 3.462524, loss bulk: 3.462523, loss surf: 0.000001\n",
      "Train Epoch: 393, Loss: 3.450771, loss bulk: 3.450768, loss surf: 0.000003\n",
      "Train Epoch: 394, Loss: 3.451296, loss bulk: 3.451293, loss surf: 0.000003\n",
      "Train Epoch: 395, Loss: 3.436733, loss bulk: 3.436728, loss surf: 0.000005\n",
      "Train Epoch: 396, Loss: 3.428825, loss bulk: 3.428825, loss surf: 0.000001\n",
      "Train Epoch: 397, Loss: 3.429178, loss bulk: 3.429172, loss surf: 0.000006\n",
      "Train Epoch: 398, Loss: 3.424213, loss bulk: 3.424213, loss surf: 0.000000\n",
      "Train Epoch: 399, Loss: 3.427581, loss bulk: 3.427577, loss surf: 0.000004\n",
      "Train Epoch: 400, Loss: 3.495521, loss bulk: 3.495519, loss surf: 0.000002\n",
      "Train Epoch: 401, Loss: 3.421031, loss bulk: 3.421028, loss surf: 0.000003\n",
      "Train Epoch: 402, Loss: 3.430151, loss bulk: 3.430150, loss surf: 0.000001\n",
      "Train Epoch: 403, Loss: 3.435216, loss bulk: 3.435209, loss surf: 0.000007\n",
      "Train Epoch: 404, Loss: 3.412018, loss bulk: 3.412007, loss surf: 0.000011\n",
      "Train Epoch: 405, Loss: 3.406473, loss bulk: 3.406458, loss surf: 0.000015\n",
      "Train Epoch: 406, Loss: 3.432420, loss bulk: 3.432419, loss surf: 0.000002\n",
      "Train Epoch: 407, Loss: 3.496681, loss bulk: 3.496679, loss surf: 0.000002\n",
      "Train Epoch: 408, Loss: 3.446341, loss bulk: 3.446331, loss surf: 0.000010\n",
      "Train Epoch: 409, Loss: 3.485388, loss bulk: 3.485377, loss surf: 0.000011\n",
      "Train Epoch: 410, Loss: 3.407156, loss bulk: 3.407156, loss surf: 0.000001\n",
      "Train Epoch: 411, Loss: 3.447243, loss bulk: 3.447220, loss surf: 0.000023\n",
      "Train Epoch: 412, Loss: 3.474045, loss bulk: 3.474033, loss surf: 0.000012\n",
      "Train Epoch: 413, Loss: 3.436745, loss bulk: 3.436741, loss surf: 0.000004\n",
      "Train Epoch: 414, Loss: 3.474164, loss bulk: 3.474162, loss surf: 0.000002\n",
      "Train Epoch: 415, Loss: 3.450840, loss bulk: 3.450834, loss surf: 0.000006\n",
      "Train Epoch: 416, Loss: 3.473999, loss bulk: 3.473996, loss surf: 0.000003\n",
      "Train Epoch: 417, Loss: 3.449026, loss bulk: 3.449021, loss surf: 0.000005\n",
      "Train Epoch: 418, Loss: 3.465472, loss bulk: 3.465460, loss surf: 0.000012\n",
      "Train Epoch: 419, Loss: 3.444371, loss bulk: 3.444370, loss surf: 0.000001\n",
      "Train Epoch: 420, Loss: 3.445091, loss bulk: 3.445088, loss surf: 0.000003\n",
      "Train Epoch: 421, Loss: 3.440417, loss bulk: 3.440416, loss surf: 0.000001\n",
      "Train Epoch: 422, Loss: 3.435549, loss bulk: 3.435548, loss surf: 0.000001\n",
      "Train Epoch: 423, Loss: 3.437250, loss bulk: 3.437246, loss surf: 0.000004\n",
      "Train Epoch: 424, Loss: 3.426755, loss bulk: 3.426750, loss surf: 0.000004\n",
      "Train Epoch: 425, Loss: 3.422585, loss bulk: 3.422583, loss surf: 0.000002\n",
      "Train Epoch: 426, Loss: 3.421009, loss bulk: 3.421008, loss surf: 0.000001\n",
      "Train Epoch: 427, Loss: 3.422194, loss bulk: 3.422192, loss surf: 0.000002\n",
      "Train Epoch: 428, Loss: 3.423131, loss bulk: 3.423130, loss surf: 0.000001\n",
      "Train Epoch: 429, Loss: 3.413856, loss bulk: 3.413852, loss surf: 0.000003\n",
      "Train Epoch: 430, Loss: 3.411730, loss bulk: 3.411729, loss surf: 0.000001\n",
      "Train Epoch: 431, Loss: 3.411822, loss bulk: 3.411821, loss surf: 0.000001\n",
      "Train Epoch: 432, Loss: 3.412671, loss bulk: 3.412670, loss surf: 0.000001\n",
      "Train Epoch: 433, Loss: 3.411010, loss bulk: 3.411008, loss surf: 0.000002\n",
      "Train Epoch: 434, Loss: 3.409231, loss bulk: 3.409230, loss surf: 0.000000\n",
      "Train Epoch: 435, Loss: 3.409587, loss bulk: 3.409587, loss surf: 0.000001\n",
      "Train Epoch: 436, Loss: 3.408304, loss bulk: 3.408303, loss surf: 0.000001\n",
      "Train Epoch: 437, Loss: 3.405530, loss bulk: 3.405528, loss surf: 0.000001\n",
      "Train Epoch: 438, Loss: 3.403969, loss bulk: 3.403969, loss surf: 0.000001\n",
      "Train Epoch: 439, Loss: 3.404460, loss bulk: 3.404458, loss surf: 0.000002\n",
      "Train Epoch: 440, Loss: 3.405427, loss bulk: 3.405427, loss surf: 0.000001\n",
      "Train Epoch: 441, Loss: 3.403749, loss bulk: 3.403748, loss surf: 0.000001\n",
      "Train Epoch: 442, Loss: 3.402268, loss bulk: 3.402268, loss surf: 0.000000\n",
      "Train Epoch: 443, Loss: 3.401312, loss bulk: 3.401311, loss surf: 0.000001\n",
      "Train Epoch: 444, Loss: 3.402363, loss bulk: 3.402363, loss surf: 0.000001\n",
      "Train Epoch: 445, Loss: 3.401541, loss bulk: 3.401539, loss surf: 0.000002\n",
      "Train Epoch: 446, Loss: 3.400592, loss bulk: 3.400592, loss surf: 0.000000\n",
      "Train Epoch: 447, Loss: 3.400470, loss bulk: 3.400470, loss surf: 0.000001\n",
      "Train Epoch: 448, Loss: 3.399765, loss bulk: 3.399765, loss surf: 0.000000\n",
      "Train Epoch: 449, Loss: 3.399507, loss bulk: 3.399506, loss surf: 0.000001\n",
      "Train Epoch: 450, Loss: 3.398918, loss bulk: 3.398917, loss surf: 0.000001\n",
      "Train Epoch: 451, Loss: 3.398667, loss bulk: 3.398665, loss surf: 0.000001\n",
      "Train Epoch: 452, Loss: 3.398438, loss bulk: 3.398438, loss surf: 0.000000\n",
      "Train Epoch: 453, Loss: 3.397752, loss bulk: 3.397752, loss surf: 0.000001\n",
      "Train Epoch: 454, Loss: 3.397414, loss bulk: 3.397414, loss surf: 0.000000\n",
      "Train Epoch: 455, Loss: 3.397233, loss bulk: 3.397233, loss surf: 0.000000\n",
      "Train Epoch: 456, Loss: 3.397171, loss bulk: 3.397170, loss surf: 0.000000\n",
      "Train Epoch: 457, Loss: 3.396994, loss bulk: 3.396993, loss surf: 0.000001\n",
      "Train Epoch: 458, Loss: 3.396180, loss bulk: 3.396179, loss surf: 0.000000\n",
      "Train Epoch: 459, Loss: 3.396184, loss bulk: 3.396184, loss surf: 0.000000\n",
      "Train Epoch: 460, Loss: 3.395741, loss bulk: 3.395741, loss surf: 0.000000\n",
      "Train Epoch: 461, Loss: 3.395389, loss bulk: 3.395389, loss surf: 0.000001\n",
      "Train Epoch: 462, Loss: 3.395046, loss bulk: 3.395046, loss surf: 0.000000\n",
      "Train Epoch: 463, Loss: 3.395073, loss bulk: 3.395072, loss surf: 0.000001\n",
      "Train Epoch: 464, Loss: 3.394651, loss bulk: 3.394651, loss surf: 0.000000\n",
      "Train Epoch: 465, Loss: 3.394431, loss bulk: 3.394431, loss surf: 0.000000\n",
      "Train Epoch: 466, Loss: 3.394083, loss bulk: 3.394083, loss surf: 0.000000\n",
      "Train Epoch: 467, Loss: 3.393753, loss bulk: 3.393753, loss surf: 0.000000\n",
      "Train Epoch: 468, Loss: 3.393732, loss bulk: 3.393732, loss surf: 0.000000\n",
      "Train Epoch: 469, Loss: 3.393309, loss bulk: 3.393308, loss surf: 0.000000\n",
      "Train Epoch: 470, Loss: 3.392916, loss bulk: 3.392916, loss surf: 0.000000\n",
      "Train Epoch: 471, Loss: 3.392920, loss bulk: 3.392920, loss surf: 0.000000\n",
      "Train Epoch: 472, Loss: 3.392535, loss bulk: 3.392535, loss surf: 0.000000\n",
      "Train Epoch: 473, Loss: 3.392462, loss bulk: 3.392462, loss surf: 0.000000\n",
      "Train Epoch: 474, Loss: 3.392321, loss bulk: 3.392321, loss surf: 0.000000\n",
      "Train Epoch: 475, Loss: 3.391900, loss bulk: 3.391900, loss surf: 0.000000\n",
      "Train Epoch: 476, Loss: 3.391837, loss bulk: 3.391837, loss surf: 0.000000\n",
      "Train Epoch: 477, Loss: 3.391612, loss bulk: 3.391612, loss surf: 0.000000\n",
      "Train Epoch: 478, Loss: 3.391162, loss bulk: 3.391162, loss surf: 0.000000\n",
      "Train Epoch: 479, Loss: 3.391005, loss bulk: 3.391005, loss surf: 0.000000\n",
      "Train Epoch: 480, Loss: 3.390874, loss bulk: 3.390874, loss surf: 0.000000\n",
      "Train Epoch: 481, Loss: 3.390566, loss bulk: 3.390566, loss surf: 0.000000\n",
      "Train Epoch: 482, Loss: 3.390339, loss bulk: 3.390339, loss surf: 0.000000\n",
      "Train Epoch: 483, Loss: 3.389993, loss bulk: 3.389993, loss surf: 0.000000\n",
      "Train Epoch: 484, Loss: 3.389984, loss bulk: 3.389984, loss surf: 0.000000\n",
      "Train Epoch: 485, Loss: 3.389759, loss bulk: 3.389759, loss surf: 0.000000\n",
      "Train Epoch: 486, Loss: 3.389801, loss bulk: 3.389801, loss surf: 0.000000\n",
      "Train Epoch: 487, Loss: 3.389508, loss bulk: 3.389508, loss surf: 0.000000\n",
      "Train Epoch: 488, Loss: 3.389166, loss bulk: 3.389166, loss surf: 0.000000\n",
      "Train Epoch: 489, Loss: 3.389072, loss bulk: 3.389072, loss surf: 0.000000\n",
      "Train Epoch: 490, Loss: 3.388577, loss bulk: 3.388577, loss surf: 0.000000\n",
      "Train Epoch: 491, Loss: 3.388591, loss bulk: 3.388591, loss surf: 0.000000\n",
      "Train Epoch: 492, Loss: 3.388306, loss bulk: 3.388306, loss surf: 0.000000\n",
      "Train Epoch: 493, Loss: 3.388175, loss bulk: 3.388175, loss surf: 0.000000\n",
      "Train Epoch: 494, Loss: 3.387761, loss bulk: 3.387761, loss surf: 0.000000\n",
      "Train Epoch: 495, Loss: 3.387723, loss bulk: 3.387723, loss surf: 0.000000\n",
      "Train Epoch: 496, Loss: 3.387589, loss bulk: 3.387589, loss surf: 0.000000\n",
      "Train Epoch: 497, Loss: 3.387296, loss bulk: 3.387296, loss surf: 0.000000\n",
      "Train Epoch: 498, Loss: 3.387393, loss bulk: 3.387393, loss surf: 0.000000\n",
      "Train Epoch: 499, Loss: 3.386955, loss bulk: 3.386955, loss surf: 0.000000\n",
      "Train Epoch: 500, Loss: 3.386778, loss bulk: 3.386778, loss surf: 0.000000\n",
      "Train Epoch: 501, Loss: 3.386353, loss bulk: 3.386353, loss surf: 0.000000\n",
      "Train Epoch: 502, Loss: 3.386381, loss bulk: 3.386381, loss surf: 0.000000\n",
      "Train Epoch: 503, Loss: 3.385886, loss bulk: 3.385886, loss surf: 0.000000\n",
      "Train Epoch: 504, Loss: 3.385783, loss bulk: 3.385783, loss surf: 0.000000\n",
      "Train Epoch: 505, Loss: 3.385454, loss bulk: 3.385454, loss surf: 0.000000\n",
      "Train Epoch: 506, Loss: 3.385717, loss bulk: 3.385717, loss surf: 0.000000\n",
      "Train Epoch: 507, Loss: 3.385162, loss bulk: 3.385162, loss surf: 0.000000\n",
      "Train Epoch: 508, Loss: 3.385024, loss bulk: 3.385024, loss surf: 0.000000\n",
      "Train Epoch: 509, Loss: 3.384995, loss bulk: 3.384995, loss surf: 0.000000\n",
      "Train Epoch: 510, Loss: 3.384817, loss bulk: 3.384817, loss surf: 0.000000\n",
      "Train Epoch: 511, Loss: 3.384562, loss bulk: 3.384562, loss surf: 0.000000\n",
      "Train Epoch: 512, Loss: 3.384448, loss bulk: 3.384448, loss surf: 0.000000\n",
      "Train Epoch: 513, Loss: 3.384090, loss bulk: 3.384090, loss surf: 0.000000\n",
      "Train Epoch: 514, Loss: 3.384226, loss bulk: 3.384226, loss surf: 0.000000\n",
      "Train Epoch: 515, Loss: 3.383911, loss bulk: 3.383911, loss surf: 0.000000\n",
      "Train Epoch: 516, Loss: 3.383817, loss bulk: 3.383817, loss surf: 0.000000\n",
      "Train Epoch: 517, Loss: 3.383470, loss bulk: 3.383470, loss surf: 0.000000\n",
      "Train Epoch: 518, Loss: 3.383043, loss bulk: 3.383043, loss surf: 0.000000\n",
      "Train Epoch: 519, Loss: 3.384967, loss bulk: 3.384967, loss surf: 0.000000\n",
      "Train Epoch: 520, Loss: 3.393652, loss bulk: 3.393652, loss surf: 0.000000\n",
      "Train Epoch: 521, Loss: 3.391584, loss bulk: 3.391584, loss surf: 0.000000\n",
      "Train Epoch: 522, Loss: 3.385604, loss bulk: 3.385604, loss surf: 0.000000\n",
      "Train Epoch: 523, Loss: 3.389109, loss bulk: 3.389109, loss surf: 0.000000\n",
      "Train Epoch: 524, Loss: 3.392393, loss bulk: 3.392393, loss surf: 0.000000\n",
      "Train Epoch: 525, Loss: 3.392349, loss bulk: 3.392349, loss surf: 0.000000\n",
      "Train Epoch: 526, Loss: 3.383169, loss bulk: 3.383169, loss surf: 0.000000\n",
      "Train Epoch: 527, Loss: 3.385727, loss bulk: 3.385727, loss surf: 0.000000\n",
      "Train Epoch: 528, Loss: 3.381686, loss bulk: 3.381686, loss surf: 0.000000\n",
      "Train Epoch: 529, Loss: 3.385721, loss bulk: 3.385720, loss surf: 0.000001\n",
      "Train Epoch: 530, Loss: 3.400554, loss bulk: 3.400554, loss surf: 0.000000\n",
      "Train Epoch: 531, Loss: 3.401779, loss bulk: 3.401779, loss surf: 0.000000\n",
      "Train Epoch: 532, Loss: 3.383676, loss bulk: 3.383675, loss surf: 0.000001\n",
      "Train Epoch: 533, Loss: 3.383044, loss bulk: 3.383044, loss surf: 0.000000\n",
      "Train Epoch: 534, Loss: 3.402597, loss bulk: 3.402596, loss surf: 0.000001\n",
      "Train Epoch: 535, Loss: 3.383081, loss bulk: 3.383079, loss surf: 0.000002\n",
      "Train Epoch: 536, Loss: 3.381676, loss bulk: 3.381675, loss surf: 0.000000\n",
      "Train Epoch: 537, Loss: 3.387584, loss bulk: 3.387583, loss surf: 0.000001\n",
      "Train Epoch: 538, Loss: 3.381914, loss bulk: 3.381913, loss surf: 0.000002\n",
      "Train Epoch: 539, Loss: 3.375399, loss bulk: 3.375398, loss surf: 0.000000\n",
      "Train Epoch: 540, Loss: 3.377642, loss bulk: 3.377642, loss surf: 0.000000\n",
      "Train Epoch: 541, Loss: 3.435316, loss bulk: 3.435315, loss surf: 0.000001\n",
      "Train Epoch: 542, Loss: 3.612969, loss bulk: 3.612967, loss surf: 0.000001\n",
      "Train Epoch: 543, Loss: 3.408377, loss bulk: 3.408371, loss surf: 0.000006\n",
      "Train Epoch: 544, Loss: 3.444175, loss bulk: 3.444174, loss surf: 0.000002\n",
      "Train Epoch: 545, Loss: 3.512309, loss bulk: 3.512298, loss surf: 0.000011\n",
      "Train Epoch: 546, Loss: 3.427359, loss bulk: 3.427349, loss surf: 0.000010\n",
      "Train Epoch: 547, Loss: 3.513664, loss bulk: 3.513656, loss surf: 0.000008\n",
      "Train Epoch: 548, Loss: 3.508720, loss bulk: 3.508703, loss surf: 0.000018\n",
      "Train Epoch: 549, Loss: 3.542212, loss bulk: 3.542182, loss surf: 0.000031\n",
      "Train Epoch: 550, Loss: 3.524484, loss bulk: 3.524414, loss surf: 0.000070\n",
      "Train Epoch: 551, Loss: 3.473276, loss bulk: 3.473258, loss surf: 0.000018\n",
      "Train Epoch: 552, Loss: 3.532113, loss bulk: 3.532085, loss surf: 0.000028\n",
      "Train Epoch: 553, Loss: 3.499066, loss bulk: 3.498945, loss surf: 0.000121\n",
      "Train Epoch: 554, Loss: 3.456762, loss bulk: 3.456734, loss surf: 0.000028\n",
      "Train Epoch: 555, Loss: 3.494040, loss bulk: 3.493995, loss surf: 0.000045\n",
      "Train Epoch: 556, Loss: 3.498162, loss bulk: 3.498037, loss surf: 0.000125\n",
      "Train Epoch: 557, Loss: 3.483732, loss bulk: 3.483688, loss surf: 0.000044\n",
      "Train Epoch: 558, Loss: 3.465431, loss bulk: 3.465430, loss surf: 0.000001\n",
      "Train Epoch: 559, Loss: 3.446846, loss bulk: 3.446823, loss surf: 0.000023\n",
      "Train Epoch: 560, Loss: 3.469306, loss bulk: 3.469296, loss surf: 0.000009\n",
      "Train Epoch: 561, Loss: 3.495558, loss bulk: 3.495552, loss surf: 0.000006\n",
      "Train Epoch: 562, Loss: 3.451152, loss bulk: 3.451147, loss surf: 0.000005\n",
      "Train Epoch: 563, Loss: 3.495807, loss bulk: 3.495805, loss surf: 0.000002\n",
      "Train Epoch: 564, Loss: 3.457717, loss bulk: 3.457713, loss surf: 0.000004\n",
      "Train Epoch: 565, Loss: 3.450690, loss bulk: 3.450688, loss surf: 0.000002\n",
      "Train Epoch: 566, Loss: 3.467815, loss bulk: 3.467808, loss surf: 0.000007\n",
      "Train Epoch: 567, Loss: 3.483904, loss bulk: 3.483902, loss surf: 0.000003\n",
      "Train Epoch: 568, Loss: 3.465812, loss bulk: 3.465781, loss surf: 0.000031\n",
      "Train Epoch: 569, Loss: 3.473080, loss bulk: 3.473063, loss surf: 0.000017\n",
      "Train Epoch: 570, Loss: 3.459031, loss bulk: 3.459028, loss surf: 0.000002\n",
      "Train Epoch: 571, Loss: 3.476599, loss bulk: 3.476586, loss surf: 0.000013\n",
      "Train Epoch: 572, Loss: 3.456911, loss bulk: 3.456908, loss surf: 0.000003\n",
      "Train Epoch: 573, Loss: 3.485882, loss bulk: 3.485878, loss surf: 0.000004\n",
      "Train Epoch: 574, Loss: 3.517285, loss bulk: 3.517282, loss surf: 0.000003\n",
      "Train Epoch: 575, Loss: 3.563325, loss bulk: 3.563317, loss surf: 0.000009\n",
      "Train Epoch: 576, Loss: 3.472960, loss bulk: 3.472953, loss surf: 0.000007\n",
      "Train Epoch: 577, Loss: 3.606117, loss bulk: 3.606110, loss surf: 0.000008\n",
      "Train Epoch: 578, Loss: 3.664597, loss bulk: 3.664557, loss surf: 0.000040\n",
      "Train Epoch: 579, Loss: 3.465406, loss bulk: 3.465317, loss surf: 0.000090\n",
      "Train Epoch: 580, Loss: 3.518838, loss bulk: 3.518836, loss surf: 0.000003\n",
      "Train Epoch: 581, Loss: 3.433065, loss bulk: 3.432895, loss surf: 0.000170\n",
      "Train Epoch: 582, Loss: 3.416356, loss bulk: 3.416167, loss surf: 0.000189\n",
      "Train Epoch: 583, Loss: 3.392405, loss bulk: 3.392395, loss surf: 0.000010\n",
      "Train Epoch: 584, Loss: 3.360797, loss bulk: 3.360734, loss surf: 0.000062\n",
      "Train Epoch: 585, Loss: 3.495383, loss bulk: 3.495338, loss surf: 0.000045\n",
      "Train Epoch: 586, Loss: 3.548599, loss bulk: 3.548592, loss surf: 0.000006\n",
      "Train Epoch: 587, Loss: 3.592812, loss bulk: 3.592759, loss surf: 0.000053\n",
      "Train Epoch: 588, Loss: 3.994320, loss bulk: 3.994320, loss surf: 0.000000\n",
      "Train Epoch: 589, Loss: 3.662499, loss bulk: 3.662139, loss surf: 0.000360\n",
      "Train Epoch: 590, Loss: 3.608649, loss bulk: 3.608291, loss surf: 0.000359\n",
      "Train Epoch: 591, Loss: 3.538878, loss bulk: 3.538858, loss surf: 0.000020\n",
      "Train Epoch: 592, Loss: 4.456580, loss bulk: 4.456133, loss surf: 0.000447\n",
      "Train Epoch: 593, Loss: 3.962394, loss bulk: 3.961670, loss surf: 0.000724\n",
      "Train Epoch: 594, Loss: 3.726646, loss bulk: 3.726521, loss surf: 0.000125\n",
      "Train Epoch: 595, Loss: 5.438558, loss bulk: 5.438040, loss surf: 0.000518\n",
      "Train Epoch: 596, Loss: 5.075582, loss bulk: 5.073769, loss surf: 0.001813\n",
      "Train Epoch: 597, Loss: 3.578231, loss bulk: 3.577574, loss surf: 0.000656\n",
      "Train Epoch: 598, Loss: 4.087577, loss bulk: 4.087165, loss surf: 0.000412\n",
      "Train Epoch: 599, Loss: 4.281898, loss bulk: 4.280186, loss surf: 0.001712\n",
      "Train Epoch: 600, Loss: 4.268483, loss bulk: 4.268401, loss surf: 0.000082\n",
      "Train Epoch: 601, Loss: 4.043956, loss bulk: 4.042784, loss surf: 0.001172\n",
      "Train Epoch: 602, Loss: 3.922547, loss bulk: 3.921574, loss surf: 0.000973\n",
      "Train Epoch: 603, Loss: 4.432072, loss bulk: 4.431805, loss surf: 0.000267\n",
      "Train Epoch: 604, Loss: 3.896852, loss bulk: 3.894212, loss surf: 0.002639\n",
      "Train Epoch: 605, Loss: 3.678703, loss bulk: 3.677772, loss surf: 0.000931\n",
      "Train Epoch: 606, Loss: 3.400630, loss bulk: 3.399980, loss surf: 0.000650\n",
      "Train Epoch: 607, Loss: 3.781010, loss bulk: 3.779012, loss surf: 0.001998\n",
      "Train Epoch: 608, Loss: 3.858220, loss bulk: 3.857923, loss surf: 0.000297\n",
      "Train Epoch: 609, Loss: 5.980150, loss bulk: 5.979309, loss surf: 0.000841\n",
      "Train Epoch: 610, Loss: 8.853108, loss bulk: 8.850319, loss surf: 0.002789\n",
      "Train Epoch: 611, Loss: 4.060021, loss bulk: 4.058403, loss surf: 0.001618\n",
      "Train Epoch: 612, Loss: 9.980548, loss bulk: 9.980125, loss surf: 0.000423\n",
      "Train Epoch: 613, Loss: 18.382992, loss bulk: 18.376476, loss surf: 0.006516\n",
      "Train Epoch: 614, Loss: 18.117672, loss bulk: 18.109383, loss surf: 0.008290\n",
      "Train Epoch: 615, Loss: 14.334180, loss bulk: 14.332650, loss surf: 0.001529\n",
      "Train Epoch: 616, Loss: 5.641760, loss bulk: 5.640960, loss surf: 0.000801\n",
      "Train Epoch: 617, Loss: 15.986834, loss bulk: 15.980098, loss surf: 0.006736\n",
      "Train Epoch: 618, Loss: 11.142529, loss bulk: 11.133408, loss surf: 0.009121\n",
      "Train Epoch: 619, Loss: 7.119645, loss bulk: 7.119220, loss surf: 0.000425\n",
      "Train Epoch: 620, Loss: 10.300356, loss bulk: 10.293284, loss surf: 0.007071\n",
      "Train Epoch: 621, Loss: 12.612958, loss bulk: 12.601384, loss surf: 0.011574\n",
      "Train Epoch: 622, Loss: 10.745253, loss bulk: 10.742992, loss surf: 0.002260\n",
      "Train Epoch: 623, Loss: 9.509026, loss bulk: 9.507749, loss surf: 0.001277\n",
      "Train Epoch: 624, Loss: 9.191060, loss bulk: 9.186518, loss surf: 0.004543\n",
      "Train Epoch: 625, Loss: 10.268849, loss bulk: 10.267576, loss surf: 0.001273\n",
      "Train Epoch: 626, Loss: 9.963873, loss bulk: 9.963647, loss surf: 0.000226\n",
      "Train Epoch: 627, Loss: 7.516316, loss bulk: 7.516037, loss surf: 0.000279\n",
      "Train Epoch: 628, Loss: 7.063866, loss bulk: 7.063602, loss surf: 0.000263\n",
      "Train Epoch: 629, Loss: 8.967647, loss bulk: 8.967134, loss surf: 0.000513\n",
      "Train Epoch: 630, Loss: 10.686067, loss bulk: 10.685967, loss surf: 0.000099\n",
      "Train Epoch: 631, Loss: 10.367405, loss bulk: 10.366310, loss surf: 0.001095\n",
      "Train Epoch: 632, Loss: 12.401977, loss bulk: 12.401836, loss surf: 0.000140\n",
      "Train Epoch: 633, Loss: 9.674769, loss bulk: 9.672459, loss surf: 0.002311\n",
      "Train Epoch: 634, Loss: 7.285390, loss bulk: 7.281909, loss surf: 0.003481\n",
      "Train Epoch: 635, Loss: 6.187225, loss bulk: 6.187076, loss surf: 0.000149\n",
      "Train Epoch: 636, Loss: 5.353244, loss bulk: 5.351701, loss surf: 0.001543\n",
      "Train Epoch: 637, Loss: 6.257755, loss bulk: 6.256208, loss surf: 0.001547\n",
      "Train Epoch: 638, Loss: 7.263579, loss bulk: 7.263365, loss surf: 0.000214\n",
      "Train Epoch: 639, Loss: 7.650736, loss bulk: 7.648131, loss surf: 0.002604\n",
      "Train Epoch: 640, Loss: 7.677495, loss bulk: 7.676593, loss surf: 0.000902\n",
      "Train Epoch: 641, Loss: 7.910676, loss bulk: 7.909764, loss surf: 0.000912\n",
      "Train Epoch: 642, Loss: 7.424595, loss bulk: 7.420404, loss surf: 0.004191\n",
      "Train Epoch: 643, Loss: 7.414969, loss bulk: 7.413818, loss surf: 0.001152\n",
      "Train Epoch: 644, Loss: 7.455158, loss bulk: 7.454400, loss surf: 0.000758\n",
      "Train Epoch: 645, Loss: 7.162822, loss bulk: 7.160184, loss surf: 0.002638\n",
      "Train Epoch: 646, Loss: 7.076580, loss bulk: 7.076311, loss surf: 0.000269\n",
      "Train Epoch: 647, Loss: 7.386159, loss bulk: 7.385158, loss surf: 0.001001\n",
      "Train Epoch: 648, Loss: 6.233694, loss bulk: 6.232350, loss surf: 0.001343\n",
      "Train Epoch: 649, Loss: 5.370158, loss bulk: 5.370148, loss surf: 0.000010\n",
      "Train Epoch: 650, Loss: 4.765688, loss bulk: 4.764643, loss surf: 0.001045\n",
      "Train Epoch: 651, Loss: 6.084085, loss bulk: 6.083784, loss surf: 0.000301\n",
      "Train Epoch: 652, Loss: 4.843037, loss bulk: 4.842312, loss surf: 0.000725\n",
      "Train Epoch: 653, Loss: 5.772296, loss bulk: 5.770623, loss surf: 0.001673\n",
      "Train Epoch: 654, Loss: 7.470687, loss bulk: 7.470618, loss surf: 0.000069\n",
      "Train Epoch: 655, Loss: 5.389122, loss bulk: 5.387839, loss surf: 0.001283\n",
      "Train Epoch: 656, Loss: 5.039857, loss bulk: 5.038205, loss surf: 0.001652\n",
      "Train Epoch: 657, Loss: 5.381032, loss bulk: 5.380201, loss surf: 0.000832\n",
      "Train Epoch: 658, Loss: 4.923525, loss bulk: 4.920533, loss surf: 0.002992\n",
      "Train Epoch: 659, Loss: 4.510590, loss bulk: 4.509215, loss surf: 0.001375\n",
      "Train Epoch: 660, Loss: 6.016736, loss bulk: 6.015787, loss surf: 0.000949\n",
      "Train Epoch: 661, Loss: 6.028466, loss bulk: 6.026550, loss surf: 0.001916\n",
      "Train Epoch: 662, Loss: 5.265086, loss bulk: 5.263903, loss surf: 0.001183\n",
      "Train Epoch: 663, Loss: 5.388980, loss bulk: 5.387602, loss surf: 0.001379\n",
      "Train Epoch: 664, Loss: 5.519831, loss bulk: 5.517436, loss surf: 0.002395\n",
      "Train Epoch: 665, Loss: 5.559645, loss bulk: 5.558039, loss surf: 0.001606\n",
      "Train Epoch: 666, Loss: 5.522145, loss bulk: 5.522090, loss surf: 0.000055\n",
      "Train Epoch: 667, Loss: 5.475760, loss bulk: 5.475596, loss surf: 0.000164\n",
      "Train Epoch: 668, Loss: 5.500146, loss bulk: 5.499977, loss surf: 0.000170\n",
      "Train Epoch: 669, Loss: 5.359678, loss bulk: 5.358939, loss surf: 0.000738\n",
      "Train Epoch: 670, Loss: 5.078927, loss bulk: 5.078541, loss surf: 0.000386\n",
      "Train Epoch: 671, Loss: 5.912070, loss bulk: 5.911757, loss surf: 0.000312\n",
      "Train Epoch: 672, Loss: 5.092242, loss bulk: 5.091715, loss surf: 0.000527\n",
      "Train Epoch: 673, Loss: 5.316244, loss bulk: 5.316157, loss surf: 0.000087\n",
      "Train Epoch: 674, Loss: 6.489187, loss bulk: 6.488880, loss surf: 0.000307\n",
      "Train Epoch: 675, Loss: 5.503475, loss bulk: 5.502060, loss surf: 0.001415\n",
      "Train Epoch: 676, Loss: 5.080772, loss bulk: 5.080195, loss surf: 0.000577\n",
      "Train Epoch: 677, Loss: 4.724476, loss bulk: 4.724160, loss surf: 0.000316\n",
      "Train Epoch: 678, Loss: 4.830361, loss bulk: 4.828828, loss surf: 0.001533\n",
      "Train Epoch: 679, Loss: 4.938803, loss bulk: 4.938305, loss surf: 0.000498\n",
      "Train Epoch: 680, Loss: 4.724980, loss bulk: 4.724548, loss surf: 0.000432\n",
      "Train Epoch: 681, Loss: 4.961618, loss bulk: 4.960714, loss surf: 0.000904\n",
      "Train Epoch: 682, Loss: 4.889112, loss bulk: 4.888760, loss surf: 0.000352\n",
      "Train Epoch: 683, Loss: 4.596359, loss bulk: 4.596215, loss surf: 0.000144\n",
      "Train Epoch: 684, Loss: 4.932311, loss bulk: 4.932263, loss surf: 0.000047\n",
      "Train Epoch: 685, Loss: 4.527691, loss bulk: 4.527660, loss surf: 0.000031\n",
      "Train Epoch: 686, Loss: 4.400721, loss bulk: 4.400432, loss surf: 0.000289\n",
      "Train Epoch: 687, Loss: 4.405713, loss bulk: 4.405322, loss surf: 0.000391\n",
      "Train Epoch: 688, Loss: 4.376308, loss bulk: 4.376177, loss surf: 0.000131\n",
      "Train Epoch: 689, Loss: 4.427820, loss bulk: 4.427768, loss surf: 0.000051\n",
      "Train Epoch: 690, Loss: 4.146345, loss bulk: 4.146257, loss surf: 0.000088\n",
      "Train Epoch: 691, Loss: 3.926585, loss bulk: 3.926558, loss surf: 0.000027\n",
      "Train Epoch: 692, Loss: 4.146173, loss bulk: 4.146142, loss surf: 0.000032\n",
      "Train Epoch: 693, Loss: 4.211820, loss bulk: 4.211573, loss surf: 0.000247\n",
      "Train Epoch: 694, Loss: 3.919430, loss bulk: 3.919132, loss surf: 0.000298\n",
      "Train Epoch: 695, Loss: 3.802579, loss bulk: 3.802510, loss surf: 0.000069\n",
      "Train Epoch: 696, Loss: 4.052876, loss bulk: 4.052819, loss surf: 0.000057\n",
      "Train Epoch: 697, Loss: 3.888505, loss bulk: 3.888319, loss surf: 0.000185\n",
      "Train Epoch: 698, Loss: 3.938474, loss bulk: 3.938351, loss surf: 0.000122\n",
      "Train Epoch: 699, Loss: 3.755111, loss bulk: 3.755041, loss surf: 0.000070\n",
      "Train Epoch: 700, Loss: 3.864113, loss bulk: 3.864086, loss surf: 0.000027\n",
      "Train Epoch: 701, Loss: 3.539206, loss bulk: 3.539177, loss surf: 0.000029\n",
      "Train Epoch: 702, Loss: 4.677524, loss bulk: 4.677492, loss surf: 0.000033\n",
      "Train Epoch: 703, Loss: 4.793350, loss bulk: 4.793193, loss surf: 0.000157\n",
      "Train Epoch: 704, Loss: 4.434181, loss bulk: 4.433901, loss surf: 0.000280\n",
      "Train Epoch: 705, Loss: 4.049751, loss bulk: 4.049670, loss surf: 0.000081\n",
      "Train Epoch: 706, Loss: 3.836391, loss bulk: 3.836356, loss surf: 0.000036\n",
      "Train Epoch: 707, Loss: 3.840056, loss bulk: 3.839850, loss surf: 0.000206\n",
      "Train Epoch: 708, Loss: 4.432446, loss bulk: 4.432378, loss surf: 0.000068\n",
      "Train Epoch: 709, Loss: 3.722975, loss bulk: 3.722857, loss surf: 0.000118\n",
      "Train Epoch: 710, Loss: 4.157683, loss bulk: 4.157550, loss surf: 0.000133\n",
      "Train Epoch: 711, Loss: 3.855351, loss bulk: 3.855313, loss surf: 0.000038\n",
      "Train Epoch: 712, Loss: 4.004848, loss bulk: 4.004667, loss surf: 0.000182\n",
      "Train Epoch: 713, Loss: 3.974429, loss bulk: 3.974427, loss surf: 0.000003\n",
      "Train Epoch: 714, Loss: 3.813373, loss bulk: 3.813241, loss surf: 0.000132\n",
      "Train Epoch: 715, Loss: 3.860658, loss bulk: 3.860561, loss surf: 0.000097\n",
      "Train Epoch: 716, Loss: 3.903445, loss bulk: 3.903422, loss surf: 0.000023\n",
      "Train Epoch: 717, Loss: 3.638143, loss bulk: 3.637920, loss surf: 0.000223\n",
      "Train Epoch: 718, Loss: 3.922635, loss bulk: 3.922619, loss surf: 0.000016\n",
      "Train Epoch: 719, Loss: 3.786471, loss bulk: 3.786382, loss surf: 0.000090\n",
      "Train Epoch: 720, Loss: 3.879612, loss bulk: 3.879488, loss surf: 0.000125\n",
      "Train Epoch: 721, Loss: 3.693106, loss bulk: 3.693104, loss surf: 0.000002\n",
      "Train Epoch: 722, Loss: 3.601414, loss bulk: 3.601281, loss surf: 0.000134\n",
      "Train Epoch: 723, Loss: 3.742777, loss bulk: 3.742664, loss surf: 0.000113\n",
      "Train Epoch: 724, Loss: 3.664606, loss bulk: 3.664601, loss surf: 0.000005\n",
      "Train Epoch: 725, Loss: 3.690810, loss bulk: 3.690797, loss surf: 0.000013\n",
      "Train Epoch: 726, Loss: 3.821018, loss bulk: 3.821017, loss surf: 0.000001\n",
      "Train Epoch: 727, Loss: 3.740449, loss bulk: 3.740444, loss surf: 0.000005\n",
      "Train Epoch: 728, Loss: 3.651403, loss bulk: 3.651384, loss surf: 0.000019\n",
      "Train Epoch: 729, Loss: 3.853067, loss bulk: 3.853051, loss surf: 0.000016\n",
      "Train Epoch: 730, Loss: 3.784373, loss bulk: 3.784360, loss surf: 0.000013\n",
      "Train Epoch: 731, Loss: 3.722126, loss bulk: 3.722113, loss surf: 0.000013\n",
      "Train Epoch: 732, Loss: 3.787330, loss bulk: 3.787309, loss surf: 0.000022\n",
      "Train Epoch: 733, Loss: 3.847842, loss bulk: 3.847777, loss surf: 0.000065\n",
      "Train Epoch: 734, Loss: 3.833132, loss bulk: 3.833043, loss surf: 0.000089\n",
      "Train Epoch: 735, Loss: 3.947576, loss bulk: 3.947488, loss surf: 0.000088\n",
      "Train Epoch: 736, Loss: 3.827923, loss bulk: 3.827920, loss surf: 0.000003\n",
      "Train Epoch: 737, Loss: 4.163263, loss bulk: 4.163199, loss surf: 0.000064\n",
      "Train Epoch: 738, Loss: 3.486885, loss bulk: 3.486792, loss surf: 0.000093\n",
      "Train Epoch: 739, Loss: 3.796948, loss bulk: 3.796924, loss surf: 0.000023\n",
      "Train Epoch: 740, Loss: 3.736226, loss bulk: 3.736061, loss surf: 0.000164\n",
      "Train Epoch: 741, Loss: 3.583151, loss bulk: 3.583002, loss surf: 0.000148\n",
      "Train Epoch: 742, Loss: 3.863993, loss bulk: 3.863991, loss surf: 0.000003\n",
      "Train Epoch: 743, Loss: 3.314926, loss bulk: 3.314692, loss surf: 0.000233\n",
      "Train Epoch: 744, Loss: 3.643448, loss bulk: 3.643319, loss surf: 0.000130\n",
      "Train Epoch: 745, Loss: 3.523301, loss bulk: 3.523249, loss surf: 0.000052\n",
      "Train Epoch: 746, Loss: 3.313940, loss bulk: 3.313812, loss surf: 0.000127\n",
      "Train Epoch: 747, Loss: 3.591644, loss bulk: 3.591643, loss surf: 0.000001\n",
      "Train Epoch: 748, Loss: 3.474708, loss bulk: 3.474664, loss surf: 0.000044\n",
      "Train Epoch: 749, Loss: 3.415257, loss bulk: 3.415250, loss surf: 0.000008\n",
      "Train Epoch: 750, Loss: 3.287988, loss bulk: 3.287980, loss surf: 0.000008\n",
      "Train Epoch: 751, Loss: 3.459787, loss bulk: 3.459767, loss surf: 0.000019\n",
      "Train Epoch: 752, Loss: 3.934669, loss bulk: 3.934650, loss surf: 0.000019\n",
      "Train Epoch: 753, Loss: 3.887093, loss bulk: 3.887022, loss surf: 0.000071\n",
      "Train Epoch: 754, Loss: 4.315634, loss bulk: 4.315629, loss surf: 0.000005\n",
      "Train Epoch: 755, Loss: 4.415616, loss bulk: 4.415299, loss surf: 0.000317\n",
      "Train Epoch: 756, Loss: 4.014101, loss bulk: 4.013635, loss surf: 0.000466\n",
      "Train Epoch: 757, Loss: 3.637868, loss bulk: 3.637861, loss surf: 0.000007\n",
      "Train Epoch: 758, Loss: 3.863489, loss bulk: 3.863128, loss surf: 0.000361\n",
      "Train Epoch: 759, Loss: 3.866407, loss bulk: 3.866008, loss surf: 0.000399\n",
      "Train Epoch: 760, Loss: 3.659667, loss bulk: 3.659663, loss surf: 0.000004\n",
      "Train Epoch: 761, Loss: 3.665169, loss bulk: 3.664911, loss surf: 0.000258\n",
      "Train Epoch: 762, Loss: 3.710495, loss bulk: 3.710188, loss surf: 0.000307\n",
      "Train Epoch: 763, Loss: 3.534940, loss bulk: 3.534925, loss surf: 0.000015\n",
      "Train Epoch: 764, Loss: 3.341631, loss bulk: 3.341553, loss surf: 0.000079\n",
      "Train Epoch: 765, Loss: 4.141422, loss bulk: 4.141407, loss surf: 0.000014\n",
      "Train Epoch: 766, Loss: 3.755686, loss bulk: 3.755473, loss surf: 0.000213\n",
      "Train Epoch: 767, Loss: 5.059654, loss bulk: 5.059608, loss surf: 0.000046\n",
      "Train Epoch: 768, Loss: 5.523909, loss bulk: 5.523304, loss surf: 0.000604\n",
      "Train Epoch: 769, Loss: 4.596283, loss bulk: 4.594974, loss surf: 0.001310\n",
      "Train Epoch: 770, Loss: 4.250903, loss bulk: 4.250804, loss surf: 0.000099\n",
      "Train Epoch: 771, Loss: 3.671410, loss bulk: 3.670798, loss surf: 0.000611\n",
      "Train Epoch: 772, Loss: 3.955113, loss bulk: 3.953901, loss surf: 0.001213\n",
      "Train Epoch: 773, Loss: 4.075594, loss bulk: 4.075305, loss surf: 0.000289\n",
      "Train Epoch: 774, Loss: 4.185447, loss bulk: 4.185275, loss surf: 0.000171\n",
      "Train Epoch: 775, Loss: 4.146248, loss bulk: 4.145641, loss surf: 0.000607\n",
      "Train Epoch: 776, Loss: 4.315368, loss bulk: 4.315236, loss surf: 0.000133\n",
      "Train Epoch: 777, Loss: 3.656172, loss bulk: 3.656166, loss surf: 0.000006\n",
      "Train Epoch: 778, Loss: 3.940475, loss bulk: 3.940469, loss surf: 0.000006\n",
      "Train Epoch: 779, Loss: 4.014979, loss bulk: 4.014946, loss surf: 0.000033\n",
      "Train Epoch: 780, Loss: 3.902223, loss bulk: 3.902207, loss surf: 0.000016\n",
      "Train Epoch: 781, Loss: 3.594270, loss bulk: 3.594111, loss surf: 0.000159\n",
      "Train Epoch: 782, Loss: 3.869413, loss bulk: 3.869376, loss surf: 0.000037\n",
      "Train Epoch: 783, Loss: 3.910236, loss bulk: 3.910203, loss surf: 0.000033\n",
      "Train Epoch: 784, Loss: 4.162018, loss bulk: 4.162001, loss surf: 0.000017\n",
      "Train Epoch: 785, Loss: 4.887002, loss bulk: 4.886996, loss surf: 0.000007\n",
      "Train Epoch: 786, Loss: 4.445614, loss bulk: 4.445522, loss surf: 0.000093\n",
      "Train Epoch: 787, Loss: 3.893349, loss bulk: 3.893260, loss surf: 0.000089\n",
      "Train Epoch: 788, Loss: 4.078842, loss bulk: 4.078835, loss surf: 0.000007\n",
      "Train Epoch: 789, Loss: 3.808885, loss bulk: 3.808844, loss surf: 0.000041\n",
      "Train Epoch: 790, Loss: 3.950856, loss bulk: 3.950819, loss surf: 0.000037\n",
      "Train Epoch: 791, Loss: 3.966395, loss bulk: 3.966383, loss surf: 0.000011\n",
      "Train Epoch: 792, Loss: 3.728354, loss bulk: 3.728346, loss surf: 0.000008\n",
      "Train Epoch: 793, Loss: 3.831955, loss bulk: 3.831952, loss surf: 0.000003\n",
      "Train Epoch: 794, Loss: 3.916572, loss bulk: 3.916492, loss surf: 0.000080\n",
      "Train Epoch: 795, Loss: 3.713380, loss bulk: 3.713256, loss surf: 0.000124\n",
      "Train Epoch: 796, Loss: 3.606710, loss bulk: 3.606693, loss surf: 0.000017\n",
      "Train Epoch: 797, Loss: 3.548185, loss bulk: 3.548179, loss surf: 0.000006\n",
      "Train Epoch: 798, Loss: 3.783910, loss bulk: 3.783873, loss surf: 0.000036\n",
      "Train Epoch: 799, Loss: 3.664197, loss bulk: 3.664011, loss surf: 0.000186\n",
      "Train Epoch: 800, Loss: 4.178141, loss bulk: 4.178124, loss surf: 0.000017\n",
      "Train Epoch: 801, Loss: 4.408924, loss bulk: 4.408288, loss surf: 0.000637\n",
      "Train Epoch: 802, Loss: 4.268970, loss bulk: 4.268357, loss surf: 0.000613\n",
      "Train Epoch: 803, Loss: 4.204636, loss bulk: 4.204628, loss surf: 0.000008\n",
      "Train Epoch: 804, Loss: 3.860703, loss bulk: 3.860502, loss surf: 0.000202\n",
      "Train Epoch: 805, Loss: 3.764409, loss bulk: 3.764233, loss surf: 0.000176\n",
      "Train Epoch: 806, Loss: 4.812471, loss bulk: 4.812355, loss surf: 0.000116\n",
      "Train Epoch: 807, Loss: 3.740929, loss bulk: 3.740713, loss surf: 0.000216\n",
      "Train Epoch: 808, Loss: 4.269763, loss bulk: 4.269677, loss surf: 0.000086\n",
      "Train Epoch: 809, Loss: 4.887103, loss bulk: 4.886937, loss surf: 0.000165\n",
      "Train Epoch: 810, Loss: 4.766936, loss bulk: 4.766166, loss surf: 0.000770\n",
      "Train Epoch: 811, Loss: 3.912772, loss bulk: 3.912316, loss surf: 0.000455\n",
      "Train Epoch: 812, Loss: 3.996033, loss bulk: 3.995962, loss surf: 0.000071\n",
      "Train Epoch: 813, Loss: 5.489246, loss bulk: 5.488095, loss surf: 0.001151\n",
      "Train Epoch: 814, Loss: 5.162941, loss bulk: 5.162067, loss surf: 0.000874\n",
      "Train Epoch: 815, Loss: 4.217104, loss bulk: 4.217104, loss surf: 0.000001\n",
      "Train Epoch: 816, Loss: 3.972710, loss bulk: 3.972349, loss surf: 0.000361\n",
      "Train Epoch: 817, Loss: 4.094287, loss bulk: 4.094037, loss surf: 0.000250\n",
      "Train Epoch: 818, Loss: 5.890647, loss bulk: 5.890560, loss surf: 0.000087\n",
      "Train Epoch: 819, Loss: 5.251594, loss bulk: 5.251296, loss surf: 0.000298\n",
      "Train Epoch: 820, Loss: 3.845485, loss bulk: 3.845186, loss surf: 0.000299\n",
      "Train Epoch: 821, Loss: 4.252230, loss bulk: 4.252120, loss surf: 0.000111\n",
      "Train Epoch: 822, Loss: 5.598425, loss bulk: 5.597233, loss surf: 0.001192\n",
      "Train Epoch: 823, Loss: 3.919192, loss bulk: 3.918010, loss surf: 0.001182\n",
      "Train Epoch: 824, Loss: 6.328839, loss bulk: 6.328783, loss surf: 0.000056\n",
      "Train Epoch: 825, Loss: 6.768673, loss bulk: 6.765750, loss surf: 0.002924\n",
      "Train Epoch: 826, Loss: 3.911426, loss bulk: 3.909370, loss surf: 0.002056\n",
      "Train Epoch: 827, Loss: 5.623330, loss bulk: 5.622805, loss surf: 0.000524\n",
      "Train Epoch: 828, Loss: 4.670112, loss bulk: 4.665292, loss surf: 0.004820\n",
      "Train Epoch: 829, Loss: 3.984586, loss bulk: 3.983744, loss surf: 0.000841\n",
      "Train Epoch: 830, Loss: 4.594119, loss bulk: 4.591809, loss surf: 0.002310\n",
      "Train Epoch: 831, Loss: 6.753502, loss bulk: 6.749006, loss surf: 0.004496\n",
      "Train Epoch: 832, Loss: 4.823740, loss bulk: 4.823322, loss surf: 0.000417\n",
      "Train Epoch: 833, Loss: 4.357563, loss bulk: 4.356035, loss surf: 0.001528\n",
      "Train Epoch: 834, Loss: 5.269578, loss bulk: 5.267163, loss surf: 0.002415\n",
      "Train Epoch: 835, Loss: 4.087194, loss bulk: 4.087088, loss surf: 0.000106\n",
      "Train Epoch: 836, Loss: 4.630869, loss bulk: 4.630014, loss surf: 0.000855\n",
      "Train Epoch: 837, Loss: 5.754583, loss bulk: 5.753960, loss surf: 0.000623\n",
      "Train Epoch: 838, Loss: 5.931444, loss bulk: 5.931428, loss surf: 0.000015\n",
      "Train Epoch: 839, Loss: 5.625833, loss bulk: 5.625797, loss surf: 0.000036\n",
      "Train Epoch: 840, Loss: 5.127016, loss bulk: 5.126853, loss surf: 0.000163\n",
      "Train Epoch: 841, Loss: 4.346981, loss bulk: 4.346763, loss surf: 0.000218\n",
      "Train Epoch: 842, Loss: 4.459083, loss bulk: 4.458925, loss surf: 0.000158\n",
      "Train Epoch: 843, Loss: 5.606823, loss bulk: 5.605539, loss surf: 0.001283\n",
      "Train Epoch: 844, Loss: 4.445910, loss bulk: 4.444986, loss surf: 0.000924\n",
      "Train Epoch: 845, Loss: 4.241469, loss bulk: 4.241461, loss surf: 0.000008\n",
      "Train Epoch: 846, Loss: 4.531859, loss bulk: 4.531041, loss surf: 0.000819\n",
      "Train Epoch: 847, Loss: 5.287474, loss bulk: 5.286900, loss surf: 0.000573\n",
      "Train Epoch: 848, Loss: 4.462432, loss bulk: 4.462395, loss surf: 0.000037\n",
      "Train Epoch: 849, Loss: 4.340089, loss bulk: 4.339973, loss surf: 0.000116\n",
      "Train Epoch: 850, Loss: 4.584419, loss bulk: 4.584235, loss surf: 0.000184\n",
      "Train Epoch: 851, Loss: 4.823932, loss bulk: 4.823784, loss surf: 0.000147\n",
      "Train Epoch: 852, Loss: 4.241988, loss bulk: 4.241856, loss surf: 0.000132\n",
      "Train Epoch: 853, Loss: 4.296639, loss bulk: 4.296630, loss surf: 0.000010\n",
      "Train Epoch: 854, Loss: 4.533450, loss bulk: 4.533279, loss surf: 0.000171\n",
      "Train Epoch: 855, Loss: 4.093779, loss bulk: 4.093323, loss surf: 0.000456\n",
      "Train Epoch: 856, Loss: 4.302886, loss bulk: 4.302776, loss surf: 0.000110\n",
      "Train Epoch: 857, Loss: 4.413920, loss bulk: 4.413703, loss surf: 0.000217\n",
      "Train Epoch: 858, Loss: 4.450136, loss bulk: 4.449361, loss surf: 0.000775\n",
      "Train Epoch: 859, Loss: 4.433747, loss bulk: 4.433581, loss surf: 0.000166\n",
      "Train Epoch: 860, Loss: 3.941377, loss bulk: 3.941119, loss surf: 0.000258\n",
      "Train Epoch: 861, Loss: 4.235786, loss bulk: 4.235419, loss surf: 0.000367\n",
      "Train Epoch: 862, Loss: 4.264592, loss bulk: 4.264586, loss surf: 0.000006\n",
      "Train Epoch: 863, Loss: 4.104024, loss bulk: 4.103942, loss surf: 0.000082\n",
      "Train Epoch: 864, Loss: 4.895115, loss bulk: 4.895070, loss surf: 0.000045\n",
      "Train Epoch: 865, Loss: 4.428181, loss bulk: 4.427917, loss surf: 0.000264\n",
      "Train Epoch: 866, Loss: 5.001945, loss bulk: 5.001920, loss surf: 0.000025\n",
      "Train Epoch: 867, Loss: 5.129171, loss bulk: 5.128371, loss surf: 0.000800\n",
      "Train Epoch: 868, Loss: 4.506504, loss bulk: 4.505200, loss surf: 0.001304\n",
      "Train Epoch: 869, Loss: 4.330386, loss bulk: 4.330348, loss surf: 0.000038\n",
      "Train Epoch: 870, Loss: 4.386933, loss bulk: 4.386044, loss surf: 0.000889\n",
      "Train Epoch: 871, Loss: 4.753306, loss bulk: 4.752517, loss surf: 0.000789\n",
      "Train Epoch: 872, Loss: 4.606367, loss bulk: 4.606355, loss surf: 0.000012\n",
      "Train Epoch: 873, Loss: 4.558524, loss bulk: 4.558436, loss surf: 0.000088\n",
      "Train Epoch: 874, Loss: 4.589502, loss bulk: 4.589490, loss surf: 0.000012\n",
      "Train Epoch: 875, Loss: 4.305670, loss bulk: 4.305536, loss surf: 0.000134\n",
      "Train Epoch: 876, Loss: 4.363004, loss bulk: 4.362999, loss surf: 0.000004\n",
      "Train Epoch: 877, Loss: 3.726348, loss bulk: 3.726221, loss surf: 0.000127\n",
      "Train Epoch: 878, Loss: 4.409386, loss bulk: 4.409235, loss surf: 0.000151\n",
      "Train Epoch: 879, Loss: 4.643898, loss bulk: 4.643723, loss surf: 0.000175\n",
      "Train Epoch: 880, Loss: 4.258059, loss bulk: 4.257951, loss surf: 0.000107\n",
      "Train Epoch: 881, Loss: 3.931380, loss bulk: 3.931378, loss surf: 0.000002\n",
      "Train Epoch: 882, Loss: 4.055791, loss bulk: 4.055725, loss surf: 0.000066\n",
      "Train Epoch: 883, Loss: 4.230463, loss bulk: 4.230343, loss surf: 0.000120\n",
      "Train Epoch: 884, Loss: 4.166781, loss bulk: 4.166734, loss surf: 0.000047\n",
      "Train Epoch: 885, Loss: 4.224937, loss bulk: 4.224928, loss surf: 0.000008\n",
      "Train Epoch: 886, Loss: 4.035287, loss bulk: 4.035190, loss surf: 0.000097\n",
      "Train Epoch: 887, Loss: 4.189965, loss bulk: 4.189943, loss surf: 0.000022\n",
      "Train Epoch: 888, Loss: 4.482669, loss bulk: 4.482660, loss surf: 0.000009\n",
      "Train Epoch: 889, Loss: 3.949506, loss bulk: 3.949474, loss surf: 0.000032\n",
      "Train Epoch: 890, Loss: 4.137471, loss bulk: 4.137360, loss surf: 0.000111\n",
      "Train Epoch: 891, Loss: 4.011703, loss bulk: 4.011701, loss surf: 0.000003\n",
      "Train Epoch: 892, Loss: 4.015728, loss bulk: 4.015698, loss surf: 0.000030\n",
      "Train Epoch: 893, Loss: 3.928819, loss bulk: 3.928795, loss surf: 0.000024\n",
      "Train Epoch: 894, Loss: 3.844081, loss bulk: 3.844061, loss surf: 0.000020\n",
      "Train Epoch: 895, Loss: 3.891001, loss bulk: 3.890995, loss surf: 0.000006\n",
      "Train Epoch: 896, Loss: 3.808092, loss bulk: 3.808003, loss surf: 0.000088\n",
      "Train Epoch: 897, Loss: 3.529084, loss bulk: 3.529042, loss surf: 0.000043\n",
      "Train Epoch: 898, Loss: 3.758760, loss bulk: 3.758733, loss surf: 0.000027\n",
      "Train Epoch: 899, Loss: 3.431679, loss bulk: 3.431672, loss surf: 0.000007\n",
      "Train Epoch: 900, Loss: 3.553523, loss bulk: 3.553520, loss surf: 0.000003\n",
      "Train Epoch: 901, Loss: 3.375926, loss bulk: 3.375914, loss surf: 0.000012\n",
      "Train Epoch: 902, Loss: 3.602606, loss bulk: 3.602590, loss surf: 0.000016\n",
      "Train Epoch: 903, Loss: 3.365388, loss bulk: 3.365363, loss surf: 0.000025\n",
      "Train Epoch: 904, Loss: 3.595357, loss bulk: 3.595346, loss surf: 0.000011\n",
      "Train Epoch: 905, Loss: 3.727400, loss bulk: 3.727338, loss surf: 0.000062\n",
      "Train Epoch: 906, Loss: 3.639261, loss bulk: 3.639189, loss surf: 0.000072\n",
      "Train Epoch: 907, Loss: 3.597383, loss bulk: 3.597360, loss surf: 0.000023\n",
      "Train Epoch: 908, Loss: 3.634455, loss bulk: 3.634403, loss surf: 0.000053\n",
      "Train Epoch: 909, Loss: 3.604814, loss bulk: 3.604765, loss surf: 0.000049\n",
      "Train Epoch: 910, Loss: 3.655138, loss bulk: 3.655127, loss surf: 0.000011\n",
      "Train Epoch: 911, Loss: 3.579416, loss bulk: 3.579285, loss surf: 0.000131\n",
      "Train Epoch: 912, Loss: 3.601884, loss bulk: 3.601844, loss surf: 0.000040\n",
      "Train Epoch: 913, Loss: 3.574218, loss bulk: 3.574118, loss surf: 0.000099\n",
      "Train Epoch: 914, Loss: 3.602888, loss bulk: 3.602844, loss surf: 0.000045\n",
      "Train Epoch: 915, Loss: 3.555123, loss bulk: 3.555007, loss surf: 0.000117\n",
      "Train Epoch: 916, Loss: 3.549287, loss bulk: 3.549173, loss surf: 0.000114\n",
      "Train Epoch: 917, Loss: 3.571478, loss bulk: 3.571401, loss surf: 0.000077\n",
      "Train Epoch: 918, Loss: 3.566400, loss bulk: 3.566229, loss surf: 0.000171\n",
      "Train Epoch: 919, Loss: 3.549125, loss bulk: 3.549081, loss surf: 0.000044\n",
      "Train Epoch: 920, Loss: 3.557747, loss bulk: 3.557617, loss surf: 0.000130\n",
      "Train Epoch: 921, Loss: 3.543376, loss bulk: 3.543165, loss surf: 0.000212\n",
      "Train Epoch: 922, Loss: 3.557428, loss bulk: 3.557248, loss surf: 0.000180\n",
      "Train Epoch: 923, Loss: 3.530514, loss bulk: 3.530231, loss surf: 0.000282\n",
      "Train Epoch: 924, Loss: 3.531931, loss bulk: 3.531648, loss surf: 0.000283\n",
      "Train Epoch: 925, Loss: 3.526711, loss bulk: 3.526221, loss surf: 0.000489\n",
      "Train Epoch: 926, Loss: 3.531003, loss bulk: 3.530386, loss surf: 0.000617\n",
      "Train Epoch: 927, Loss: 3.529347, loss bulk: 3.528430, loss surf: 0.000917\n",
      "Train Epoch: 928, Loss: 3.522914, loss bulk: 3.521595, loss surf: 0.001318\n",
      "Train Epoch: 929, Loss: 3.517973, loss bulk: 3.516181, loss surf: 0.001792\n",
      "Train Epoch: 930, Loss: 3.517752, loss bulk: 3.515336, loss surf: 0.002417\n",
      "Train Epoch: 931, Loss: 3.525802, loss bulk: 3.522055, loss surf: 0.003747\n",
      "Train Epoch: 932, Loss: 3.507731, loss bulk: 3.502627, loss surf: 0.005104\n",
      "Train Epoch: 933, Loss: 3.526415, loss bulk: 3.520017, loss surf: 0.006398\n",
      "Train Epoch: 934, Loss: 3.517989, loss bulk: 3.510736, loss surf: 0.007253\n",
      "Train Epoch: 935, Loss: 3.509053, loss bulk: 3.501233, loss surf: 0.007820\n",
      "Train Epoch: 936, Loss: 3.514297, loss bulk: 3.507227, loss surf: 0.007070\n",
      "Train Epoch: 937, Loss: 3.499444, loss bulk: 3.494708, loss surf: 0.004736\n",
      "Train Epoch: 938, Loss: 3.498703, loss bulk: 3.496537, loss surf: 0.002165\n",
      "Train Epoch: 939, Loss: 3.496147, loss bulk: 3.495701, loss surf: 0.000446\n",
      "Train Epoch: 940, Loss: 3.482279, loss bulk: 3.482224, loss surf: 0.000055\n",
      "Train Epoch: 941, Loss: 3.484910, loss bulk: 3.484072, loss surf: 0.000838\n",
      "Train Epoch: 942, Loss: 3.484551, loss bulk: 3.482428, loss surf: 0.002124\n",
      "Train Epoch: 943, Loss: 3.477092, loss bulk: 3.474132, loss surf: 0.002960\n",
      "Train Epoch: 944, Loss: 3.481407, loss bulk: 3.478500, loss surf: 0.002907\n",
      "Train Epoch: 945, Loss: 3.481176, loss bulk: 3.479380, loss surf: 0.001796\n",
      "Train Epoch: 946, Loss: 3.475881, loss bulk: 3.475179, loss surf: 0.000702\n",
      "Train Epoch: 947, Loss: 3.477272, loss bulk: 3.477201, loss surf: 0.000071\n",
      "Train Epoch: 948, Loss: 3.474538, loss bulk: 3.474419, loss surf: 0.000119\n",
      "Train Epoch: 949, Loss: 3.471070, loss bulk: 3.470342, loss surf: 0.000728\n",
      "Train Epoch: 950, Loss: 3.471445, loss bulk: 3.470167, loss surf: 0.001277\n",
      "Train Epoch: 951, Loss: 3.467980, loss bulk: 3.466640, loss surf: 0.001340\n",
      "Train Epoch: 952, Loss: 3.466964, loss bulk: 3.465911, loss surf: 0.001053\n",
      "Train Epoch: 953, Loss: 3.466964, loss bulk: 3.466434, loss surf: 0.000530\n",
      "Train Epoch: 954, Loss: 3.464575, loss bulk: 3.464472, loss surf: 0.000102\n",
      "Train Epoch: 955, Loss: 3.464945, loss bulk: 3.464929, loss surf: 0.000017\n",
      "Train Epoch: 956, Loss: 3.465080, loss bulk: 3.464845, loss surf: 0.000235\n",
      "Train Epoch: 957, Loss: 3.462848, loss bulk: 3.462313, loss surf: 0.000536\n",
      "Train Epoch: 958, Loss: 3.462297, loss bulk: 3.461606, loss surf: 0.000690\n",
      "Train Epoch: 959, Loss: 3.461592, loss bulk: 3.461003, loss surf: 0.000589\n",
      "Train Epoch: 960, Loss: 3.459645, loss bulk: 3.459278, loss surf: 0.000368\n",
      "Train Epoch: 961, Loss: 3.459291, loss bulk: 3.459162, loss surf: 0.000129\n",
      "Train Epoch: 962, Loss: 3.458585, loss bulk: 3.458582, loss surf: 0.000003\n",
      "Train Epoch: 963, Loss: 3.457293, loss bulk: 3.457241, loss surf: 0.000052\n",
      "Train Epoch: 964, Loss: 3.457110, loss bulk: 3.456934, loss surf: 0.000175\n",
      "Train Epoch: 965, Loss: 3.456685, loss bulk: 3.456400, loss surf: 0.000285\n",
      "Train Epoch: 966, Loss: 3.455837, loss bulk: 3.455504, loss surf: 0.000333\n",
      "Train Epoch: 967, Loss: 3.455468, loss bulk: 3.455205, loss surf: 0.000263\n",
      "Train Epoch: 968, Loss: 3.454634, loss bulk: 3.454490, loss surf: 0.000144\n",
      "Train Epoch: 969, Loss: 3.453900, loss bulk: 3.453859, loss surf: 0.000041\n",
      "Train Epoch: 970, Loss: 3.453131, loss bulk: 3.453130, loss surf: 0.000001\n",
      "Train Epoch: 971, Loss: 3.452444, loss bulk: 3.452421, loss surf: 0.000023\n",
      "Train Epoch: 972, Loss: 3.451981, loss bulk: 3.451903, loss surf: 0.000078\n",
      "Train Epoch: 973, Loss: 3.451888, loss bulk: 3.451754, loss surf: 0.000134\n",
      "Train Epoch: 974, Loss: 3.451107, loss bulk: 3.450958, loss surf: 0.000149\n",
      "Train Epoch: 975, Loss: 3.450271, loss bulk: 3.450148, loss surf: 0.000123\n",
      "Train Epoch: 976, Loss: 3.449926, loss bulk: 3.449852, loss surf: 0.000075\n",
      "Train Epoch: 977, Loss: 3.449221, loss bulk: 3.449187, loss surf: 0.000034\n",
      "Train Epoch: 978, Loss: 3.448641, loss bulk: 3.448635, loss surf: 0.000006\n",
      "Train Epoch: 979, Loss: 3.448263, loss bulk: 3.448261, loss surf: 0.000002\n",
      "Train Epoch: 980, Loss: 3.447756, loss bulk: 3.447735, loss surf: 0.000021\n",
      "Train Epoch: 981, Loss: 3.447484, loss bulk: 3.447444, loss surf: 0.000040\n",
      "Train Epoch: 982, Loss: 3.446897, loss bulk: 3.446836, loss surf: 0.000061\n",
      "Train Epoch: 983, Loss: 3.446422, loss bulk: 3.446358, loss surf: 0.000064\n",
      "Train Epoch: 984, Loss: 3.445980, loss bulk: 3.445917, loss surf: 0.000063\n",
      "Train Epoch: 985, Loss: 3.445415, loss bulk: 3.445376, loss surf: 0.000040\n",
      "Train Epoch: 986, Loss: 3.445114, loss bulk: 3.445090, loss surf: 0.000024\n",
      "Train Epoch: 987, Loss: 3.444411, loss bulk: 3.444403, loss surf: 0.000008\n",
      "Train Epoch: 988, Loss: 3.444134, loss bulk: 3.444132, loss surf: 0.000001\n",
      "Train Epoch: 989, Loss: 3.443430, loss bulk: 3.443429, loss surf: 0.000002\n",
      "Train Epoch: 990, Loss: 3.443011, loss bulk: 3.443003, loss surf: 0.000007\n",
      "Train Epoch: 991, Loss: 3.442807, loss bulk: 3.442790, loss surf: 0.000017\n",
      "Train Epoch: 992, Loss: 3.442272, loss bulk: 3.442249, loss surf: 0.000024\n",
      "Train Epoch: 993, Loss: 3.442058, loss bulk: 3.442032, loss surf: 0.000027\n",
      "Train Epoch: 994, Loss: 3.441338, loss bulk: 3.441314, loss surf: 0.000024\n",
      "Train Epoch: 995, Loss: 3.440200, loss bulk: 3.440177, loss surf: 0.000023\n",
      "Train Epoch: 996, Loss: 3.442080, loss bulk: 3.442064, loss surf: 0.000016\n",
      "Train Epoch: 997, Loss: 3.426644, loss bulk: 3.426628, loss surf: 0.000016\n",
      "Train Epoch: 998, Loss: 3.428416, loss bulk: 3.428408, loss surf: 0.000007\n",
      "Train Epoch: 999, Loss: 3.440208, loss bulk: 3.440203, loss surf: 0.000005\n",
      "Elapsed time: 322.118964 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "local_min = 100.0\n",
    "for j in range(epochs):\n",
    "    loss_bulk = torch.zeros(1)\n",
    "    loss_surf = torch.zeros(1)\n",
    "\n",
    "    for point in bulk_set:\n",
    "        x = torch.tensor([point+ 0.5*h])\n",
    "        loss_bulk += h*model.loss_function_bulk(x)\n",
    "               \n",
    "    for point in surf_set:\n",
    "        x = torch.tensor([point])\n",
    "        loss_surf += model.loss_function_surf(x)\n",
    "\n",
    "    # record each loss\n",
    "    loss_bulk_record.append(loss_bulk.data[0])\n",
    "    loss_surf_record.append(loss_surf.data[0])\n",
    "    \n",
    "\n",
    "    \n",
    "    loss = loss_bulk + loss_surf\n",
    "    if loss.item() < local_min:\n",
    "        print('updating the parameters')\n",
    "        local_min = loss.item()\n",
    "        torch.save(model.state_dict(),'./24_14/LeakyReLU/poisson_equation_leaky')\n",
    "    print('Train Epoch: {}, Loss: {:.6f}, loss bulk: {:.6f}, loss surf: {:.6f}'.format(j, loss.item(), loss_bulk.item(), loss_surf.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "toc()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./24_14/LeakyReLU/poisson_equation_leaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1: 5.184605, H1_rel: 0.105280, L2: 0.001132, L2_rel: 0.027462, boundary: 0.000005\n",
      "u: H1 : 0.038735, H1_rel: 0.160653, L2: 0.000030, L2_rel: 0.045997, H1_semi: 0.038705, H1_semi_rel: 0.161357\n",
      "sigma: H1: 5.145870, H1_rel: 0.105055, L2: 0.001102, L2_rel: 0.027223, H1_semi: 5.144768, H1_semi_rel: 0.105212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu_err_h1 = torch.zeros(1)\n",
    "sigma_err_h1 = torch.zeros(1)\n",
    "bdd_err = torch.zeros(1)\n",
    "mu_err_l2 = torch.zeros(1)\n",
    "sigma_err_l2 = torch.zeros(1)\n",
    "    \n",
    "for point in test_set:\n",
    "\n",
    "    x = torch.tensor([point+ 0.5*dx])\n",
    "    mu, sigma = model(x)\n",
    "    mu_grad, sigma_grad = model.net_grad(x)\n",
    "\n",
    "    # esitmate H1 norm error\n",
    "    mu_diff = (mu - u_exact(x))**2  + (mu_grad + sigma_exact(x))**2\n",
    "    sigma_diff = (sigma - sigma_exact(x))**2 + (sigma_grad - f(x))**2\n",
    "    mu_err_h1 += dx*mu_diff\n",
    "    sigma_err_h1 += dx*sigma_diff\n",
    "    \n",
    "    # estimate L2 norm error\n",
    "    mu_err_l2 += dx*(mu - u_exact(x))**2\n",
    "    sigma_err_l2 += dx*(sigma - sigma_exact(x))**2\n",
    "    \n",
    "    # estimate H1 semi norm  error\n",
    "    mu_err_semi    = mu_err_h1 - mu_err_l2\n",
    "    sigma_err_semi = sigma_err_h1 - sigma_err_l2\n",
    "\n",
    "H1_err = mu_err_h1 + sigma_err_h1  \n",
    "H1_err_relative = ((H1_err)**(1/2)) / ((u_h1 + sigma_h1)**(1/2))\n",
    "L2_err = mu_err_l2 + sigma_err_l2\n",
    "L2_err_relative = ((L2_err)**(1/2)) / ((u_l2 + sigma_l2)**(1/2))\n",
    "mu_err_h1_relative = (mu_err_h1/u_h1)**(1/2)\n",
    "mu_err_l2_relative = (mu_err_l2/u_l2)**(1/2)\n",
    "mu_err_semi_relative = (mu_err_semi/(sigma_l2))**(1/2)\n",
    "sigma_err_h1_relative = (sigma_err_h1/sigma_h1)**(1/2)\n",
    "sigma_err_l2_relative = (sigma_err_l2/sigma_l2)**(1/2)\n",
    "sigma_err_semi_relative = (sigma_err_semi/(sigma_h1 - sigma_l2))**(1/2)\n",
    "bdd_err += abs(mu - g(x))\n",
    "\n",
    "\n",
    "#     print('u_prime: {:.6f}, sigma_prime: {:.6f}, u_prime_max: {:.6f}, sigma_prime_max: {:.6f} '.format(difference1.item(), difference2.item(), diff1_max.item(), diff2_max.item()))\n",
    "print('H1: {:.6f}, H1_rel: {:.6f}, L2: {:.6f}, L2_rel: {:.6f}, boundary: {:.6f}'.format(H1_err.item(), H1_err_relative.item(), L2_err.item(), L2_err_relative.item(), bdd_err.item()))\n",
    "print('u: H1 : {:.6f}, H1_rel: {:.6f}, L2: {:.6f}, L2_rel: {:.6f}, H1_semi: {:.6f}, H1_semi_rel: {:.6f}'.format(mu_err_h1.item(), mu_err_h1_relative.item(), mu_err_l2.item(), mu_err_l2_relative.item(),                                                                                                              mu_err_semi.item(), mu_err_semi_relative.item()))\n",
    "print('sigma: H1: {:.6f}, H1_rel: {:.6f}, L2: {:.6f}, L2_rel: {:.6f}, H1_semi: {:.6f}, H1_semi_rel: {:.6f}\\n'.format(sigma_err_h1.item(), sigma_err_h1_relative.item(), sigma_err_l2.item(), sigma_err_l2_relative.item(),\n",
    "                                                                                                                    sigma_err_semi.item(), sigma_err_semi_relative.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = test_set\n",
    "yt = np.zeros_like(points)\n",
    "y_diff = np.zeros_like(points)\n",
    "ymu = np.zeros_like(points)\n",
    "ysig = np.zeros_like(points)\n",
    "for i in range(len(points)):\n",
    "    yt[i] = u_exact(points[i])\n",
    "    y_diff[i] =  -exp(-(points[i]-1/3)**2/0.01)*(-200*points[i]**2 + (200/3)*points[i] + 1)\n",
    "    ymu[i], ysig[i] = model(torch.tensor([points[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x272b7da8048>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(points, yt, color = 'b', label = 'u_true')\n",
    "plt.plot(points, ymu, color = 'r', label = 'u_approximation')\n",
    "plt.legend()\n",
    "# plt.savefig('./24_14/mu.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x272b1687e80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(points, y_diff, color = 'b', label = 'sigma_true')\n",
    "plt.plot(points, ysig, color = 'r', label = 'sigma_approximation')\n",
    "plt.legend()\n",
    "# plt.savefig('./24_14/sigma.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x272b7de8588>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deZgcVbn48e/b3bMkkz2ZQEKAsASEEBNg2LkIQkJAZBEQomBYNCAiysWfIHhBr16vCl5Q4QrBBBAh4EX2hCWCyE4yAbID2cmeyT6TWbv7/f1RNTPdPdXT09M9S9W8n+fpZ6pOneo6NdXzzulTp84RVcUYY0xwhbq6AMYYYzqWBXpjjAk4C/TGGBNwFuiNMSbgLNAbY0zARbq6AF6GDBmiI0eO7OpiGGOMb8ybN2+rqpZ6beuWgX7kyJGUl5d3dTGMMcY3RGRNum3WdGOMMQFngd4YYwLOAr0xxgScBXpjjAk4C/TGGBNwFuiNMSbgLNAbY0zAWaA3xmfi8Ti/+NfDVNfXdnVRjE9YoDfGZx4on8nfVv+OK5/7eVcXxfiEBXpjfEbcB9o/r1rexSUxfmGB3hifGVoyAIAa3dHFJTF+YYHeGJ9RnOk/Y1R3cUmMX1igN8ZnNG7zPJvsWKA3xrcs4Ju2sUBvjM/EmwK8BXrTNhnHoxeR6cA5wBZVPcJNexI41M0yANipquM89l0NVAIxIKqqZXkqtzE9lqoFepOdtkw88jBwL/CXxgRVvaRxWUR+B+xqZf/TVHVrewtojEmmFuBNljIGelV9U0RGem0TEQG+Dnw5v8UyxqRjNXqTrVzb6P8N2Kyqy9JsV+BVEZknIlNaeyMRmSIi5SJSXlFRkWOxjAmuuAV4k6VcA/0kYEYr209S1aOAs4Dvicgp6TKq6lRVLVPVstJSz/ltjTFYjd5kr92BXkQiwNeAJ9PlUdUN7s8twDPAse09njHGoRbfTZZyqdGfAXyiquu8NopIiYj0bVwGJgCLcjieMQZorMmrdHExjG9kDPQiMgN4DzhURNaJyNXupktJabYRkeEiMstd3Qt4W0TmA3OAmar6cv6KbkzPFLemG5OltvS6mZQm/QqPtA3A2e7ySmBsjuUzxqSwNnqTLXsy1hifUXsy1mTJAr0xPqN2N9ZkyQK9MT5jNXqTLQv0xviMjVJssmWB3hjfsohv2sYCvTE+o8SdBbFAb9rGAr0xPmM3Y022LNAb4zPxpiUL+KZtLNAb4zOq8cyZjElggd4Yn2lsuhFRnl/6XheXxviBBXpjfCaxwea2Oa1O82AMYIHeGGMCzwK9MT5jbfQmWxbojfEZ611psmWB3hifsTljTbYs0BvjM/bAlMmWBXpjfMYCvclWW6YSnC4iW0RkUULaz0RkvYh87L7OTrPvRBH5VESWi8gt+Sy4MT2VWtONyVJbavQPAxM90u9W1XHua1bqRhEJA/cBZwGHA5NE5PBcCmuMsUBvspcx0Kvqm8D2drz3scByVV2pqvXAE8B57XgfY0wCa7kx2cqljf56EVngNu0M9Ni+D7A2YX2dm2aMyYlFepOd9gb6PwEHAeOAjcDvPPKIR1raT6iITBGRchEpr6ioaGexjAk+uxlrstWuQK+qm1U1ps4jeg/iNNOkWgfsm7A+AtjQyntOVdUyVS0rLS1tT7GM6RGsH73JVrsCvYgMS1i9AFjkkW0uMEpEDhCRQuBS4Pn2HM8Y08wq9CZbkUwZRGQGcCowRETWAXcAp4rIOJymmNXANW7e4cCfVfVsVY2KyPXAK0AYmK6qizvkLIzpQRQb68ZkJ2OgV9VJHsnT0uTdAJydsD4LaNH10hjTftZGb7JlT8YaY0zAWaA3xmesPm+yZYHeGJ+J23j0JksW6I0xJuAs0BvjM3Yz1mTLAr0xPmODmplsWaA3xmesRm+yZYHeGGMCzgK9MT4Ttxq9yZIFemN8xtroTbYs0BvjM9ZGb7Jlgd4Yn7EavcmWBXpj/MbivMmSBXpjfCZuwxSbLFmgN8aYgLNAb4zP2L1Yky0L9Mb4jN2MNdmyQG+Mz1igN9nKGOhFZLqIbBGRRQlpd4rIJyKyQESeEZEBafZdLSILReRjESnPZ8GN6amsH73JVltq9A8DE1PSZgNHqOoXgc+An7Sy/2mqOk5Vy9pXRGNMotQa/XtrF6XJaYwjY6BX1TeB7Slpr6pq1F19HxjRAWUzxnhIrdBPeX1S1xTE+EY+2uivAl5Ks02BV0VknohMycOxjDFp+tFHYzE+31nRyWUxfpBToBeR24Ao8FiaLCep6lHAWcD3ROSUVt5rioiUi0h5RYV9WI3J1lXP/Tdfee7LfFqxoauLYrqZdgd6EZkMnAN8U9PcHVLVDe7PLcAzwLHp3k9Vp6pqmaqWlZaWtrdYxgReuudiF+54C4AV2zd2XmGML7Qr0IvIROBm4FxVrU6Tp0RE+jYuAxMAu2tkTI4y9boJiXRSSYxftKV75QzgPeBQEVknIlcD9wJ9gdlu18n73bzDRWSWu+tewNsiMh+YA8xU1Zc75CyM6UHSB3onXSzQmxSRTBlU1euW/rQ0eTcAZ7vLK4GxOZXOGNNCpgemQligN8nsyVhjfCZTjd6YVBbojfGZdDX6xlRrujGpLNAbEzA2RIJJZYHeGJ/J1HQTU5uYxCSzQG9MYDiBviEW6+JymO7GAr0xPpOp101MLdCbZBbojfGZTIE+ajV6k8ICvTE+49VGH4/Hm/4BRK1Gb1JYoDfGZ7xq9LWxhqblaNwCvUlmgd6YAKiPNgf6WNx63ZhkFuiN8Rmv3pW10QYax7WMWY3epLBAb4zPqMdAxbUNCU031kZvUligN8ZnvGr0ddZGb1oRqEC/q24Xm/ds7upiGNPBPG7GRhuIR7YB1nRjWso4TLGfjH9qPDXRGhZOXtjVRTGmw3j1utlRU9W0HLWbsSZFoGr0NdGari6CMR3Oqx99ZV3zZz9uY92YFIEK9Mb0BF41+uqG2qZla7oxqSzQGxMAn2xb1bRsvW5MqjYFehGZLiJbRGRRQtogEZktIsvcnwPT7DvZzbNMRCbnq+DG9FRevW6e+vzOpmXrdWNStbVG/zAwMSXtFuA1VR0FvOauJxGRQcAdwHHAscAd6f4hGGPaJtOgZtZGb1K1KdCr6pvA9pTk84BH3OVHgPM9dj0TmK2q21V1BzCblv8w8u7Ex09k7qa5HX0YY7pEphmkrEZvUuXSRr+Xqm4EcH8O9cizD7A2YX2dm9aCiEwRkXIRKa+oqMihWFDZUMkD8x/I6T2M6b6sRm+y09E3Y71mKfb8lKrqVFUtU9Wy0tLSnA8cz/DHYIxfZZx4xPrRmxS5BPrNIjIMwP25xSPPOmDfhPURwIYcjtlma7ZWd8ZhjOl0NsOUyVYugf55oLEXzWTgOY88rwATRGSgexN2gpvW4fbURzvjMMZ0ugxN9MQt0JsUbe1eOQN4DzhURNaJyNXAr4HxIrIMGO+uIyJlIvJnAFXdDvwCmOu+/tNNM8a0k2Zog7chEEyqNo11o6qT0mw63SNvOfDthPXpwPR2lc4Y04I13Zhs2ZOxxvhMpkCfqcZvep7ABvpM7ZjG+FXmGr0FepMssIEeYHPVLsY8MoZbZ0/r6qIYkzeZHpiyphuTKtCBfukW51mtWZ8/3sUlMSZ/bAgEk61AB/riSAEAitVwTHCk1uiPG3BZ0ro9MGVSBTrQF4SdTkUqFuhNcEVCyZ3nrB+9SRXgQK80jrZgNXoTFJ/vqOCz2plJaWEJJ63HsRq9SRbgQA/RWONX3BiV9ZVdWhZj8uHdtUtbpBWEC5LW49Z0Y1IENtBXhz9l6pK7nZVwNSfOOJEZC17r2kIZkyOvrpNhSf4zjtk3WJMisIEeYM7mt5LWX1nxfheVxJj88OpaGZHUNnqr0ZtkgQ70LXmNmmyMf3j1qImEUtrorenGpOhRgd7CvPE7zxp9aqC3m7EmRY8K9Bbqjd95NcuEUwO9da80KXpUoBeL88bnvJ6KLQg197pRDVkbvWmhRwV6Y/wunqnpRiOoNd2YFBbojfERr9p6YqAXjViN3rTQowK9WBu98TmvEJ7cj94J9Fc9+yvOfuz6ziqW6ebaNMNUcFigN/7mNalI4r2nxhr93F0zOrFUprtrd41eRA4VkY8TXrtF5IcpeU4VkV0JeW7PvcjG9Fxe3SsloUYvhG1sJ9NCu2v0qvopMA5ARMLAeuAZj6xvqeo57T1OPll93vidV/u7JC0LWxtWgNtsH4/HCYV6VAut8ZCvT8DpwApVXZOn9+sYFumNz3lNLiVJ/YZDxMIVTWtbq53B/PbU1bGrtrqDS2e6q3wF+kuBdI2CJ4jIfBF5SURGp3sDEZkiIuUiUl5RUZEuW07sZqzxO69BzZKbbpL/pM/522Q2V+3khMdO5+Qnj+vw8pnuKedALyKFwLnA/3ls/hDYX1XHAn8Enk33Pqo6VVXLVLWstLQ012J5l9UCvfE5rzb6xD9iSRnJsia8gvF/OwcN7+rgkpnuLB81+rOAD1V1c+oGVd2tqlXu8iygQESG5OGYxvRI3jX65gpMY41e40WMLDjDWbYg3+PlI9BPIk2zjYjsLe6nUESOdY+3LQ/HbJfWp1Q2xg88et0k/BmH3P4VooX0LezXaaUy3VtOgV5EegPjgacT0q4VkWvd1YuARSIyH/gDcKl6fffsJPbEoPE7rxr9MSMObVoOizPujWgBBw04oNPKZbq3nAK9qlar6mBV3ZWQdr+q3u8u36uqo1V1rKoer6rv5lrgXNiofsbvYvHkz/Apg7/NuGEjueHwOzl96LVNgT5EhNu+9A0AwtGhnV5O0730qCdjYxbojc+l1ugbhz/4zjETATjlkTkAhKSQ4oJCBuhY9sh2e4Sqh7NAb4yPZGp+jEgBKIRwavZhKUC1oTOK1qleWPoBC7cs57gRoykKRzh55BFdXaRuLVCBPt7Qj1DBbmK1exMu3tRyu02xZnwuGm+9shIJFUAMwu48spFQAXGinVG0TnXrnG8DMGO1s75w5MIuK4sfBOrZaI31omH3aNI9Ams1euN3qZWV1IlICkKFAIQpbFpXCV6N3mQnUIE+Ewv0xu+iGT7DTYHenXUqEipAW6nRP7fkA375xuP5K6DplgLVdIMoIBSExXPc7mjcajbG3zJVVgrdQB9xe98USCEq0bTPhP90rtME8lO+kbcymu4nUIG+KBKif1Exu2Pegb4hXt/pZTImn7z60ScqDDcGerfpJlwAAWyjN9kJVNONqrqPg3s/kxVV+8Abf0vtR5+qMdCH3G6XhaFCJGRNlj1doAI9aKsDl1nTjfG7tgb65vWijiyO8YmABXoAadEToVEsgP2JTc8S92yUbFaUEtiLUgL/L974Kze9/L8t9qttsGbNIAtUG32mYcss0Bu/i6V2r0wZOqo4JdAXRpID/d/W/MZduo7V27c0pe9pqKO4IDlvd7SrttpzqOYZ8//Fqp0buPVLk7qgVN1foGr0Cq220VfFNzDlxV94flCM8YNM4zWFU6YNLAqlD94XPHN503JlbU1uBeskJ8/4Eic/eUKL9F99fD0zVv+qC0rkD4EK9Jna6GPhbby37W+Ur1/eiWUyJn+yHYG1OOId6Ksb6ohGNjSt72moy6lcnSZUi0j6ilo0ZjeevQQr0Lv96DM14WzdYxMxGH/K1L0yVVGaQL87Zf7YqrradpepO1mxfZMNdeIhWIFe2xLmYX1ll819YkxOUptuLj78jFbz94oUe6Z/d+Z/Ja1XRzsv0H+2dQOVdR3TVHTRrIncPHtqh7y3nwUr0LexRv/2uvd4oDzt9LXGdBsrt29m/sbVTeuJgX7h5IV86cDRre5fXODdvXJ53StJ651Vo4/H41w480zOeeKarPZbvPlzjp5+dpvyvrnh1fYULdACFeidMJ95AvB5u/7OvYv/o+MLZEyOzn3uTC579atN69k23fQKt60nzdbqzmnO3O3W5LfzUVb73fnOo9SH13ZEkXqEQAV6tLHtxnrVmGBIfao1083YLx9wDABnHjAegD6FvTzz9YodnLResWd7e4uYlY2VOwBQzVwhS9S/qO3z31aHlvH4/Deyev+gyznQi8hqEVkoIh+LSLnHdhGRP4jIchFZICJH5XrM9IVxet2o2J13EyyNNxgzBfozDv4i5d/8iH8/6QIABvTq45nvhiNvSFrfWrMjD6XMbHPVTmdBw1ntN7BX/6zy//fH388qf9Dlq0Z/mqqOU9Uyj21nAaPc1xTgT3k6pidB0DZOnGZ3541fbHQDZFuG2i6KND8HOdAj0J806AouO/K0pLTttTtzLGHbbNnjHEe0IKv9Up/4bY9jp5/P+U/8KOf38aPOaLo5D/iLOt4HBojIsI45lNNK39ZAX22PfRuf2LDbaVrRLNvoB/fq2yKtQFoG2R21O/h8R0X7CpeFbTW73aXsavQNOY5TVVlXQ014BStSbkL3FPkI9Aq8KiLzRGSKx/Z9gMS7KOvctCQiMkVEykWkvKKi/R84ESDDeCCNqur98TSgMY1j0WQ7ec7gkpaBPuQ+Pbtw8kIWTl6Ixov4rHYmX3n+yxz30IW5F7YVdVHvytVbq5Ywb/0Kz20bdm9n7qZ5OR33wbkv5bS/3+Uj0J+kqkfhNNF8T0ROSdnuddelxd1SVZ2qqmWqWlZaWtrOorhPxraxjT4oD4mY4KuPOTVabWMlplFJQvdKiQ0AoDalz7yEmp+KrQ591qJJ8901Sylfl5+nyeubAn1yCLjuzUu44h/nt8i/cvtmznzmS6yu/0fWx7r11WlNyw+tuCPr/YMk50Cvqhvcn1uAZ4BjU7KsA/ZNWB8BbKADaFPTTdv+GDbu2ZL1I+XGdIW6mDOXQjzLHmWhhLFvxvQ/HYDqhta/yVZUVyatX/PG17nytQuyOm46tTHvQJ/O5c/f2O5jvbDxHs/0Yx46t93v6Vc5BXoRKRGRvo3LwARgUUq254Fvub1vjgd2qerGXI6boUxNNfrqz6+ges23qV57Bb1qT26R97o3LmPKCzYQkun+Gmv0mQY183LH0Q/w5Fkv0afAuTFbE0se/uDUId9JWm+8HwCwqTK/vXEaYo2T/7StglUb3505U5ZqQ6t4a9USxj96bdrmoqDJtUa/F/C2iMwH5gAzVfVlEblWRK5188wCVgLLgQeB63I8ZiucphsR50M065rLmfej77LipzdR2tf7Ln95xT87rjjG5Eld1G26acc30IuOOJHDh46gpKAEgNpoco3+j1+5gYWTFzatb0gYImRbdVV7iptWUxNUJ3WBfuTD1zzTr3vzEjbF3+HHr/+6U8rR1XIK9Kq6UlXHuq/Rqvpfbvr9qnq/u6yq+j1VPUhVx6hqi772HWXvvn0Z0Nt5MjD9fLH2cJXpnh6YM6tpuaahjv/4x0M5TYfZ+DDV8cOP89w+5ZBfAnDnB/excvtmwBnlstEtrz5IfTT747+7ZmnTt4SGuLO/hDL3ovm0Yj0xzW1UzbsW/pCLnrwl7fYwYW6bPZ0xj4zJ+7eX7iRYT8bSOGeso6Sw+UZUNE2gtzBvuqt7l97ctPzI4sd5dv3/UKEftPv9zvlCGa+c/yY/PdV7co4rjpoAwDYt58ZXfgs4/2Aazdz4B37yjwezOmZlXQ3XvPF1Lvy78wW/Ptb8d7ijuopPKtYl3fxNHGb4olkTiUU2ZXU8L5/Wzky7rSBcyIuf/wVwBlsLquAFeoT+1RcTj/YhEm4+vYsPvpJYXSnxuqFJe8TC2/j9u890dkGNycpWnZuX9xnef2DabX2LenHj6N8BUBXdTUM0mlSjB9ie5RO0sz51vsBX4vTaSfxGcvqMi7l41llN498AbHGHEK+oyr5tfmTh+Fa3940fDkAk2vwYz6balcQjTlNVVX1we+EFLNA7T8a+8K1beGxCcr/Z7574b8y7YjaPfeUhLtv/l835Jc6fl93OV2f8kHveeYanF7/b2UU2ptu4qmwCBbF92RJ/n/GPf4falH7vS3amb3ldumUd33vxnqQa+vrdzjMxok4T6oqdy5q2NUTWAbAtoZfP5ion0M/ftCrrsj950X8zrs/FANxw+J0ttjdoLe9fWs7rk55uSqsPr2laXrA5uBMSBSzQOw0xA0sKGbfvgBZbexWGGbfPftx86nn826Crk7atrn+Nactv547ya1ixLfevi8b4VVyd4L5Ny/lk65qkbdWhZV67APCtF7/Hm9umMXddc0+W7bWNo2I6f5ub4u+02G/JlubnKad9+DwAa3dtaZEvk94FRTx64e0snLyQ7xwzkUh0eNL2kEQoKSpiYG/v8X8eW/VLz/QgCFigB5G2ndL/fvWHTNz7es9tN77623wWqUtNL3+VnTV7uroYJkvXvXB3q9v767gOO3biECJewe+22dM996sNrwSSn97dtKcxYKe/G3brnKualv+17c8s2LSaHTWVafOnOmHgtzh+wOUt0u894x6O639Z0/q0s1r/nQI8ueCtNh/XTwIW6LO7tXrGAU7vgwMKz2T+5fN5f1I5gyljVf1sfjDrjx1RwE71zpql3L34JiY9fXPmzKZbeWu7dzAFKIztz9tXPNphxx4Q2a/V7c9vuJsxj4zhQrc3y+LNa3l9xYKm7bNXzOWTCqdZ5oOdf3USwzW8/NmHbTr+N1/5KjNXtX3IgrsmXM+D5/24RfpJ+x/Gn893PvtFsZEcsbf3eTXelwD45UfXBbL3TbACvbRt4pFGZx4yjhe++jrPXXonoVCIksIiBhQNAeD1Cv9PR9Y4JOy66L+67UidD8/7B++u+SQpbVPljnZ14+spIpL7SI6tmXHB3RTG9s+Y77PamXztiZu59OWz+cHb32xKf2rtb7n4hYtb5P/RO99pkZbOlvj7LdJuP+p+7jvlCQA0HuaLJV+n/Bsf0q/Ye8z9Rg+f8Swvff0xz20/L5vKVWUTePac2U1NPd9/qWX7vt8FKtBr41g3WRg5qDSpS2aie99/IR/F6jJbq5uHnh376FiOnHZmh83V2V6/W3Qj17zRHBSqG+oY//QpXPLUrV1Yqq6xbOtGKqp2J3UxbBLrRSTqjAV4WP+Om9IBYHi/QTz7tUcYSPNxzhn2A8+8y+pmeaYTdh600njzkMkSan202B9/8fdptw2RMi4ecxKnHDCa3xz/EB9eXs5jF/0HRQWZhzs+ep+DKO3jPXHJ10afAMBBg/fmmQucb0mf1LzA9PJXOWr62cxe9jG7UiZS96NI5iz+ki5ot1Xik4cPfHor3ymbSFEku7Gzu4tNVcmToEcjGzjxiWPReDGl4TEcNnAMxZFiRvTdi0G9+jGgVx8GFPehtKQ/Q0v607+4N4WRjvuI7Klr7rq3bOtGRg0ZxuLNzo25ZTXZD2LVHd366jSG9xvK9cd/tdV8a3du42szJ9BPj+CJC+5tsV0oJCyFRIG9+wxt+QZ5tu+Awbw5+RHGPDIGgAkHH0dMv8+pI4/mlrd/iEZajl9/4+jfcffim5rWv/n3nyGhlt/M9i84nTUNrxGODiUWcdrwVYXLj/wyvSL387Py7zc9UPXmxe8xtXwWVx01sWn/sw/1mvYiO9cf9lsKQ8mf7ZGDmn+vdy++CcLw7+86bf+HFH+F7x49iaOHH5T2Zm53FrBAn//Hn8oeOyrp8fB8OGr62TSE11Iqx/LyNx7osGC6tdqdti0eQUJR+usXqY7voCG8lor4h2zd7vbNbqWDg2oItADRMEIBohFCFBCSAkIUEBZnOUyYsEQISYSwRAhLmJCEUBRVRVGqojvZu9d+lPYuJa5x3t32JI33zr82cwKH9jqHTdVrnCa4UAMffL6M4/YbRX00SiQUoj4WpbigkD++9xxfH3MqTy95h6q6ak7ZfywxjTF6r/3oX9y7Q36X7fXCxntgI9y/9A4k1ECf+GFccPBFTd0W9+8/jJAIv13g1Jh3yyLufDuhmSHWC8I1HNb3NNZUfUIdMLhX+r7w+dYnfhhVoaXUNNTz2zOdUciHljzEzM/e58XV/8cXB53A0cOOYPK48byxOvnvZEHV3wE4fsC3KAhFeKtihnMugw/jvmNvY3i/QTy58E1+M/8GvjTEuSF70ZiTmLvhu8za9Af2KziNgb37cPMpX8/7eV1z7FlZ5f+sdiY3vuM8eBWKDaGXDEJpoH9hKfv1OYgvDB7F0JKB9CvqzdCSgfQvLqFfUW/6FPaid2EhheFIzpXQXAQu0Of6y7zzjB9x4azkWeQ3V+1irz7ZTWWW6q1VS5i/eQUPfHpr05wLFTqH7754F/d95d8pLmjbJM4fbVjFta/cxPXjvs/lKbMEpdpRuxNV4cPL57b4ZxKNxdhYuZMNu7ezvnIrO2sq2V1fze66KudVX0lttI76WB0N8QYa4vU0xBuIxuuJaj3ReANRrSemDUS1lrjGUBJeEkOJu01pglIP4Wp21y7kM/e5lNQOUp/WvJg0qPXVr19IUXwEdaH1iMRRDRGKDUAj25n6WXO+v6xsXi6KHUD/gr3oFS5hQNFACsOFFIYLKQgVUBAKEwkVEAk5/4TCEkJECEnIfQlCiLAI4m4PhUKEEOeniJMmYUJC0n676/Y0jYS6sWorYQkzvF9zDbGxhloVWsqjK3/R6nX759YHiUT3YcKICzluxBGUDR/F8H4DOfXRya3u1xH+8tXfc+e7f2XiqCOb0spGHEzZiIO5g8uS8h44cG/P9xjeZyg/P30y5z+xgRV1L1NVv4f9BzpDkV827jQuG5f8D+IXp1+JvA43HN+xY+N7OX3otby25f6m9fmXz+fN1Ut4aslrVEerWb7rE2KhBurjNdSwmT3169i4cw4ftGGCLqfSFEIIAyHQMEIIcH6KFlIU6sMHVz6d6a2yFrBAn93NWC+HlA5j4eSFLNmyjktecv7rn/H3k7nh8Lu4+ujxScO+ZuO6Ny/xTJ+z6zGOefwxhsgx3HX6bRw57ABCoRDLtm7koEF7tTjePe8/TnVoGb9dcAN3fjiQktDevHjJNAb3diaYiMZihMQJTLvrdyLxXp7fGCLhMPsOGMy+AwbjzPLYseLxOOt372Cb222utqGOhZtXc/L+oxnUuy/l65dxy/tXNuU/pPgrrK9eQY1uIxQbwN6Fh9ErUsL6mmXU0XIia4n1p0LhJDIAAAtuSURBVH/4ABqklq31y4nLHtZkGI63w61vXhwaOp4pY79F+cYlNMSjfLylnBhRDuz7BWIaZezQ0fQvLuHeBb9GI9sZ3utQfnNm8s3Lcw88j0dXzefE/Y7otFMYNWQYU8/9f23Ke9jQEbx0/r8469kvJaV/YchIAIaXDGdFHWyvbX0i8sJIhF9PaPuN23y6a8K1HPnX5kAfCoU49cAjOPXAlr9zdYeMXr5tC/M2fEZlXQ2VddXsqqukuqGa2lgNtdF6GuJRok2vGFGNEtcYcY0Ri0eJEyOmMWp0K/3Cw4nFY4RD2c3AlYloluNbd4aysjItL89+7LMV2zbTr6hX2hsv2Vq5fTPnvXBGc0KsN/sVH8+O+i3c/eX/5JgRB7U58De2dbaFxouQUB0aL2JI6AgUZWTfQzjr4H/jzvJfUx9em7xDrC9XHvIjDh86kp+89RMApp55Hze//mu2N6zi46u9R/Drjj7asIr+xb05cNBereZL7EWU7hrE43FqGhqoaqilpr6OuliUulg99dEoCsTiMWIaJx5X56fGicXjxDTelB5vWo435Ymjznrc3UeVfkW9UZz3KQhFWLbtcwrDhQwtGcjl405r8+ckGotx6z+m8Y0vTmDcsJEtttdHox163yQfLnzyFj5LGF9m1nlvsO+AwWyrruTyZ27l92feyqghHTSbaB5sq67k9tencfHhZ3gG+O5KROalmbc7WIG+Izy9+D3umvN7KnUNIBBOeJAj1puw9qNfZBiDivZiYNFAiiPF9CksoSjsNMUIwo7aXfxr25+T3jccHUpYigkRISyF7AkldzFMJLH+xEO7EXGuVa/YKGrC3k8oqoqTL9YPwrvpGx/Nu1c+kdsvwZh22FVbzdx1yzjj4LFdXZQewQJ9HlXW1fDq8o94dOEzbKxZSZgCquIbiIeqk6ZkayFWDGGncVrjERZMnudZy4vGYkTCzte21du3EAlHGNF/ECu2beLZpe8wtGQg3xx7KnFV6uNRFm9eS2VdNe+tXcT6ys3cdOIkPtqwgnvm3UdtvIpbjrmZi8ac1CG/C2NM92GBvpPsqatjV90etlbvpi4aJR53frehkPCF0n0Ih0L0LujYh12MMT1Ta4G+ezf2+UxJURElRUUM7zeoq4tijDFNAvVkrDHGmJbaHehFZF8R+aeILBWRxSLS4hlpETlVRHaJyMfu6/bcimuMMSZbuTTdRIGbVPVDEekLzBOR2aq6JCXfW6p6Tg7HMcYYk4N21+hVdaOqfuguVwJLgX3yVTBjjDH5kZc2ehEZCRwJeM1cfIKIzBeRl0RkdCvvMUVEykWkvKKiIh/FMsYYQx4CvYj0Af4O/FBVU2f0/RDYX1XHAn8Enk33Pqo6VVXLVLWstLQ012IZY4xx5RToRaQAJ8g/pqotRuJR1d2qWuUuzwIKRGRILsc0xhiTnVx63QgwDViqqv+TJs/ebj5E5Fj3eNu88hpjjOkYufS6OQm4HFgoIh+7abcC+wGo6v3ARcB3RSQK1ACXand8FNcYYwKs3YFeVd+G1scEVtV7gZbT5RhjjOk09mSsMcYEnAV6Y4wJOAv0xhgTcBbojTEm4CzQG2NMwFmgN8aYgLNAb4wxAWeB3hhjAs4CvTHGBJwFemOMCTgL9MYYE3AW6I0xJuAs0BtjTMBZoDfGmICzQG+MMQFngd4YYwIuWIH+n7+C/z2xq0thjDHdSrAC/b9+A1sWw5wHoaGmq0tjjDHdQk6BXkQmisinIrJcRG7x2F4kIk+62z8QkZG5HK/NZv0IPri/Uw5ljDHdXbvnjBWRMHAfMB5YB8wVkedVdUlCtquBHap6sIhcCvwGuCSXAreqqB/U7XaWX/8vWP02HHEhFPSCYeOgz14gIYgUNZ5EhxXFGGO6i3YHeuBYYLmqrgQQkSeA84DEQH8e8DN3+SngXhERVdUcjpueJHxBiTfA8n84LwAJAwqhAudnuMgJ+KEwFPR29m0K/JK8nDetnHbGX0mG7X7eP+OnoRuXPdf9cz12POZUZCLFTl5V57MrIfL72c2Tblm56kZl6j0Yrnop72+bS6DfB1ibsL4OOC5dHlWNisguYDCwNfXNRGQKMAVgv/32y740qnDIRNj/BDjsXFg2Gw46Dd6+B4aMgl1uUWt2QmEJROsgVufs11ANGm9+n8Y/rqz/HykZPzStftBz2dfv+3f0sTPs7udzb6iBWH1CcFfnH0C30zH1u5x0UJ2z3Yr7dcjb5hLovT59qb+1tuRxElWnAlMBysrKsv/ti8DXHmheH+u2EE38VdZvZYwxQZLLzdh1wL4J6yOADenyiEgE6A9sz+GYxhhjspRLoJ8LjBKRA0SkELgUeD4lz/PAZHf5IuD1DmufN8YY46ndTTdum/v1wCtAGJiuqotF5D+BclV9HpgGPCoiy3Fq8pfmo9DGGGPaLpc2elR1FjArJe32hOVa4OJcjmGMMSY3wXoy1hhjTAsW6I0xJuAs0BtjTMBZoDfGmICT7tjbUUQqgDXt2HUIHk/dBpydc89g59wz5HLO+6tqqdeGbhno20tEylW1rKvL0ZnsnHsGO+eeoaPO2ZpujDEm4CzQG2NMwAUt0E/t6gJ0ATvnnsHOuWfokHMOVBu9McaYloJWozfGGJPCAr0xxgRcYAJ9ponK/UpE9hWRf4rIUhFZLCI/cNMHichsEVnm/hzopouI/MH9PSwQkaO69gzaR0TCIvKRiLzorh/gTjC/zJ1wvtBN75oJ6DuAiAwQkadE5BP3ep8Q5OssIje6n+lFIjJDRIqDeJ1FZLqIbBGRRQlpWV9XEZns5l8mIpO9jpVOIAJ9wkTlZwGHA5NE5PCuLVXeRIGbVPUw4Hjge+653QK8pqqjgNfcdXB+B6Pc1xTgT51f5Lz4AbA0Yf03wN3u+e7AmXgeEiagB+528/nV74GXVfULwFic8w/kdRaRfYAbgDJVPQJnqPNLCeZ1fhiYmJKW1XUVkUHAHTjTtR4L3NH4z6FNVNX3L+AE4JWE9Z8AP+nqcnXQuT4HjAc+BYa5acOAT93lB4BJCfmb8vnlhTNb2WvAl4EXcaak3ApEUq83znwIJ7jLETefdPU5tOOc+wGrUsse1OtM83zSg9zr9iJwZlCvMzASWNTe6wpMAh5ISE/Kl+kViBo93hOV79NFZekw7tfVI4EPgL1UdSOA+3Oomy0Iv4t7gB8D7oztDAZ2qmrUXU88p6QJ6IHGCej95kCgAnjIbbL6s4iUENDrrKrrgbuAz4GNONdtHsG/zo2yva45Xe+gBPo2T0LuVyLSB/g78ENV3d1aVo803/wuROQcYIuqzktM9siqbdjmJxHgKOBPqnoksIfmr/NefH3ebrPDecABwHCgBKfZIlXQrnMm6c4zp/MPSqBvy0TlviUiBThB/jFVfdpN3iwiw9ztw4AtbrrffxcnAeeKyGrgCZzmm3uAAe4E85B8TkGZgH4dsE5VP3DXn8IJ/EG9zmcAq1S1QlUbgKeBEwn+dW6U7XXN6XoHJdC3ZaJyXxIRwZl7d6mq/k/CpsSJ1yfjtN03pn/LvXt/PLCr8SuiH6jqT1R1hKqOxLmOr6vqN4F/4kwwDy3P1/cT0KvqJmCtiBzqJp0OLCGg1xmnyeZ4EentfsYbzzfQ1zlBttf1FWCCiAx0vw1NcNPapqtvUuTxZsfZwGfACuC2ri5PHs/rZJyvaAuAj93X2Tjtk68By9yfg9z8gtMDaQWwEKdXQ5efRzvP/VTgRXf5QGAOsBz4P6DITS9215e72w/s6nLncL7jgHL3Wj8LDAzydQZ+DnwCLAIeBYqCeJ2BGTj3IRpwauZXt+e6Ale5578cuDKbMtgQCMYYE3BBaboxxhiThgV6Y4wJOAv0xhgTcBbojTEm4CzQG2NMwFmgN8aYgLNAb4wxAff/AZdz4yCBDGsIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = np.arange(1, len(loss_bulk_record)+1, 1)\n",
    "plt.plot(num, loss_bulk_record)\n",
    "plt.plot(num, loss_surf_record)\n",
    "plt.plot(num, np.add(loss_bulk_record , loss_surf_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),'./poisson_equation_24_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
