{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def TicTocGenerator():\n",
    "    # Generator that returns time differences\n",
    "    ti = 0           # initial time\n",
    "    tf = time.time() # final time\n",
    "    while True:\n",
    "        ti = tf\n",
    "        tf = time.time()\n",
    "        yield tf-ti # returns the time difference\n",
    "\n",
    "TicToc = TicTocGenerator() # create an instance of the TicTocGen generator\n",
    "\n",
    "# This will be the main function through which we define both tic() and toc()\n",
    "def toc(tempBool=True):\n",
    "    # Prints the time difference yielded by generator instance TicToc\n",
    "    tempTimeInterval = next(TicToc)\n",
    "    if tempBool:\n",
    "        print( \"Elapsed time: %f seconds.\\n\" %tempTimeInterval )\n",
    "\n",
    "def tic():\n",
    "    # Records a time in TicToc, marks the beginning of a time interval\n",
    "    toc(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "# dx is the step size in test set\n",
    "\n",
    "global k, dx, beta \n",
    "k, dx, beta = 10, .001, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x< 0.5 :\n",
    "        return 8*k*(3*x-1)\n",
    "    else :\n",
    "        return 4*k*(k+1)\n",
    "    \n",
    "def g(x):\n",
    "    return torch.tensor([0.], requires_grad=True)\n",
    "\n",
    "def u_exact(x):\n",
    "    if x< 0.5 :\n",
    "        return 4*k*x**2*(1-x)\n",
    "    else :\n",
    "        return (2*(k+1)*x-1)*(1-x)\n",
    "\n",
    "def sigma_exact(x):\n",
    "    if x< 0.5 :\n",
    "        return -12*k*x**2+8*k*x\n",
    "    else :\n",
    "        return -4*(k+1)*x + 2 *k + 3\n",
    "\n",
    "sq = lambda x: x ** 2\n",
    "vsq = np.vectorize(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute H1 norm of true u and sigma\n",
    "L = 0.\n",
    "R = 1.\n",
    "test_set1 =  np.arange(L, R/2, dx)\n",
    "test_set2 =  np.arange(R/2, R, dx)\n",
    "test_set = np.concatenate((test_set1, test_set2))\n",
    "u1 = np.vectorize(u_exact)(test_set1)\n",
    "ud1 = np.vectorize(sigma_exact)(test_set1)\n",
    "u2 = np.vectorize(u_exact)(test_set2)\n",
    "ud2 = np.vectorize(sigma_exact)(test_set2)\n",
    "u_h1 = np.sum(dx*(vsq(ud1)+ vsq(ud2) ))\n",
    "u_l2 = np.sum(dx*vsq(u1)+ dx*vsq(u2) )\n",
    "\n",
    "sigma1 = np.vectorize(sigma_exact)(test_set1)\n",
    "sigmad1 = -np.vectorize(f)(test_set1)\n",
    "sigma2 = np.vectorize(sigma_exact)(test_set2)\n",
    "sigmad2 = -np.vectorize(f)(test_set2)/k\n",
    "\n",
    "sigma_h1 = np.sum(dx*(vsq(sigma1) + vsq(sigmad1) + vsq(sigma2) + vsq(sigmad2)))\n",
    "sigma_l2 = np.sum(dx*vsq(sigma1) + dx*vsq(sigma2) )\n",
    "\n",
    "print('u: H1 norm square: %.6f, L2 norm square: %.6f ' %(u_h1, u_l2))\n",
    "print('sigma: H1 norm square: %.6f, L2 norm square: %.6f ' %(sigma_h1, sigma_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuSigmaPde(nn.Module):\n",
    "    def __init__(self, dimension, mesh = 32, neuron = 24):\n",
    "        super(MuSigmaPde, self).__init__()\n",
    "\n",
    "        self.xdim = dimension\n",
    "        # Layer 1\n",
    "        self.fc1mu = nn.Linear(dimension, mesh)\n",
    "        #self.fc1sig = nn.Linear(dimension, mesh)\n",
    "        # Layer 2\n",
    "        self.fc2mu = nn.Linear(mesh, neuron)\n",
    "        #self.fc2sig = nn.Linear(mesh, neuron)\n",
    "        # Layer 3\n",
    "        self.fc3mu = nn.Linear(neuron, neuron)\n",
    "        #self.fc3sig = nn.Linear(neuron, neuron)\n",
    "        # Layer 4\n",
    "        self.fc4mu = nn.Linear(neuron, 1)\n",
    "        #self.fc4sig = nn.Linear(neuron, dimension)\n",
    "\n",
    "    def forward(self, x):   #Activation function sigmoid\n",
    "        assert(len(x.shape) == 1 and x.shape[0] == self.xdim)\n",
    "        mu =  torch.sigmoid(self.fc2mu(torch.sigmoid(self.fc1mu(x))))\n",
    "        mu =  self.fc4mu(torch.sigmoid(self.fc3mu(mu)))\n",
    "        return mu\n",
    "    \n",
    "    \n",
    "    def net_grad(self, x, h):\n",
    "        mu_center = self.forward(x)\n",
    "        mu_forward = self.forward(x - .5*h)\n",
    "        mu_backward =self.forward(x + 0.5*h)\n",
    "\n",
    "        mu_grad_forward = (mu_center - mu_forward)/(.5*h)\n",
    "        mu_grad_backward = (mu_backward - mu_center)/(0.5*h)\n",
    "        mu_lap = (mu_grad_backward - mu_grad_forward)/ (0.5*h)       \n",
    "    \n",
    "        return mu_grad_forward, mu_lap     \n",
    "    \n",
    "    def loss_function_bulk_ls(self, x,h):   #LS functional\n",
    "        mu_grad_forward, mu_lap = self.net_grad(x,h)\n",
    "        if x< 0.5:\n",
    "            LSE = (mu_lap + f(x))**2\n",
    "        else: \n",
    "            LSE = (mu_lap *k + f(x))**2\n",
    "        return LSE \n",
    "    \n",
    "    def loss_function_bulk_en(self, x,h):   #Energy functional\n",
    "        mu = self.forward(x)\n",
    "        mu_grad_forward, mu_lap = self.net_grad(x,h)\n",
    "        if x< 0.5:\n",
    "            LSE = 0.5 * mu_grad_forward**2 - f(x)*mu\n",
    "        else: \n",
    "            LSE = 0.5 * k * mu_grad_forward**2 - f(x)*mu\n",
    "        return LSE \n",
    "\n",
    "    def loss_function_surf(self, x):\n",
    "        mu = self.forward(x)\n",
    "        # Boundary condition penalty\n",
    "        BCP = beta * (mu - g(x))**2\n",
    "        return BCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MuSigmaPde(dimension =1, mesh = 32, neuron = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h= .002\n",
    "L, R = 0., 1.\n",
    "epochs = 20000\n",
    "bulk_set, surf_set =  np.arange(L, R, h), [L, R]\n",
    "loss_bulk_record, loss_surf_record = [], []\n",
    "print('bulk points number %d \\nsurface points number %d\\ntest points number %d\\ndx for difference in testing %.3g\\ntrainging iteration %d' %(np.size(bulk_set), np.size(surf_set), np.size(test_set), dx, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, lr_decay=0.1, lr_decay_epoch=10000):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
    "    if epoch % lr_decay_epoch:\n",
    "        return optimizer\n",
    "    if epoch == 0:\n",
    "        return optimizer\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    return optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "local_min = -135\n",
    "for j in range(epochs):\n",
    "    loss_bulk = torch.zeros(1)\n",
    "    loss_surf = torch.zeros(1)\n",
    "\n",
    "    for point in bulk_set:\n",
    "        x = torch.tensor([point+ 0.5*h])\n",
    "        loss_bulk += h*model.loss_function_bulk_ls(x,h)\n",
    "               \n",
    "    for point in surf_set:\n",
    "        x = torch.tensor([point])\n",
    "        loss_surf += model.loss_function_surf(x)\n",
    "\n",
    "    loss_bulk_record.append(loss_bulk.data[0])\n",
    "    loss_surf_record.append(loss_surf.data[0])\n",
    "       \n",
    "    loss = loss_bulk + loss_surf        \n",
    "    print('Train Epoch: {}, Loss: {:.6f}, loss bulk: {:.6f}, loss surf: {:.6f}'.format(j, loss.item(), loss_bulk.item(), loss_surf.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    exp_lr_scheduler(optimizer, j)\n",
    "    \n",
    "    if loss.item() < local_min:\n",
    "        print('updating the parameters')\n",
    "        local_min = loss.item()\n",
    "        torch.save(model.state_dict(),'./diffusion_sigmoid_en')\n",
    "\n",
    "    optimizer.step()   \n",
    "toc() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./diffusion_sigmoid_en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_err_h1 = torch.zeros(1)\n",
    "sigma_err_h1 = torch.zeros(1)\n",
    "bdd_err = torch.zeros(1)\n",
    "mu_err_l2 = torch.zeros(1)\n",
    "sigma_err_l2 = torch.zeros(1)\n",
    "G_relative = torch.zeros(1)\n",
    "mu_err_semi = torch.zeros(1)\n",
    "\n",
    "for point in test_set:\n",
    "    x = torch.tensor([point+ 0.5*dx])\n",
    "    mu = model(x)\n",
    "    mu_grad, mu_lap = model.net_grad(x,dx)\n",
    "\n",
    "    # esitmate H1 norm error\n",
    "    mu_diff_simi = (mu_grad - sigma_exact(x))**2\n",
    "    \n",
    "    # estimate L2 norm error\n",
    "    mu_err_l2 += dx*(mu - u_exact(x))**2\n",
    "    \n",
    "    # estimate H1 semi norm  error\n",
    "    mu_err_semi += dx*mu_diff_simi\n",
    "    \n",
    "\n",
    "\n",
    "mu_err_l2_relative = (mu_err_l2/u_l2)**(1/2)\n",
    "mu_err_semi_relative = (mu_err_semi/(sigma_l2))**(1/2)\n",
    "\n",
    "print('u: L2_rel: {:.6f}, H1_semi_rel: {:.6f}'.format( mu_err_l2_relative.item(), mu_err_semi_relative.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = test_set\n",
    "yt = np.zeros_like(points)\n",
    "y_diff = np.zeros_like(points)\n",
    "ymu = np.zeros_like(points)\n",
    "ysig = np.zeros_like(points)\n",
    "for i in range(len(points)):\n",
    "    yt[i] = u_exact(points[i])\n",
    "    y_diff[i] =  sigma_exact(points[i])\n",
    "    ymu[i] = model(torch.tensor([points[i]]))\n",
    "    ysig[i], ss = model.net_grad(torch.tensor([points[i]]),dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points, yt, color = 'b', label = 'u_true')\n",
    "plt.plot(points, ymu, color = 'r', label = 'u_approximation')\n",
    "plt.ylim([0,7])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points, y_diff, color = 'b', label = 'u\\'_true')\n",
    "plt.plot(points, ysig, color = 'r', label = 'u\\'_approximation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = np.arange(1, len(loss_bulk_record)+1, 1)\n",
    "plt.plot(num, loss_bulk_record)\n",
    "plt.plot(num, loss_surf_record)\n",
    "plt.plot(num, np.add(loss_bulk_record , loss_surf_record))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
