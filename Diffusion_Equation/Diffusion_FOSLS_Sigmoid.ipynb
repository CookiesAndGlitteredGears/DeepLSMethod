{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xyPiwh8A3EVY"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def TicTocGenerator():\n",
        "    # Generator that returns time differences\n",
        "    ti = 0           # initial time\n",
        "    tf = time.time() # final time\n",
        "    while True:\n",
        "        ti = tf\n",
        "        tf = time.time()\n",
        "        yield tf-ti # returns the time difference\n",
        "\n",
        "TicToc = TicTocGenerator() # create an instance of the TicTocGen generator\n",
        "\n",
        "# This will be the main function through which we define both tic() and toc()\n",
        "def toc(tempBool=True):\n",
        "    # Prints the time difference yielded by generator instance TicToc\n",
        "    tempTimeInterval = next(TicToc)\n",
        "    if tempBool:\n",
        "        print( \"Elapsed time: %f seconds.\\n\" %tempTimeInterval )\n",
        "\n",
        "def tic():\n",
        "    # Records a time in TicToc, marks the beginning of a time interval\n",
        "    toc(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9hZDAflD3EVc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torch import nn, optim\n",
        "from math import exp\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "XfarxT613EVd"
      },
      "outputs": [],
      "source": [
        "# initialize parameters\n",
        "#dx is the step size for test set\n",
        "torch.manual_seed(0)\n",
        "today = datetime.datetime.today()\n",
        "d4 = today.strftime(\"%b-%d-%Y\")\n",
        "\n",
        "global k, dx, beta\n",
        "k, dx, beta = 10, .001, 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "APCNELBk3EVe"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    if x< 0.5 :\n",
        "        return 8*k*(3*x-1)\n",
        "    else :\n",
        "        return 4*k*(k+1)\n",
        "\n",
        "def g(x):\n",
        "    return torch.tensor([0.], requires_grad=True)\n",
        "\n",
        "def u_exact(x):\n",
        "    if x< 0.5 :\n",
        "        return 4*k*x**2*(1-x)\n",
        "    else :\n",
        "        return (2*(k+1)*x-1)*(1-x)\n",
        "\n",
        "def sigma_exact(x):\n",
        "    if x< 0.5 :\n",
        "        return 12*k*x**2-8*k*x\n",
        "    else :\n",
        "        return 4*k*(k+1)*x - 2 *k*k - 3*k\n",
        "\n",
        "sq = lambda x: x ** 2\n",
        "vsq = np.vectorize(sq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArziQL9k3EVf",
        "outputId": "558121f2-dcc4-43ea-c138-8fde86fcd445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u: H1 norm square: 70076.634481, L2 norm square: 10.414888 \n",
            "sigma: H1 norm square: 1038805.849400, L2 norm square: 7051.299400 \n"
          ]
        }
      ],
      "source": [
        "# compute H1 norm of true u and sigma\n",
        "L = 0.\n",
        "R = 1.\n",
        "test_set1 =  np.arange(L, R/2, dx)\n",
        "test_set2 =  np.arange(R/2, R, dx)\n",
        "test_set = np.concatenate((test_set1, test_set2))\n",
        "u1 = np.vectorize(u_exact)(test_set1)\n",
        "ud1 = -np.vectorize(sigma_exact)(test_set1)\n",
        "u2 = np.vectorize(u_exact)(test_set2)\n",
        "ud2 = -np.vectorize(sigma_exact)(test_set2)\n",
        "u_h1 = np.sum(dx*(vsq(u1) + vsq(ud1)+ k* vsq(u2) + k * vsq(ud2) ))\n",
        "u_l2 = np.sum(dx*vsq(u1)+ dx*vsq(u2) )\n",
        "\n",
        "sigma1 = np.vectorize(sigma_exact)(test_set1)\n",
        "sigmad1 = np.vectorize(f)(test_set1)\n",
        "sigma2 = np.vectorize(sigma_exact)(test_set2)\n",
        "sigmad2 = np.vectorize(f)(test_set2)\n",
        "\n",
        "sigma_h1 = np.sum(dx*(vsq(sigma1) + vsq(sigmad1) + k* vsq(sigma2) + k* vsq(sigmad2)))\n",
        "sigma_l2 = np.sum(dx*vsq(sigma1) + dx*vsq(sigma2) )\n",
        "\n",
        "print('u: H1 norm square: %.6f, L2 norm square: %.6f ' %(u_h1, u_l2))\n",
        "print('sigma: H1 norm square: %.6f, L2 norm square: %.6f ' %(sigma_h1, sigma_l2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "c486Cek03EVg"
      },
      "outputs": [],
      "source": [
        "class MuSigmaPde(nn.Module):\n",
        "    def __init__(self, dimension, mesh = 32, neuron = 24):\n",
        "        super(MuSigmaPde, self).__init__()\n",
        "\n",
        "        self.xdim = dimension\n",
        "        # Layer 1\n",
        "        self.fc1mu = nn.Linear(dimension, mesh)\n",
        "        self.fc1sig = nn.Linear(dimension, mesh)\n",
        "        # Layer 2\n",
        "        self.fc2mu = nn.Linear(mesh, neuron)\n",
        "        self.fc2sig = nn.Linear(mesh, neuron)\n",
        "        # Layer 3\n",
        "        self.fc3mu = nn.Linear(neuron, neuron)\n",
        "        self.fc3sig = nn.Linear(neuron, neuron)\n",
        "        # Layer 4\n",
        "        self.fc4mu = nn.Linear(neuron, 1)\n",
        "        self.fc4sig = nn.Linear(neuron, dimension)\n",
        "\n",
        "    def forward(self, x):   #Activation Function Sigmoid\n",
        "        assert(len(x.shape) == 1 and x.shape[0] == self.xdim)\n",
        "        mu =  torch.sigmoid(self.fc2mu(torch.sigmoid(self.fc1mu(x))))\n",
        "        sig =  torch.sigmoid(self.fc2sig(torch.sigmoid(self.fc1sig(x))))\n",
        "        mu =  self.fc4mu(torch.sigmoid(self.fc3mu(mu)))\n",
        "        sig = self.fc4sig(torch.sigmoid(self.fc3sig(sig)))\n",
        "        return mu, sig\n",
        "\n",
        "    def net_grad(self, x, h):\n",
        "        mu_center, sigma_center = self.forward(x)\n",
        "        mu_forward, sigma_forward = self.forward(x - 0.5*h)\n",
        "\n",
        "        mu_grad_forward = (mu_center - mu_forward)/(0.5*h)\n",
        "        sigma_grad_forward = (sigma_center - sigma_forward)/(0.5*h)\n",
        "\n",
        "        return mu_grad_forward, sigma_grad_forward\n",
        "\n",
        "    def loss_function_bulk(self, x,h):    #FOSLS\n",
        "        mu, sigma = self.forward(x)\n",
        "        mu_grad, sigma_grad = self.net_grad(x,h)\n",
        "        if x< 0.5:\n",
        "            LSE = torch.sum((mu_grad + sigma)**2) + (sigma_grad - f(x))**2\n",
        "        else:\n",
        "            LSE = torch.sum((mu_grad * k**(0.5) + sigma* k**(-0.5))**2) + (sigma_grad - f(x))**2\n",
        "        return LSE\n",
        "\n",
        "    def loss_function_surf(self, x):\n",
        "        mu, sigma = self.forward(x)\n",
        "        # Boundary condition penalty\n",
        "        BCP = beta * (mu - g(x))**2\n",
        "        return BCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6tPM8LDR3EVk"
      },
      "outputs": [],
      "source": [
        "layerArray = [1,32,24,24,1]\n",
        "\n",
        "model = MuSigmaPde(dimension =1, mesh = 32, neuron = 24).double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKzEkY2B3EVm",
        "outputId": "21ca3bbd-d88d-44fa-d175-ce4a398b6bcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2962"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "sum([p.numel() for p in model.parameters()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxCh_j_b3EVn",
        "outputId": "938e9dd5-0ed6-4a03-d1bd-8f889cd3c7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bulk points number 500 \n",
            "surface points number 2\n",
            "test points number 1000\n",
            "dx for difference in testing 0.001\n",
            "trainging iteration 50000\n"
          ]
        }
      ],
      "source": [
        "h = .002\n",
        "L, R = 0., 1.\n",
        "epochs = 50000\n",
        "bulk_set, surf_set =  np.arange(L, R, h), [L, R]\n",
        "loss_bulk_record, loss_surf_record = [], []\n",
        "print('bulk points number %d \\nsurface points number %d\\ntest points number %d\\ndx for difference in testing %.3g\\ntrainging iteration %d' %(np.size(bulk_set), np.size(surf_set), np.size(test_set), dx, epochs))\n",
        "\n",
        "folder = f'{d4} fosls sigmoid/{layerArray}/{int(1/h)}_base'\n",
        "if not os.path.exists(folder): os.makedirs(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "kJRLrTOQ3EVq"
      },
      "outputs": [],
      "source": [
        "def exp_lr_scheduler(optimizer, epoch, lr_decay=0.1, lr_decay_epoch=10000):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "    if epoch % lr_decay_epoch:\n",
        "        return optimizer\n",
        "    if epoch == 0:\n",
        "        return optimizer\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] *= lr_decay\n",
        "    return optimizer\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S2uHkLj3EVr",
        "outputId": "e24b87fb-bb77-4ead-9be5-34d0f7444b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0, Loss: 97600.164062, loss bulk: 97600.140625, loss surf: 0.022158\n",
            "updating the parameters\n",
            "Train Epoch: 1, Loss: 97599.476562, loss bulk: 97599.460938, loss surf: 0.013216\n",
            "updating the parameters\n",
            "Train Epoch: 2, Loss: 97599.367188, loss bulk: 97599.359375, loss surf: 0.006644\n",
            "updating the parameters\n",
            "Train Epoch: 3, Loss: 97599.015625, loss bulk: 97599.015625, loss surf: 0.002413\n",
            "updating the parameters\n",
            "Train Epoch: 4, Loss: 97598.906250, loss bulk: 97598.906250, loss surf: 0.000350\n",
            "updating the parameters\n",
            "Train Epoch: 5, Loss: 97598.875000, loss bulk: 97598.875000, loss surf: 0.000078\n",
            "updating the parameters\n",
            "Train Epoch: 6, Loss: 97598.765625, loss bulk: 97598.765625, loss surf: 0.000988\n",
            "updating the parameters\n",
            "Train Epoch: 7, Loss: 97598.398438, loss bulk: 97598.398438, loss surf: 0.002347\n",
            "updating the parameters\n",
            "Train Epoch: 8, Loss: 97598.273438, loss bulk: 97598.273438, loss surf: 0.003509\n",
            "updating the parameters\n",
            "Train Epoch: 9, Loss: 97598.070312, loss bulk: 97598.062500, loss surf: 0.004103\n",
            "updating the parameters\n",
            "Train Epoch: 10, Loss: 97597.500000, loss bulk: 97597.492188, loss surf: 0.004050\n",
            "updating the parameters\n",
            "Train Epoch: 11, Loss: 97597.132812, loss bulk: 97597.132812, loss surf: 0.003475\n",
            "updating the parameters\n",
            "Train Epoch: 12, Loss: 97597.015625, loss bulk: 97597.015625, loss surf: 0.002598\n",
            "updating the parameters\n",
            "Train Epoch: 13, Loss: 97596.898438, loss bulk: 97596.898438, loss surf: 0.001653\n",
            "updating the parameters\n",
            "Train Epoch: 14, Loss: 97596.562500, loss bulk: 97596.562500, loss surf: 0.000834\n",
            "updating the parameters\n",
            "Train Epoch: 15, Loss: 97596.406250, loss bulk: 97596.406250, loss surf: 0.000271\n",
            "updating the parameters\n",
            "Train Epoch: 16, Loss: 97595.671875, loss bulk: 97595.671875, loss surf: 0.000018\n",
            "updating the parameters\n",
            "Train Epoch: 17, Loss: 97595.226562, loss bulk: 97595.226562, loss surf: 0.000053\n",
            "updating the parameters\n",
            "Train Epoch: 18, Loss: 97595.132812, loss bulk: 97595.132812, loss surf: 0.000293\n",
            "updating the parameters\n",
            "Train Epoch: 19, Loss: 97594.953125, loss bulk: 97594.953125, loss surf: 0.000621\n",
            "updating the parameters\n",
            "Train Epoch: 20, Loss: 97594.546875, loss bulk: 97594.546875, loss surf: 0.000924\n",
            "updating the parameters\n",
            "Train Epoch: 21, Loss: 97593.914062, loss bulk: 97593.914062, loss surf: 0.001112\n",
            "updating the parameters\n",
            "Train Epoch: 22, Loss: 97593.296875, loss bulk: 97593.296875, loss surf: 0.001145\n",
            "updating the parameters\n",
            "Train Epoch: 23, Loss: 97593.156250, loss bulk: 97593.156250, loss surf: 0.001028\n",
            "updating the parameters\n",
            "Train Epoch: 24, Loss: 97592.664062, loss bulk: 97592.664062, loss surf: 0.000804\n",
            "updating the parameters\n",
            "Train Epoch: 25, Loss: 97591.968750, loss bulk: 97591.968750, loss surf: 0.000534\n",
            "updating the parameters\n",
            "Train Epoch: 26, Loss: 97591.359375, loss bulk: 97591.359375, loss surf: 0.000283\n",
            "updating the parameters\n",
            "Train Epoch: 27, Loss: 97590.867188, loss bulk: 97590.867188, loss surf: 0.000099\n",
            "updating the parameters\n",
            "Train Epoch: 28, Loss: 97590.492188, loss bulk: 97590.492188, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 29, Loss: 97589.460938, loss bulk: 97589.460938, loss surf: 0.000012\n",
            "updating the parameters\n",
            "Train Epoch: 30, Loss: 97588.953125, loss bulk: 97588.953125, loss surf: 0.000085\n",
            "updating the parameters\n",
            "Train Epoch: 31, Loss: 97588.125000, loss bulk: 97588.125000, loss surf: 0.000188\n",
            "updating the parameters\n",
            "Train Epoch: 32, Loss: 97587.421875, loss bulk: 97587.421875, loss surf: 0.000283\n",
            "updating the parameters\n",
            "Train Epoch: 33, Loss: 97586.945312, loss bulk: 97586.945312, loss surf: 0.000340\n",
            "updating the parameters\n",
            "Train Epoch: 34, Loss: 97585.703125, loss bulk: 97585.703125, loss surf: 0.000344\n",
            "updating the parameters\n",
            "Train Epoch: 35, Loss: 97585.093750, loss bulk: 97585.093750, loss surf: 0.000297\n",
            "updating the parameters\n",
            "Train Epoch: 36, Loss: 97583.835938, loss bulk: 97583.835938, loss surf: 0.000219\n",
            "updating the parameters\n",
            "Train Epoch: 37, Loss: 97583.148438, loss bulk: 97583.148438, loss surf: 0.000130\n",
            "updating the parameters\n",
            "Train Epoch: 38, Loss: 97581.789062, loss bulk: 97581.789062, loss surf: 0.000056\n",
            "updating the parameters\n",
            "Train Epoch: 39, Loss: 97580.984375, loss bulk: 97580.984375, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 40, Loss: 97579.617188, loss bulk: 97579.617188, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 41, Loss: 97578.460938, loss bulk: 97578.460938, loss surf: 0.000019\n",
            "updating the parameters\n",
            "Train Epoch: 42, Loss: 97577.343750, loss bulk: 97577.343750, loss surf: 0.000053\n",
            "updating the parameters\n",
            "Train Epoch: 43, Loss: 97575.812500, loss bulk: 97575.812500, loss surf: 0.000089\n",
            "updating the parameters\n",
            "Train Epoch: 44, Loss: 97574.437500, loss bulk: 97574.437500, loss surf: 0.000112\n",
            "updating the parameters\n",
            "Train Epoch: 45, Loss: 97573.085938, loss bulk: 97573.085938, loss surf: 0.000116\n",
            "updating the parameters\n",
            "Train Epoch: 46, Loss: 97571.593750, loss bulk: 97571.593750, loss surf: 0.000101\n",
            "updating the parameters\n",
            "Train Epoch: 47, Loss: 97569.859375, loss bulk: 97569.859375, loss surf: 0.000074\n",
            "updating the parameters\n",
            "Train Epoch: 48, Loss: 97568.101562, loss bulk: 97568.101562, loss surf: 0.000043\n",
            "updating the parameters\n",
            "Train Epoch: 49, Loss: 97566.390625, loss bulk: 97566.390625, loss surf: 0.000017\n",
            "updating the parameters\n",
            "Train Epoch: 50, Loss: 97564.562500, loss bulk: 97564.562500, loss surf: 0.000003\n",
            "updating the parameters\n",
            "Train Epoch: 51, Loss: 97562.656250, loss bulk: 97562.656250, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 52, Loss: 97560.640625, loss bulk: 97560.640625, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 53, Loss: 97558.578125, loss bulk: 97558.578125, loss surf: 0.000023\n",
            "updating the parameters\n",
            "Train Epoch: 54, Loss: 97556.390625, loss bulk: 97556.390625, loss surf: 0.000034\n",
            "updating the parameters\n",
            "Train Epoch: 55, Loss: 97554.062500, loss bulk: 97554.062500, loss surf: 0.000040\n",
            "updating the parameters\n",
            "Train Epoch: 56, Loss: 97551.679688, loss bulk: 97551.679688, loss surf: 0.000038\n",
            "updating the parameters\n",
            "Train Epoch: 57, Loss: 97549.210938, loss bulk: 97549.210938, loss surf: 0.000030\n",
            "updating the parameters\n",
            "Train Epoch: 58, Loss: 97546.632812, loss bulk: 97546.632812, loss surf: 0.000019\n",
            "updating the parameters\n",
            "Train Epoch: 59, Loss: 97543.882812, loss bulk: 97543.882812, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 60, Loss: 97541.164062, loss bulk: 97541.164062, loss surf: 0.000003\n",
            "updating the parameters\n",
            "Train Epoch: 61, Loss: 97538.140625, loss bulk: 97538.140625, loss surf: 0.000002\n",
            "updating the parameters\n",
            "Train Epoch: 62, Loss: 97535.179688, loss bulk: 97535.179688, loss surf: 0.000005\n",
            "updating the parameters\n",
            "Train Epoch: 63, Loss: 97531.960938, loss bulk: 97531.960938, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 64, Loss: 97528.703125, loss bulk: 97528.703125, loss surf: 0.000014\n",
            "updating the parameters\n",
            "Train Epoch: 65, Loss: 97525.296875, loss bulk: 97525.296875, loss surf: 0.000017\n",
            "updating the parameters\n",
            "Train Epoch: 66, Loss: 97521.734375, loss bulk: 97521.734375, loss surf: 0.000016\n",
            "updating the parameters\n",
            "Train Epoch: 67, Loss: 97518.078125, loss bulk: 97518.078125, loss surf: 0.000014\n",
            "updating the parameters\n",
            "Train Epoch: 68, Loss: 97514.265625, loss bulk: 97514.265625, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 69, Loss: 97510.296875, loss bulk: 97510.296875, loss surf: 0.000006\n",
            "updating the parameters\n",
            "Train Epoch: 70, Loss: 97506.210938, loss bulk: 97506.210938, loss surf: 0.000004\n",
            "updating the parameters\n",
            "Train Epoch: 71, Loss: 97501.945312, loss bulk: 97501.945312, loss surf: 0.000004\n",
            "updating the parameters\n",
            "Train Epoch: 72, Loss: 97497.554688, loss bulk: 97497.554688, loss surf: 0.000005\n",
            "updating the parameters\n",
            "Train Epoch: 73, Loss: 97493.007812, loss bulk: 97493.007812, loss surf: 0.000007\n",
            "updating the parameters\n",
            "Train Epoch: 74, Loss: 97488.289062, loss bulk: 97488.289062, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 75, Loss: 97483.398438, loss bulk: 97483.398438, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 76, Loss: 97478.382812, loss bulk: 97478.382812, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 77, Loss: 97473.132812, loss bulk: 97473.132812, loss surf: 0.000008\n",
            "updating the parameters\n",
            "Train Epoch: 78, Loss: 97467.734375, loss bulk: 97467.734375, loss surf: 0.000007\n",
            "updating the parameters\n",
            "Train Epoch: 79, Loss: 97462.164062, loss bulk: 97462.164062, loss surf: 0.000006\n",
            "updating the parameters\n",
            "Train Epoch: 80, Loss: 97456.390625, loss bulk: 97456.390625, loss surf: 0.000006\n",
            "updating the parameters\n",
            "Train Epoch: 81, Loss: 97450.437500, loss bulk: 97450.437500, loss surf: 0.000007\n",
            "updating the parameters\n",
            "Train Epoch: 82, Loss: 97444.304688, loss bulk: 97444.304688, loss surf: 0.000008\n",
            "updating the parameters\n",
            "Train Epoch: 83, Loss: 97437.953125, loss bulk: 97437.953125, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 84, Loss: 97431.421875, loss bulk: 97431.421875, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 85, Loss: 97424.664062, loss bulk: 97424.664062, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 86, Loss: 97417.695312, loss bulk: 97417.695312, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 87, Loss: 97410.515625, loss bulk: 97410.515625, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 88, Loss: 97403.085938, loss bulk: 97403.085938, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 89, Loss: 97395.476562, loss bulk: 97395.476562, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 90, Loss: 97387.617188, loss bulk: 97387.617188, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 91, Loss: 97379.523438, loss bulk: 97379.523438, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 92, Loss: 97371.171875, loss bulk: 97371.171875, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 93, Loss: 97362.578125, loss bulk: 97362.578125, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 94, Loss: 97353.726562, loss bulk: 97353.726562, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 95, Loss: 97344.640625, loss bulk: 97344.640625, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 96, Loss: 97335.281250, loss bulk: 97335.281250, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 97, Loss: 97325.648438, loss bulk: 97325.648438, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 98, Loss: 97315.757812, loss bulk: 97315.757812, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 99, Loss: 97305.554688, loss bulk: 97305.554688, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 100, Loss: 97295.093750, loss bulk: 97295.093750, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 101, Loss: 97284.312500, loss bulk: 97284.312500, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 102, Loss: 97273.242188, loss bulk: 97273.242188, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 103, Loss: 97261.882812, loss bulk: 97261.882812, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 104, Loss: 97250.203125, loss bulk: 97250.203125, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 105, Loss: 97238.234375, loss bulk: 97238.234375, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 106, Loss: 97225.914062, loss bulk: 97225.914062, loss surf: 0.000010\n",
            "updating the parameters\n",
            "Train Epoch: 107, Loss: 97213.273438, loss bulk: 97213.273438, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 108, Loss: 97200.312500, loss bulk: 97200.312500, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 109, Loss: 97187.007812, loss bulk: 97187.007812, loss surf: 0.000008\n",
            "updating the parameters\n",
            "Train Epoch: 110, Loss: 97173.335938, loss bulk: 97173.335938, loss surf: 0.000008\n",
            "updating the parameters\n",
            "Train Epoch: 111, Loss: 97159.328125, loss bulk: 97159.328125, loss surf: 0.000008\n",
            "updating the parameters\n",
            "Train Epoch: 112, Loss: 97144.945312, loss bulk: 97144.945312, loss surf: 0.000007\n",
            "updating the parameters\n",
            "Train Epoch: 113, Loss: 97130.210938, loss bulk: 97130.210938, loss surf: 0.000006\n",
            "updating the parameters\n",
            "Train Epoch: 114, Loss: 97115.093750, loss bulk: 97115.093750, loss surf: 0.000006\n",
            "updating the parameters\n",
            "Train Epoch: 115, Loss: 97099.617188, loss bulk: 97099.617188, loss surf: 0.000005\n",
            "updating the parameters\n",
            "Train Epoch: 116, Loss: 97083.734375, loss bulk: 97083.734375, loss surf: 0.000005\n",
            "updating the parameters\n",
            "Train Epoch: 117, Loss: 97067.468750, loss bulk: 97067.468750, loss surf: 0.000004\n",
            "updating the parameters\n",
            "Train Epoch: 118, Loss: 97050.820312, loss bulk: 97050.820312, loss surf: 0.000003\n",
            "updating the parameters\n",
            "Train Epoch: 119, Loss: 97033.726562, loss bulk: 97033.726562, loss surf: 0.000003\n",
            "updating the parameters\n",
            "Train Epoch: 120, Loss: 97016.250000, loss bulk: 97016.250000, loss surf: 0.000002\n",
            "updating the parameters\n",
            "Train Epoch: 121, Loss: 96998.328125, loss bulk: 96998.328125, loss surf: 0.000002\n",
            "updating the parameters\n",
            "Train Epoch: 122, Loss: 96980.000000, loss bulk: 96980.000000, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 123, Loss: 96961.218750, loss bulk: 96961.218750, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 124, Loss: 96942.039062, loss bulk: 96942.039062, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 125, Loss: 96922.367188, loss bulk: 96922.367188, loss surf: 0.000000\n",
            "updating the parameters\n",
            "Train Epoch: 126, Loss: 96902.281250, loss bulk: 96902.281250, loss surf: 0.000000\n",
            "updating the parameters\n",
            "Train Epoch: 127, Loss: 96881.695312, loss bulk: 96881.695312, loss surf: 0.000000\n",
            "updating the parameters\n",
            "Train Epoch: 128, Loss: 96860.664062, loss bulk: 96860.664062, loss surf: 0.000000\n",
            "updating the parameters\n",
            "Train Epoch: 129, Loss: 96839.140625, loss bulk: 96839.140625, loss surf: 0.000000\n",
            "updating the parameters\n",
            "Train Epoch: 130, Loss: 96817.156250, loss bulk: 96817.156250, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 131, Loss: 96794.656250, loss bulk: 96794.656250, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 132, Loss: 96771.703125, loss bulk: 96771.703125, loss surf: 0.000001\n",
            "updating the parameters\n",
            "Train Epoch: 133, Loss: 96748.179688, loss bulk: 96748.179688, loss surf: 0.000002\n",
            "updating the parameters\n",
            "Train Epoch: 134, Loss: 96724.195312, loss bulk: 96724.195312, loss surf: 0.000003\n",
            "updating the parameters\n",
            "Train Epoch: 135, Loss: 96699.648438, loss bulk: 96699.648438, loss surf: 0.000004\n",
            "updating the parameters\n",
            "Train Epoch: 136, Loss: 96674.617188, loss bulk: 96674.617188, loss surf: 0.000005\n",
            "updating the parameters\n",
            "Train Epoch: 137, Loss: 96649.031250, loss bulk: 96649.031250, loss surf: 0.000007\n",
            "updating the parameters\n",
            "Train Epoch: 138, Loss: 96622.921875, loss bulk: 96622.921875, loss surf: 0.000009\n",
            "updating the parameters\n",
            "Train Epoch: 139, Loss: 96596.273438, loss bulk: 96596.273438, loss surf: 0.000011\n",
            "updating the parameters\n",
            "Train Epoch: 140, Loss: 96569.031250, loss bulk: 96569.031250, loss surf: 0.000013\n",
            "updating the parameters\n",
            "Train Epoch: 141, Loss: 96541.242188, loss bulk: 96541.242188, loss surf: 0.000016\n",
            "updating the parameters\n",
            "Train Epoch: 142, Loss: 96512.882812, loss bulk: 96512.882812, loss surf: 0.000019\n",
            "updating the parameters\n",
            "Train Epoch: 143, Loss: 96483.937500, loss bulk: 96483.937500, loss surf: 0.000023\n",
            "updating the parameters\n",
            "Train Epoch: 144, Loss: 96454.453125, loss bulk: 96454.453125, loss surf: 0.000027\n",
            "updating the parameters\n",
            "Train Epoch: 145, Loss: 96424.375000, loss bulk: 96424.375000, loss surf: 0.000032\n",
            "updating the parameters\n",
            "Train Epoch: 146, Loss: 96393.671875, loss bulk: 96393.671875, loss surf: 0.000038\n",
            "updating the parameters\n",
            "Train Epoch: 147, Loss: 96362.382812, loss bulk: 96362.382812, loss surf: 0.000044\n",
            "updating the parameters\n",
            "Train Epoch: 148, Loss: 96330.484375, loss bulk: 96330.484375, loss surf: 0.000051\n",
            "updating the parameters\n",
            "Train Epoch: 149, Loss: 96298.070312, loss bulk: 96298.070312, loss surf: 0.000059\n",
            "updating the parameters\n",
            "Train Epoch: 150, Loss: 96264.835938, loss bulk: 96264.835938, loss surf: 0.000068\n",
            "updating the parameters\n",
            "Train Epoch: 151, Loss: 96231.156250, loss bulk: 96231.156250, loss surf: 0.000078\n",
            "updating the parameters\n",
            "Train Epoch: 152, Loss: 96196.695312, loss bulk: 96196.695312, loss surf: 0.000090\n",
            "updating the parameters\n",
            "Train Epoch: 153, Loss: 96161.718750, loss bulk: 96161.718750, loss surf: 0.000103\n",
            "updating the parameters\n",
            "Train Epoch: 154, Loss: 96126.046875, loss bulk: 96126.046875, loss surf: 0.000118\n",
            "updating the parameters\n",
            "Train Epoch: 155, Loss: 96089.796875, loss bulk: 96089.796875, loss surf: 0.000136\n",
            "updating the parameters\n",
            "Train Epoch: 156, Loss: 96052.828125, loss bulk: 96052.828125, loss surf: 0.000156\n",
            "updating the parameters\n",
            "Train Epoch: 157, Loss: 96015.242188, loss bulk: 96015.242188, loss surf: 0.000178\n",
            "updating the parameters\n",
            "Train Epoch: 158, Loss: 95976.976562, loss bulk: 95976.976562, loss surf: 0.000204\n",
            "updating the parameters\n",
            "Train Epoch: 159, Loss: 95938.046875, loss bulk: 95938.046875, loss surf: 0.000234\n",
            "updating the parameters\n",
            "Train Epoch: 160, Loss: 95898.437500, loss bulk: 95898.437500, loss surf: 0.000268\n",
            "updating the parameters\n",
            "Train Epoch: 161, Loss: 95858.195312, loss bulk: 95858.195312, loss surf: 0.000307\n",
            "updating the parameters\n",
            "Train Epoch: 162, Loss: 95817.257812, loss bulk: 95817.257812, loss surf: 0.000351\n",
            "updating the parameters\n",
            "Train Epoch: 163, Loss: 95775.703125, loss bulk: 95775.703125, loss surf: 0.000402\n",
            "updating the parameters\n",
            "Train Epoch: 164, Loss: 95733.390625, loss bulk: 95733.390625, loss surf: 0.000461\n",
            "updating the parameters\n",
            "Train Epoch: 165, Loss: 95690.453125, loss bulk: 95690.453125, loss surf: 0.000527\n",
            "updating the parameters\n",
            "Train Epoch: 166, Loss: 95646.789062, loss bulk: 95646.789062, loss surf: 0.000604\n",
            "updating the parameters\n",
            "Train Epoch: 167, Loss: 95602.476562, loss bulk: 95602.476562, loss surf: 0.000691\n",
            "updating the parameters\n",
            "Train Epoch: 168, Loss: 95557.460938, loss bulk: 95557.460938, loss surf: 0.000792\n",
            "updating the parameters\n",
            "Train Epoch: 169, Loss: 95511.804688, loss bulk: 95511.804688, loss surf: 0.000906\n",
            "updating the parameters\n",
            "Train Epoch: 170, Loss: 95465.414062, loss bulk: 95465.414062, loss surf: 0.001036\n",
            "updating the parameters\n",
            "Train Epoch: 171, Loss: 95418.320312, loss bulk: 95418.320312, loss surf: 0.001185\n",
            "updating the parameters\n",
            "Train Epoch: 172, Loss: 95370.648438, loss bulk: 95370.648438, loss surf: 0.001354\n",
            "updating the parameters\n",
            "Train Epoch: 173, Loss: 95322.179688, loss bulk: 95322.179688, loss surf: 0.001545\n",
            "updating the parameters\n",
            "Train Epoch: 174, Loss: 95273.093750, loss bulk: 95273.093750, loss surf: 0.001762\n",
            "updating the parameters\n",
            "Train Epoch: 175, Loss: 95223.273438, loss bulk: 95223.273438, loss surf: 0.002008\n",
            "updating the parameters\n",
            "Train Epoch: 176, Loss: 95172.765625, loss bulk: 95172.765625, loss surf: 0.002284\n",
            "updating the parameters\n",
            "Train Epoch: 177, Loss: 95121.593750, loss bulk: 95121.593750, loss surf: 0.002596\n",
            "updating the parameters\n",
            "Train Epoch: 178, Loss: 95069.851562, loss bulk: 95069.851562, loss surf: 0.002945\n",
            "updating the parameters\n",
            "Train Epoch: 179, Loss: 95017.281250, loss bulk: 95017.281250, loss surf: 0.003336\n",
            "updating the parameters\n",
            "Train Epoch: 180, Loss: 94964.101562, loss bulk: 94964.101562, loss surf: 0.003772\n",
            "updating the parameters\n",
            "Train Epoch: 181, Loss: 94910.335938, loss bulk: 94910.328125, loss surf: 0.004257\n",
            "updating the parameters\n",
            "Train Epoch: 182, Loss: 94855.796875, loss bulk: 94855.789062, loss surf: 0.004795\n",
            "updating the parameters\n",
            "Train Epoch: 183, Loss: 94800.625000, loss bulk: 94800.617188, loss surf: 0.005388\n",
            "updating the parameters\n",
            "Train Epoch: 184, Loss: 94744.851562, loss bulk: 94744.843750, loss surf: 0.006040\n",
            "updating the parameters\n",
            "Train Epoch: 185, Loss: 94688.351562, loss bulk: 94688.343750, loss surf: 0.006754\n",
            "updating the parameters\n",
            "Train Epoch: 186, Loss: 94631.273438, loss bulk: 94631.265625, loss surf: 0.007532\n",
            "updating the parameters\n",
            "Train Epoch: 187, Loss: 94573.515625, loss bulk: 94573.507812, loss surf: 0.008374\n",
            "updating the parameters\n",
            "Train Epoch: 188, Loss: 94515.218750, loss bulk: 94515.210938, loss surf: 0.009282\n",
            "updating the parameters\n",
            "Train Epoch: 189, Loss: 94456.218750, loss bulk: 94456.210938, loss surf: 0.010253\n",
            "updating the parameters\n",
            "Train Epoch: 190, Loss: 94396.609375, loss bulk: 94396.601562, loss surf: 0.011287\n",
            "updating the parameters\n",
            "Train Epoch: 191, Loss: 94336.437500, loss bulk: 94336.421875, loss surf: 0.012380\n",
            "updating the parameters\n",
            "Train Epoch: 192, Loss: 94275.617188, loss bulk: 94275.601562, loss surf: 0.013527\n",
            "updating the parameters\n",
            "Train Epoch: 193, Loss: 94214.203125, loss bulk: 94214.187500, loss surf: 0.014721\n",
            "updating the parameters\n",
            "Train Epoch: 194, Loss: 94152.281250, loss bulk: 94152.265625, loss surf: 0.015957\n",
            "updating the parameters\n",
            "Train Epoch: 195, Loss: 94089.742188, loss bulk: 94089.726562, loss surf: 0.017226\n",
            "updating the parameters\n",
            "Train Epoch: 196, Loss: 94026.625000, loss bulk: 94026.609375, loss surf: 0.018521\n",
            "updating the parameters\n",
            "Train Epoch: 197, Loss: 93963.046875, loss bulk: 93963.023438, loss surf: 0.019834\n",
            "updating the parameters\n",
            "Train Epoch: 198, Loss: 93898.781250, loss bulk: 93898.757812, loss surf: 0.021159\n",
            "updating the parameters\n",
            "Train Epoch: 199, Loss: 93834.046875, loss bulk: 93834.023438, loss surf: 0.022490\n",
            "updating the parameters\n",
            "Train Epoch: 200, Loss: 93768.820312, loss bulk: 93768.796875, loss surf: 0.023823\n",
            "updating the parameters\n",
            "Train Epoch: 201, Loss: 93703.070312, loss bulk: 93703.046875, loss surf: 0.025158\n",
            "updating the parameters\n",
            "Train Epoch: 202, Loss: 93636.835938, loss bulk: 93636.812500, loss surf: 0.026494\n",
            "updating the parameters\n",
            "Train Epoch: 203, Loss: 93570.101562, loss bulk: 93570.070312, loss surf: 0.027836\n",
            "updating the parameters\n",
            "Train Epoch: 204, Loss: 93502.929688, loss bulk: 93502.898438, loss surf: 0.029190\n",
            "updating the parameters\n",
            "Train Epoch: 205, Loss: 93435.226562, loss bulk: 93435.195312, loss surf: 0.030563\n",
            "updating the parameters\n",
            "Train Epoch: 206, Loss: 93367.179688, loss bulk: 93367.148438, loss surf: 0.031968\n",
            "updating the parameters\n",
            "Train Epoch: 207, Loss: 93298.601562, loss bulk: 93298.570312, loss surf: 0.033416\n",
            "updating the parameters\n",
            "Train Epoch: 208, Loss: 93229.679688, loss bulk: 93229.648438, loss surf: 0.034922\n",
            "updating the parameters\n",
            "Train Epoch: 209, Loss: 93160.296875, loss bulk: 93160.257812, loss surf: 0.036503\n",
            "updating the parameters\n",
            "Train Epoch: 210, Loss: 93090.562500, loss bulk: 93090.523438, loss surf: 0.038173\n",
            "updating the parameters\n",
            "Train Epoch: 211, Loss: 93020.468750, loss bulk: 93020.429688, loss surf: 0.039950\n",
            "updating the parameters\n",
            "Train Epoch: 212, Loss: 92949.976562, loss bulk: 92949.937500, loss surf: 0.041847\n",
            "updating the parameters\n",
            "Train Epoch: 213, Loss: 92879.117188, loss bulk: 92879.070312, loss surf: 0.043881\n",
            "updating the parameters\n",
            "Train Epoch: 214, Loss: 92807.968750, loss bulk: 92807.921875, loss surf: 0.046063\n",
            "updating the parameters\n",
            "Train Epoch: 215, Loss: 92736.476562, loss bulk: 92736.429688, loss surf: 0.048404\n",
            "updating the parameters\n",
            "Train Epoch: 216, Loss: 92664.710938, loss bulk: 92664.656250, loss surf: 0.050912\n",
            "updating the parameters\n",
            "Train Epoch: 217, Loss: 92592.632812, loss bulk: 92592.578125, loss surf: 0.053597\n",
            "updating the parameters\n",
            "Train Epoch: 218, Loss: 92520.257812, loss bulk: 92520.203125, loss surf: 0.056461\n",
            "updating the parameters\n",
            "Train Epoch: 219, Loss: 92447.664062, loss bulk: 92447.601562, loss surf: 0.059509\n",
            "updating the parameters\n",
            "Train Epoch: 220, Loss: 92374.820312, loss bulk: 92374.757812, loss surf: 0.062741\n",
            "updating the parameters\n",
            "Train Epoch: 221, Loss: 92301.710938, loss bulk: 92301.648438, loss surf: 0.066157\n",
            "updating the parameters\n",
            "Train Epoch: 222, Loss: 92228.460938, loss bulk: 92228.390625, loss surf: 0.069756\n",
            "updating the parameters\n",
            "Train Epoch: 223, Loss: 92154.968750, loss bulk: 92154.898438, loss surf: 0.073535\n",
            "updating the parameters\n",
            "Train Epoch: 224, Loss: 92081.335938, loss bulk: 92081.257812, loss surf: 0.077490\n",
            "updating the parameters\n",
            "Train Epoch: 225, Loss: 92007.507812, loss bulk: 92007.429688, loss surf: 0.081617\n",
            "updating the parameters\n",
            "Train Epoch: 226, Loss: 91933.484375, loss bulk: 91933.398438, loss surf: 0.085915\n",
            "updating the parameters\n",
            "Train Epoch: 227, Loss: 91859.382812, loss bulk: 91859.289062, loss surf: 0.090379\n",
            "updating the parameters\n",
            "Train Epoch: 228, Loss: 91785.132812, loss bulk: 91785.039062, loss surf: 0.095010\n",
            "updating the parameters\n",
            "Train Epoch: 229, Loss: 91710.867188, loss bulk: 91710.765625, loss surf: 0.099807\n",
            "updating the parameters\n",
            "Train Epoch: 230, Loss: 91636.421875, loss bulk: 91636.320312, loss surf: 0.104775\n",
            "updating the parameters\n",
            "Train Epoch: 231, Loss: 91561.953125, loss bulk: 91561.843750, loss surf: 0.109920\n",
            "updating the parameters\n",
            "Train Epoch: 232, Loss: 91487.367188, loss bulk: 91487.250000, loss surf: 0.115251\n",
            "updating the parameters\n",
            "Train Epoch: 233, Loss: 91412.765625, loss bulk: 91412.648438, loss surf: 0.120780\n",
            "updating the parameters\n",
            "Train Epoch: 234, Loss: 91338.179688, loss bulk: 91338.054688, loss surf: 0.126523\n",
            "updating the parameters\n",
            "Train Epoch: 235, Loss: 91263.523438, loss bulk: 91263.390625, loss surf: 0.132499\n",
            "updating the parameters\n",
            "Train Epoch: 236, Loss: 91188.882812, loss bulk: 91188.742188, loss surf: 0.138730\n",
            "updating the parameters\n",
            "Train Epoch: 237, Loss: 91114.242188, loss bulk: 91114.093750, loss surf: 0.145238\n",
            "updating the parameters\n",
            "Train Epoch: 238, Loss: 91039.687500, loss bulk: 91039.539062, loss surf: 0.152050\n",
            "updating the parameters\n",
            "Train Epoch: 239, Loss: 90965.125000, loss bulk: 90964.968750, loss surf: 0.159192\n",
            "updating the parameters\n",
            "Train Epoch: 240, Loss: 90890.625000, loss bulk: 90890.460938, loss surf: 0.166694\n",
            "updating the parameters\n",
            "Train Epoch: 241, Loss: 90816.203125, loss bulk: 90816.031250, loss surf: 0.174582\n",
            "updating the parameters\n",
            "Train Epoch: 242, Loss: 90741.875000, loss bulk: 90741.695312, loss surf: 0.182885\n",
            "updating the parameters\n",
            "Train Epoch: 243, Loss: 90667.632812, loss bulk: 90667.437500, loss surf: 0.191631\n",
            "updating the parameters\n",
            "Train Epoch: 244, Loss: 90593.523438, loss bulk: 90593.320312, loss surf: 0.200845\n",
            "updating the parameters\n",
            "Train Epoch: 245, Loss: 90519.500000, loss bulk: 90519.289062, loss surf: 0.210551\n",
            "updating the parameters\n",
            "Train Epoch: 246, Loss: 90445.632812, loss bulk: 90445.414062, loss surf: 0.220769\n",
            "updating the parameters\n",
            "Train Epoch: 247, Loss: 90371.921875, loss bulk: 90371.687500, loss surf: 0.231518\n",
            "updating the parameters\n",
            "Train Epoch: 248, Loss: 90298.343750, loss bulk: 90298.101562, loss surf: 0.242817\n",
            "updating the parameters\n",
            "Train Epoch: 249, Loss: 90224.968750, loss bulk: 90224.710938, loss surf: 0.254679\n",
            "updating the parameters\n",
            "Train Epoch: 250, Loss: 90151.750000, loss bulk: 90151.484375, loss surf: 0.267122\n",
            "updating the parameters\n",
            "Train Epoch: 251, Loss: 90078.718750, loss bulk: 90078.437500, loss surf: 0.280161\n",
            "updating the parameters\n",
            "Train Epoch: 252, Loss: 90005.898438, loss bulk: 90005.601562, loss surf: 0.293814\n",
            "updating the parameters\n",
            "Train Epoch: 253, Loss: 89933.320312, loss bulk: 89933.015625, loss surf: 0.308103\n",
            "updating the parameters\n",
            "Train Epoch: 254, Loss: 89860.929688, loss bulk: 89860.609375, loss surf: 0.323052\n",
            "updating the parameters\n",
            "Train Epoch: 255, Loss: 89788.796875, loss bulk: 89788.460938, loss surf: 0.338690\n",
            "updating the parameters\n",
            "Train Epoch: 256, Loss: 89716.882812, loss bulk: 89716.531250, loss surf: 0.355048\n",
            "updating the parameters\n",
            "Train Epoch: 257, Loss: 89645.273438, loss bulk: 89644.898438, loss surf: 0.372161\n",
            "updating the parameters\n",
            "Train Epoch: 258, Loss: 89573.843750, loss bulk: 89573.453125, loss surf: 0.390065\n",
            "updating the parameters\n",
            "Train Epoch: 259, Loss: 89502.773438, loss bulk: 89502.367188, loss surf: 0.408796\n",
            "updating the parameters\n",
            "Train Epoch: 260, Loss: 89431.859375, loss bulk: 89431.429688, loss surf: 0.428392\n",
            "updating the parameters\n",
            "Train Epoch: 261, Loss: 89361.351562, loss bulk: 89360.906250, loss surf: 0.448886\n",
            "updating the parameters\n",
            "Train Epoch: 262, Loss: 89291.070312, loss bulk: 89290.601562, loss surf: 0.470312\n",
            "updating the parameters\n",
            "Train Epoch: 263, Loss: 89221.078125, loss bulk: 89220.585938, loss surf: 0.492698\n",
            "updating the parameters\n",
            "Train Epoch: 264, Loss: 89151.492188, loss bulk: 89150.976562, loss surf: 0.516070\n",
            "updating the parameters\n",
            "Train Epoch: 265, Loss: 89082.117188, loss bulk: 89081.578125, loss surf: 0.540452\n",
            "updating the parameters\n",
            "Train Epoch: 266, Loss: 89013.070312, loss bulk: 89012.507812, loss surf: 0.565861\n",
            "updating the parameters\n",
            "Train Epoch: 267, Loss: 88944.343750, loss bulk: 88943.750000, loss surf: 0.592309\n",
            "updating the parameters\n",
            "Train Epoch: 268, Loss: 88876.007812, loss bulk: 88875.390625, loss surf: 0.619804\n",
            "updating the parameters\n",
            "Train Epoch: 269, Loss: 88807.992188, loss bulk: 88807.343750, loss surf: 0.648343\n",
            "updating the parameters\n",
            "Train Epoch: 270, Loss: 88740.273438, loss bulk: 88739.593750, loss surf: 0.677918\n",
            "updating the parameters\n",
            "Train Epoch: 271, Loss: 88672.960938, loss bulk: 88672.250000, loss surf: 0.708510\n",
            "updating the parameters\n",
            "Train Epoch: 272, Loss: 88605.945312, loss bulk: 88605.203125, loss surf: 0.740088\n",
            "updating the parameters\n",
            "Train Epoch: 273, Loss: 88539.312500, loss bulk: 88538.539062, loss surf: 0.772615\n",
            "updating the parameters\n",
            "Train Epoch: 274, Loss: 88473.007812, loss bulk: 88472.203125, loss surf: 0.806048\n",
            "updating the parameters\n",
            "Train Epoch: 275, Loss: 88407.132812, loss bulk: 88406.289062, loss surf: 0.840346\n",
            "updating the parameters\n",
            "Train Epoch: 276, Loss: 88341.554688, loss bulk: 88340.679688, loss surf: 0.875469\n",
            "updating the parameters\n",
            "Train Epoch: 277, Loss: 88276.367188, loss bulk: 88275.453125, loss surf: 0.911381\n",
            "updating the parameters\n",
            "Train Epoch: 278, Loss: 88211.539062, loss bulk: 88210.593750, loss surf: 0.948045\n",
            "updating the parameters\n",
            "Train Epoch: 279, Loss: 88147.109375, loss bulk: 88146.125000, loss surf: 0.985421\n",
            "updating the parameters\n",
            "Train Epoch: 280, Loss: 88083.054688, loss bulk: 88082.031250, loss surf: 1.023457\n",
            "updating the parameters\n",
            "Train Epoch: 281, Loss: 88019.390625, loss bulk: 88018.328125, loss surf: 1.062091\n",
            "updating the parameters\n",
            "Train Epoch: 282, Loss: 87956.109375, loss bulk: 87955.007812, loss surf: 1.101246\n",
            "updating the parameters\n",
            "Train Epoch: 283, Loss: 87893.156250, loss bulk: 87892.015625, loss surf: 1.140831\n",
            "updating the parameters\n",
            "Train Epoch: 284, Loss: 87830.664062, loss bulk: 87829.484375, loss surf: 1.180744\n",
            "updating the parameters\n",
            "Train Epoch: 285, Loss: 87768.500000, loss bulk: 87767.281250, loss surf: 1.220881\n",
            "updating the parameters\n",
            "Train Epoch: 286, Loss: 87706.750000, loss bulk: 87705.492188, loss surf: 1.261135\n",
            "updating the parameters\n",
            "Train Epoch: 287, Loss: 87645.429688, loss bulk: 87644.125000, loss surf: 1.301406\n",
            "updating the parameters\n",
            "Train Epoch: 288, Loss: 87584.406250, loss bulk: 87583.062500, loss surf: 1.341596\n",
            "updating the parameters\n",
            "Train Epoch: 289, Loss: 87523.859375, loss bulk: 87522.476562, loss surf: 1.381613\n",
            "updating the parameters\n",
            "Train Epoch: 290, Loss: 87463.679688, loss bulk: 87462.257812, loss surf: 1.421361\n",
            "updating the parameters\n",
            "Train Epoch: 291, Loss: 87403.859375, loss bulk: 87402.398438, loss surf: 1.460741\n",
            "updating the parameters\n",
            "Train Epoch: 292, Loss: 87344.468750, loss bulk: 87342.968750, loss surf: 1.499646\n",
            "updating the parameters\n",
            "Train Epoch: 293, Loss: 87285.445312, loss bulk: 87283.906250, loss surf: 1.537968\n",
            "updating the parameters\n",
            "Train Epoch: 294, Loss: 87226.835938, loss bulk: 87225.257812, loss surf: 1.575605\n",
            "updating the parameters\n",
            "Train Epoch: 295, Loss: 87168.570312, loss bulk: 87166.960938, loss surf: 1.612472\n",
            "updating the parameters\n",
            "Train Epoch: 296, Loss: 87110.750000, loss bulk: 87109.101562, loss surf: 1.648510\n",
            "updating the parameters\n",
            "Train Epoch: 297, Loss: 87053.304688, loss bulk: 87051.617188, loss surf: 1.683687\n",
            "updating the parameters\n",
            "Train Epoch: 298, Loss: 86996.234375, loss bulk: 86994.515625, loss surf: 1.718002\n",
            "updating the parameters\n",
            "Train Epoch: 299, Loss: 86939.484375, loss bulk: 86937.734375, loss surf: 1.751478\n",
            "updating the parameters\n",
            "Train Epoch: 300, Loss: 86883.210938, loss bulk: 86881.429688, loss surf: 1.784163\n",
            "updating the parameters\n",
            "Train Epoch: 301, Loss: 86827.289062, loss bulk: 86825.476562, loss surf: 1.816124\n",
            "updating the parameters\n",
            "Train Epoch: 302, Loss: 86771.742188, loss bulk: 86769.898438, loss surf: 1.847447\n",
            "updating the parameters\n",
            "Train Epoch: 303, Loss: 86716.601562, loss bulk: 86714.726562, loss surf: 1.878238\n",
            "updating the parameters\n",
            "Train Epoch: 304, Loss: 86661.757812, loss bulk: 86659.851562, loss surf: 1.908618\n",
            "updating the parameters\n",
            "Train Epoch: 305, Loss: 86607.343750, loss bulk: 86605.406250, loss surf: 1.938718\n",
            "updating the parameters\n",
            "Train Epoch: 306, Loss: 86553.343750, loss bulk: 86551.375000, loss surf: 1.968670\n",
            "updating the parameters\n",
            "Train Epoch: 307, Loss: 86499.640625, loss bulk: 86497.640625, loss surf: 1.998601\n",
            "updating the parameters\n",
            "Train Epoch: 308, Loss: 86446.335938, loss bulk: 86444.304688, loss surf: 2.028622\n",
            "updating the parameters\n",
            "Train Epoch: 309, Loss: 86393.375000, loss bulk: 86391.312500, loss surf: 2.058818\n",
            "updating the parameters\n",
            "Train Epoch: 310, Loss: 86340.750000, loss bulk: 86338.664062, loss surf: 2.089248\n",
            "updating the parameters\n",
            "Train Epoch: 311, Loss: 86288.562500, loss bulk: 86286.445312, loss surf: 2.119938\n",
            "updating the parameters\n",
            "Train Epoch: 312, Loss: 86236.656250, loss bulk: 86234.507812, loss surf: 2.150874\n",
            "updating the parameters\n",
            "Train Epoch: 313, Loss: 86185.117188, loss bulk: 86182.937500, loss surf: 2.182000\n",
            "updating the parameters\n",
            "Train Epoch: 314, Loss: 86133.898438, loss bulk: 86131.687500, loss surf: 2.213212\n",
            "updating the parameters\n",
            "Train Epoch: 315, Loss: 86083.070312, loss bulk: 86080.828125, loss surf: 2.244354\n",
            "updating the parameters\n",
            "Train Epoch: 316, Loss: 86032.562500, loss bulk: 86030.289062, loss surf: 2.275211\n",
            "updating the parameters\n",
            "Train Epoch: 317, Loss: 85982.382812, loss bulk: 85980.078125, loss surf: 2.305505\n",
            "updating the parameters\n",
            "Train Epoch: 318, Loss: 85932.546875, loss bulk: 85930.210938, loss surf: 2.334896\n",
            "updating the parameters\n",
            "Train Epoch: 319, Loss: 85883.078125, loss bulk: 85880.718750, loss surf: 2.362980\n",
            "updating the parameters\n",
            "Train Epoch: 320, Loss: 85833.890625, loss bulk: 85831.500000, loss surf: 2.389298\n",
            "updating the parameters\n",
            "Train Epoch: 321, Loss: 85785.031250, loss bulk: 85782.617188, loss surf: 2.413342\n",
            "updating the parameters\n",
            "Train Epoch: 322, Loss: 85736.507812, loss bulk: 85734.070312, loss surf: 2.434581\n",
            "updating the parameters\n",
            "Train Epoch: 323, Loss: 85688.296875, loss bulk: 85685.843750, loss surf: 2.452495\n",
            "updating the parameters\n",
            "Train Epoch: 324, Loss: 85640.460938, loss bulk: 85637.992188, loss surf: 2.466645\n",
            "updating the parameters\n",
            "Train Epoch: 325, Loss: 85592.828125, loss bulk: 85590.351562, loss surf: 2.476746\n",
            "updating the parameters\n",
            "Train Epoch: 326, Loss: 85545.570312, loss bulk: 85543.085938, loss surf: 2.482769\n",
            "updating the parameters\n",
            "Train Epoch: 327, Loss: 85498.656250, loss bulk: 85496.171875, loss surf: 2.485025\n",
            "updating the parameters\n",
            "Train Epoch: 328, Loss: 85451.984375, loss bulk: 85449.500000, loss surf: 2.484203\n",
            "updating the parameters\n",
            "Train Epoch: 329, Loss: 85405.664062, loss bulk: 85403.179688, loss surf: 2.481355\n",
            "updating the parameters\n",
            "Train Epoch: 330, Loss: 85359.632812, loss bulk: 85357.156250, loss surf: 2.477769\n",
            "updating the parameters\n",
            "Train Epoch: 331, Loss: 85313.937500, loss bulk: 85311.460938, loss surf: 2.474772\n",
            "updating the parameters\n",
            "Train Epoch: 332, Loss: 85268.445312, loss bulk: 85265.968750, loss surf: 2.473493\n",
            "updating the parameters\n",
            "Train Epoch: 333, Loss: 85223.312500, loss bulk: 85220.835938, loss surf: 2.474694\n",
            "updating the parameters\n",
            "Train Epoch: 334, Loss: 85178.476562, loss bulk: 85176.000000, loss surf: 2.478748\n",
            "updating the parameters\n",
            "Train Epoch: 335, Loss: 85133.867188, loss bulk: 85131.382812, loss surf: 2.485731\n",
            "updating the parameters\n",
            "Train Epoch: 336, Loss: 85089.609375, loss bulk: 85087.117188, loss surf: 2.495570\n",
            "updating the parameters\n",
            "Train Epoch: 337, Loss: 85045.554688, loss bulk: 85043.046875, loss surf: 2.508152\n",
            "updating the parameters\n",
            "Train Epoch: 338, Loss: 85001.859375, loss bulk: 84999.335938, loss surf: 2.523365\n",
            "updating the parameters\n",
            "Train Epoch: 339, Loss: 84958.406250, loss bulk: 84955.867188, loss surf: 2.541088\n",
            "updating the parameters\n",
            "Train Epoch: 340, Loss: 84915.250000, loss bulk: 84912.687500, loss surf: 2.561129\n",
            "updating the parameters\n",
            "Train Epoch: 341, Loss: 84872.375000, loss bulk: 84869.789062, loss surf: 2.583162\n",
            "updating the parameters\n",
            "Train Epoch: 342, Loss: 84829.703125, loss bulk: 84827.093750, loss surf: 2.606696\n",
            "updating the parameters\n",
            "Train Epoch: 343, Loss: 84787.304688, loss bulk: 84784.671875, loss surf: 2.631092\n",
            "updating the parameters\n",
            "Train Epoch: 344, Loss: 84745.218750, loss bulk: 84742.562500, loss surf: 2.655629\n",
            "updating the parameters\n",
            "Train Epoch: 345, Loss: 84703.328125, loss bulk: 84700.648438, loss surf: 2.679621\n",
            "updating the parameters\n",
            "Train Epoch: 346, Loss: 84661.765625, loss bulk: 84659.062500, loss surf: 2.702518\n",
            "updating the parameters\n",
            "Train Epoch: 347, Loss: 84620.421875, loss bulk: 84617.695312, loss surf: 2.723972\n",
            "updating the parameters\n",
            "Train Epoch: 348, Loss: 84579.289062, loss bulk: 84576.546875, loss surf: 2.743824\n",
            "updating the parameters\n",
            "Train Epoch: 349, Loss: 84538.523438, loss bulk: 84535.757812, loss surf: 2.762038\n",
            "updating the parameters\n",
            "Train Epoch: 350, Loss: 84497.898438, loss bulk: 84495.117188, loss surf: 2.778594\n",
            "updating the parameters\n",
            "Train Epoch: 351, Loss: 84457.539062, loss bulk: 84454.742188, loss surf: 2.793437\n",
            "updating the parameters\n",
            "Train Epoch: 352, Loss: 84417.460938, loss bulk: 84414.656250, loss surf: 2.806474\n",
            "updating the parameters\n",
            "Train Epoch: 353, Loss: 84377.585938, loss bulk: 84374.765625, loss surf: 2.817659\n",
            "updating the parameters\n",
            "Train Epoch: 354, Loss: 84338.015625, loss bulk: 84335.187500, loss surf: 2.827078\n",
            "updating the parameters\n",
            "Train Epoch: 355, Loss: 84298.617188, loss bulk: 84295.781250, loss surf: 2.835002\n",
            "updating the parameters\n",
            "Train Epoch: 356, Loss: 84259.500000, loss bulk: 84256.656250, loss surf: 2.841853\n",
            "updating the parameters\n",
            "Train Epoch: 357, Loss: 84220.531250, loss bulk: 84217.679688, loss surf: 2.848098\n",
            "updating the parameters\n",
            "Train Epoch: 358, Loss: 84181.820312, loss bulk: 84178.968750, loss surf: 2.854127\n",
            "updating the parameters\n",
            "Train Epoch: 359, Loss: 84143.390625, loss bulk: 84140.531250, loss surf: 2.860196\n",
            "updating the parameters\n",
            "Train Epoch: 360, Loss: 84105.070312, loss bulk: 84102.203125, loss surf: 2.866449\n",
            "updating the parameters\n",
            "Train Epoch: 361, Loss: 84067.078125, loss bulk: 84064.203125, loss surf: 2.873003\n",
            "updating the parameters\n",
            "Train Epoch: 362, Loss: 84029.257812, loss bulk: 84026.375000, loss surf: 2.880028\n",
            "updating the parameters\n",
            "Train Epoch: 363, Loss: 83991.648438, loss bulk: 83988.757812, loss surf: 2.887749\n",
            "updating the parameters\n",
            "Train Epoch: 364, Loss: 83954.257812, loss bulk: 83951.359375, loss surf: 2.896376\n",
            "updating the parameters\n",
            "Train Epoch: 365, Loss: 83917.085938, loss bulk: 83914.179688, loss surf: 2.905993\n",
            "updating the parameters\n",
            "Train Epoch: 366, Loss: 83880.078125, loss bulk: 83877.164062, loss surf: 2.916524\n",
            "updating the parameters\n",
            "Train Epoch: 367, Loss: 83843.273438, loss bulk: 83840.343750, loss surf: 2.927777\n",
            "updating the parameters\n",
            "Train Epoch: 368, Loss: 83806.718750, loss bulk: 83803.781250, loss surf: 2.939555\n",
            "updating the parameters\n",
            "Train Epoch: 369, Loss: 83770.359375, loss bulk: 83767.406250, loss surf: 2.951734\n",
            "updating the parameters\n",
            "Train Epoch: 370, Loss: 83734.148438, loss bulk: 83731.187500, loss surf: 2.964262\n",
            "updating the parameters\n",
            "Train Epoch: 371, Loss: 83698.164062, loss bulk: 83695.187500, loss surf: 2.977080\n",
            "updating the parameters\n",
            "Train Epoch: 372, Loss: 83662.398438, loss bulk: 83659.406250, loss surf: 2.990050\n",
            "updating the parameters\n",
            "Train Epoch: 373, Loss: 83626.757812, loss bulk: 83623.757812, loss surf: 3.002966\n",
            "updating the parameters\n",
            "Train Epoch: 374, Loss: 83591.343750, loss bulk: 83588.328125, loss surf: 3.015633\n",
            "updating the parameters\n",
            "Train Epoch: 375, Loss: 83556.078125, loss bulk: 83553.046875, loss surf: 3.027959\n",
            "updating the parameters\n",
            "Train Epoch: 376, Loss: 83520.992188, loss bulk: 83517.953125, loss surf: 3.039966\n",
            "updating the parameters\n",
            "Train Epoch: 377, Loss: 83486.140625, loss bulk: 83483.085938, loss surf: 3.051733\n",
            "updating the parameters\n",
            "Train Epoch: 378, Loss: 83451.429688, loss bulk: 83448.367188, loss surf: 3.063314\n",
            "updating the parameters\n",
            "Train Epoch: 379, Loss: 83416.882812, loss bulk: 83413.804688, loss surf: 3.074713\n",
            "updating the parameters\n",
            "Train Epoch: 380, Loss: 83382.507812, loss bulk: 83379.421875, loss surf: 3.085931\n",
            "updating the parameters\n",
            "Train Epoch: 381, Loss: 83348.335938, loss bulk: 83345.242188, loss surf: 3.097029\n",
            "updating the parameters\n",
            "Train Epoch: 382, Loss: 83314.312500, loss bulk: 83311.203125, loss surf: 3.108145\n",
            "updating the parameters\n",
            "Train Epoch: 383, Loss: 83280.437500, loss bulk: 83277.320312, loss surf: 3.119444\n",
            "updating the parameters\n",
            "Train Epoch: 384, Loss: 83246.726562, loss bulk: 83243.593750, loss surf: 3.131051\n",
            "updating the parameters\n",
            "Train Epoch: 385, Loss: 83213.242188, loss bulk: 83210.101562, loss surf: 3.143012\n",
            "updating the parameters\n",
            "Train Epoch: 386, Loss: 83179.828125, loss bulk: 83176.671875, loss surf: 3.155324\n",
            "updating the parameters\n",
            "Train Epoch: 387, Loss: 83146.625000, loss bulk: 83143.453125, loss surf: 3.167993\n",
            "updating the parameters\n",
            "Train Epoch: 388, Loss: 83113.570312, loss bulk: 83110.390625, loss surf: 3.181051\n",
            "updating the parameters\n",
            "Train Epoch: 389, Loss: 83080.664062, loss bulk: 83077.468750, loss surf: 3.194533\n",
            "updating the parameters\n",
            "Train Epoch: 390, Loss: 83047.914062, loss bulk: 83044.703125, loss surf: 3.208422\n",
            "updating the parameters\n",
            "Train Epoch: 391, Loss: 83015.320312, loss bulk: 83012.101562, loss surf: 3.222629\n",
            "updating the parameters\n",
            "Train Epoch: 392, Loss: 82982.906250, loss bulk: 82979.671875, loss surf: 3.237025\n",
            "updating the parameters\n",
            "Train Epoch: 393, Loss: 82950.640625, loss bulk: 82947.390625, loss surf: 3.251487\n",
            "updating the parameters\n",
            "Train Epoch: 394, Loss: 82918.476562, loss bulk: 82915.210938, loss surf: 3.265931\n",
            "updating the parameters\n",
            "Train Epoch: 395, Loss: 82886.445312, loss bulk: 82883.164062, loss surf: 3.280299\n",
            "updating the parameters\n",
            "Train Epoch: 396, Loss: 82854.562500, loss bulk: 82851.265625, loss surf: 3.294526\n",
            "updating the parameters\n",
            "Train Epoch: 397, Loss: 82822.867188, loss bulk: 82819.562500, loss surf: 3.308522\n",
            "updating the parameters\n",
            "Train Epoch: 398, Loss: 82791.250000, loss bulk: 82787.929688, loss surf: 3.322199\n",
            "updating the parameters\n",
            "Train Epoch: 399, Loss: 82759.843750, loss bulk: 82756.507812, loss surf: 3.335512\n",
            "updating the parameters\n",
            "Train Epoch: 400, Loss: 82728.546875, loss bulk: 82725.195312, loss surf: 3.348475\n",
            "updating the parameters\n",
            "Train Epoch: 401, Loss: 82697.375000, loss bulk: 82694.015625, loss surf: 3.361146\n",
            "updating the parameters\n",
            "Train Epoch: 402, Loss: 82666.382812, loss bulk: 82663.007812, loss surf: 3.373588\n",
            "updating the parameters\n",
            "Train Epoch: 403, Loss: 82635.398438, loss bulk: 82632.015625, loss surf: 3.385853\n",
            "updating the parameters\n",
            "Train Epoch: 404, Loss: 82604.671875, loss bulk: 82601.273438, loss surf: 3.397992\n",
            "updating the parameters\n",
            "Train Epoch: 405, Loss: 82574.000000, loss bulk: 82570.593750, loss surf: 3.410075\n",
            "updating the parameters\n",
            "Train Epoch: 406, Loss: 82543.500000, loss bulk: 82540.078125, loss surf: 3.422199\n",
            "updating the parameters\n",
            "Train Epoch: 407, Loss: 82513.125000, loss bulk: 82509.687500, loss surf: 3.434461\n",
            "updating the parameters\n",
            "Train Epoch: 408, Loss: 82482.867188, loss bulk: 82479.421875, loss surf: 3.446930\n",
            "updating the parameters\n",
            "Train Epoch: 409, Loss: 82452.710938, loss bulk: 82449.250000, loss surf: 3.459635\n",
            "updating the parameters\n",
            "Train Epoch: 410, Loss: 82422.703125, loss bulk: 82419.234375, loss surf: 3.472573\n",
            "updating the parameters\n",
            "Train Epoch: 411, Loss: 82392.789062, loss bulk: 82389.304688, loss surf: 3.485738\n",
            "updating the parameters\n",
            "Train Epoch: 412, Loss: 82363.046875, loss bulk: 82359.546875, loss surf: 3.499124\n",
            "updating the parameters\n",
            "Train Epoch: 413, Loss: 82333.359375, loss bulk: 82329.843750, loss surf: 3.512712\n",
            "updating the parameters\n",
            "Train Epoch: 414, Loss: 82303.867188, loss bulk: 82300.343750, loss surf: 3.526455\n",
            "updating the parameters\n",
            "Train Epoch: 415, Loss: 82274.367188, loss bulk: 82270.828125, loss surf: 3.540278\n",
            "updating the parameters\n",
            "Train Epoch: 416, Loss: 82245.062500, loss bulk: 82241.507812, loss surf: 3.554108\n",
            "updating the parameters\n",
            "Train Epoch: 417, Loss: 82215.898438, loss bulk: 82212.328125, loss surf: 3.567889\n",
            "updating the parameters\n",
            "Train Epoch: 418, Loss: 82186.796875, loss bulk: 82183.218750, loss surf: 3.581588\n",
            "updating the parameters\n",
            "Train Epoch: 419, Loss: 82157.796875, loss bulk: 82154.203125, loss surf: 3.595178\n",
            "updating the parameters\n",
            "Train Epoch: 420, Loss: 82128.929688, loss bulk: 82125.320312, loss surf: 3.608635\n",
            "updating the parameters\n",
            "Train Epoch: 421, Loss: 82100.195312, loss bulk: 82096.570312, loss surf: 3.621939\n",
            "updating the parameters\n",
            "Train Epoch: 422, Loss: 82071.507812, loss bulk: 82067.875000, loss surf: 3.635090\n",
            "updating the parameters\n",
            "Train Epoch: 423, Loss: 82042.992188, loss bulk: 82039.343750, loss surf: 3.648116\n",
            "updating the parameters\n",
            "Train Epoch: 424, Loss: 82014.507812, loss bulk: 82010.843750, loss surf: 3.661060\n",
            "updating the parameters\n",
            "Train Epoch: 425, Loss: 81986.179688, loss bulk: 81982.507812, loss surf: 3.673961\n",
            "updating the parameters\n",
            "Train Epoch: 426, Loss: 81957.953125, loss bulk: 81954.265625, loss surf: 3.686852\n",
            "updating the parameters\n",
            "Train Epoch: 427, Loss: 81929.796875, loss bulk: 81926.093750, loss surf: 3.699758\n",
            "updating the parameters\n",
            "Train Epoch: 428, Loss: 81901.765625, loss bulk: 81898.054688, loss surf: 3.712709\n",
            "updating the parameters\n",
            "Train Epoch: 429, Loss: 81873.898438, loss bulk: 81870.171875, loss surf: 3.725736\n",
            "updating the parameters\n",
            "Train Epoch: 430, Loss: 81846.031250, loss bulk: 81842.289062, loss surf: 3.738858\n",
            "updating the parameters\n",
            "Train Epoch: 431, Loss: 81818.226562, loss bulk: 81814.476562, loss surf: 3.752073\n",
            "updating the parameters\n",
            "Train Epoch: 432, Loss: 81790.617188, loss bulk: 81786.851562, loss surf: 3.765365\n",
            "updating the parameters\n",
            "Train Epoch: 433, Loss: 81763.078125, loss bulk: 81759.296875, loss surf: 3.778713\n",
            "updating the parameters\n",
            "Train Epoch: 434, Loss: 81735.609375, loss bulk: 81731.820312, loss surf: 3.792098\n",
            "updating the parameters\n",
            "Train Epoch: 435, Loss: 81708.257812, loss bulk: 81704.453125, loss surf: 3.805504\n",
            "updating the parameters\n",
            "Train Epoch: 436, Loss: 81680.992188, loss bulk: 81677.171875, loss surf: 3.818908\n",
            "updating the parameters\n",
            "Train Epoch: 437, Loss: 81653.804688, loss bulk: 81649.968750, loss surf: 3.832284\n",
            "updating the parameters\n",
            "Train Epoch: 438, Loss: 81626.695312, loss bulk: 81622.851562, loss surf: 3.845612\n",
            "updating the parameters\n",
            "Train Epoch: 439, Loss: 81599.679688, loss bulk: 81595.820312, loss surf: 3.858882\n",
            "updating the parameters\n",
            "Train Epoch: 440, Loss: 81572.773438, loss bulk: 81568.898438, loss surf: 3.872103\n",
            "updating the parameters\n",
            "Train Epoch: 441, Loss: 81545.968750, loss bulk: 81542.085938, loss surf: 3.885284\n",
            "updating the parameters\n",
            "Train Epoch: 442, Loss: 81519.250000, loss bulk: 81515.351562, loss surf: 3.898437\n",
            "updating the parameters\n",
            "Train Epoch: 443, Loss: 81492.562500, loss bulk: 81488.648438, loss surf: 3.911572\n",
            "updating the parameters\n",
            "Train Epoch: 444, Loss: 81465.960938, loss bulk: 81462.039062, loss surf: 3.924709\n",
            "updating the parameters\n",
            "Train Epoch: 445, Loss: 81439.476562, loss bulk: 81435.539062, loss surf: 3.937871\n",
            "updating the parameters\n",
            "Train Epoch: 446, Loss: 81413.078125, loss bulk: 81409.125000, loss surf: 3.951078\n",
            "updating the parameters\n",
            "Train Epoch: 447, Loss: 81386.726562, loss bulk: 81382.765625, loss surf: 3.964341\n",
            "updating the parameters\n",
            "Train Epoch: 448, Loss: 81360.476562, loss bulk: 81356.500000, loss surf: 3.977663\n",
            "updating the parameters\n",
            "Train Epoch: 449, Loss: 81334.320312, loss bulk: 81330.328125, loss surf: 3.991039\n",
            "updating the parameters\n",
            "Train Epoch: 450, Loss: 81308.281250, loss bulk: 81304.273438, loss surf: 4.004468\n",
            "updating the parameters\n",
            "Train Epoch: 451, Loss: 81282.234375, loss bulk: 81278.218750, loss surf: 4.017941\n",
            "updating the parameters\n",
            "Train Epoch: 452, Loss: 81256.296875, loss bulk: 81252.265625, loss surf: 4.031443\n",
            "updating the parameters\n",
            "Train Epoch: 453, Loss: 81230.445312, loss bulk: 81226.398438, loss surf: 4.044955\n",
            "updating the parameters\n",
            "Train Epoch: 454, Loss: 81204.679688, loss bulk: 81200.625000, loss surf: 4.058460\n",
            "updating the parameters\n",
            "Train Epoch: 455, Loss: 81178.953125, loss bulk: 81174.882812, loss surf: 4.071944\n",
            "updating the parameters\n",
            "Train Epoch: 456, Loss: 81153.320312, loss bulk: 81149.234375, loss surf: 4.085401\n",
            "updating the parameters\n",
            "Train Epoch: 457, Loss: 81127.781250, loss bulk: 81123.679688, loss surf: 4.098825\n",
            "updating the parameters\n",
            "Train Epoch: 458, Loss: 81102.289062, loss bulk: 81098.179688, loss surf: 4.112212\n",
            "updating the parameters\n",
            "Train Epoch: 459, Loss: 81076.882812, loss bulk: 81072.757812, loss surf: 4.125566\n",
            "updating the parameters\n",
            "Train Epoch: 460, Loss: 81051.531250, loss bulk: 81047.390625, loss surf: 4.138894\n",
            "updating the parameters\n",
            "Train Epoch: 461, Loss: 81026.281250, loss bulk: 81022.132812, loss surf: 4.152208\n",
            "updating the parameters\n",
            "Train Epoch: 462, Loss: 81001.062500, loss bulk: 80996.898438, loss surf: 4.165521\n",
            "updating the parameters\n",
            "Train Epoch: 463, Loss: 80975.914062, loss bulk: 80971.734375, loss surf: 4.178843\n",
            "updating the parameters\n",
            "Train Epoch: 464, Loss: 80950.828125, loss bulk: 80946.632812, loss surf: 4.192181\n",
            "updating the parameters\n",
            "Train Epoch: 465, Loss: 80925.875000, loss bulk: 80921.671875, loss surf: 4.205542\n",
            "updating the parameters\n",
            "Train Epoch: 466, Loss: 80900.953125, loss bulk: 80896.734375, loss surf: 4.218934\n",
            "updating the parameters\n",
            "Train Epoch: 467, Loss: 80876.031250, loss bulk: 80871.796875, loss surf: 4.232358\n",
            "updating the parameters\n",
            "Train Epoch: 468, Loss: 80851.226562, loss bulk: 80846.984375, loss surf: 4.245810\n",
            "updating the parameters\n",
            "Train Epoch: 469, Loss: 80826.523438, loss bulk: 80822.265625, loss surf: 4.259286\n",
            "updating the parameters\n",
            "Train Epoch: 470, Loss: 80801.867188, loss bulk: 80797.593750, loss surf: 4.272782\n",
            "updating the parameters\n",
            "Train Epoch: 471, Loss: 80777.296875, loss bulk: 80773.007812, loss surf: 4.286293\n",
            "updating the parameters\n",
            "Train Epoch: 472, Loss: 80752.718750, loss bulk: 80748.421875, loss surf: 4.299816\n",
            "updating the parameters\n",
            "Train Epoch: 473, Loss: 80728.281250, loss bulk: 80723.968750, loss surf: 4.313347\n",
            "updating the parameters\n",
            "Train Epoch: 474, Loss: 80703.867188, loss bulk: 80699.539062, loss surf: 4.326883\n",
            "updating the parameters\n",
            "Train Epoch: 475, Loss: 80679.484375, loss bulk: 80675.140625, loss surf: 4.340424\n",
            "updating the parameters\n",
            "Train Epoch: 476, Loss: 80655.156250, loss bulk: 80650.804688, loss surf: 4.353975\n",
            "updating the parameters\n",
            "Train Epoch: 477, Loss: 80630.945312, loss bulk: 80626.578125, loss surf: 4.367538\n",
            "updating the parameters\n",
            "Train Epoch: 478, Loss: 80606.742188, loss bulk: 80602.359375, loss surf: 4.381117\n",
            "updating the parameters\n",
            "Train Epoch: 479, Loss: 80582.632812, loss bulk: 80578.234375, loss surf: 4.394713\n",
            "updating the parameters\n",
            "Train Epoch: 480, Loss: 80558.578125, loss bulk: 80554.171875, loss surf: 4.408329\n",
            "updating the parameters\n",
            "Train Epoch: 481, Loss: 80534.554688, loss bulk: 80530.132812, loss surf: 4.421969\n",
            "updating the parameters\n",
            "Train Epoch: 482, Loss: 80510.625000, loss bulk: 80506.187500, loss surf: 4.435631\n",
            "updating the parameters\n",
            "Train Epoch: 483, Loss: 80486.710938, loss bulk: 80482.257812, loss surf: 4.449312\n",
            "updating the parameters\n",
            "Train Epoch: 484, Loss: 80462.867188, loss bulk: 80458.406250, loss surf: 4.463010\n",
            "updating the parameters\n",
            "Train Epoch: 485, Loss: 80439.093750, loss bulk: 80434.617188, loss surf: 4.476721\n",
            "updating the parameters\n",
            "Train Epoch: 486, Loss: 80415.367188, loss bulk: 80410.875000, loss surf: 4.490442\n",
            "updating the parameters\n",
            "Train Epoch: 487, Loss: 80391.679688, loss bulk: 80387.171875, loss surf: 4.504169\n",
            "updating the parameters\n",
            "Train Epoch: 488, Loss: 80368.062500, loss bulk: 80363.546875, loss surf: 4.517900\n",
            "updating the parameters\n",
            "Train Epoch: 489, Loss: 80344.523438, loss bulk: 80339.992188, loss surf: 4.531632\n",
            "updating the parameters\n",
            "Train Epoch: 490, Loss: 80320.984375, loss bulk: 80316.437500, loss surf: 4.545365\n",
            "updating the parameters\n",
            "Train Epoch: 491, Loss: 80297.515625, loss bulk: 80292.953125, loss surf: 4.559103\n",
            "updating the parameters\n",
            "Train Epoch: 492, Loss: 80274.070312, loss bulk: 80269.500000, loss surf: 4.572845\n",
            "updating the parameters\n",
            "Train Epoch: 493, Loss: 80250.718750, loss bulk: 80246.132812, loss surf: 4.586593\n",
            "updating the parameters\n",
            "Train Epoch: 494, Loss: 80227.445312, loss bulk: 80222.843750, loss surf: 4.600352\n",
            "updating the parameters\n",
            "Train Epoch: 495, Loss: 80204.109375, loss bulk: 80199.492188, loss surf: 4.614122\n",
            "updating the parameters\n",
            "Train Epoch: 496, Loss: 80180.929688, loss bulk: 80176.304688, loss surf: 4.627907\n",
            "updating the parameters\n",
            "Train Epoch: 497, Loss: 80157.757812, loss bulk: 80153.117188, loss surf: 4.641708\n",
            "updating the parameters\n",
            "Train Epoch: 498, Loss: 80134.656250, loss bulk: 80130.000000, loss surf: 4.655523\n",
            "updating the parameters\n",
            "Train Epoch: 499, Loss: 80111.578125, loss bulk: 80106.906250, loss surf: 4.669353\n",
            "updating the parameters\n",
            "Train Epoch: 500, Loss: 80088.554688, loss bulk: 80083.875000, loss surf: 4.683197\n",
            "updating the parameters\n",
            "Train Epoch: 501, Loss: 80065.554688, loss bulk: 80060.859375, loss surf: 4.697054\n",
            "updating the parameters\n",
            "Train Epoch: 502, Loss: 80042.664062, loss bulk: 80037.953125, loss surf: 4.710924\n",
            "updating the parameters\n",
            "Train Epoch: 503, Loss: 80019.750000, loss bulk: 80015.023438, loss surf: 4.724806\n",
            "updating the parameters\n",
            "Train Epoch: 504, Loss: 79996.875000, loss bulk: 79992.132812, loss surf: 4.738698\n",
            "updating the parameters\n",
            "Train Epoch: 505, Loss: 79974.117188, loss bulk: 79969.367188, loss surf: 4.752604\n",
            "updating the parameters\n",
            "Train Epoch: 506, Loss: 79951.375000, loss bulk: 79946.609375, loss surf: 4.766522\n",
            "updating the parameters\n",
            "Train Epoch: 507, Loss: 79928.671875, loss bulk: 79923.890625, loss surf: 4.780454\n",
            "updating the parameters\n",
            "Train Epoch: 508, Loss: 79905.976562, loss bulk: 79901.179688, loss surf: 4.794400\n",
            "updating the parameters\n",
            "Train Epoch: 509, Loss: 79883.343750, loss bulk: 79878.539062, loss surf: 4.808361\n",
            "updating the parameters\n",
            "Train Epoch: 510, Loss: 79860.781250, loss bulk: 79855.960938, loss surf: 4.822338\n",
            "updating the parameters\n",
            "Train Epoch: 511, Loss: 79838.257812, loss bulk: 79833.421875, loss surf: 4.836329\n",
            "updating the parameters\n",
            "Train Epoch: 512, Loss: 79815.726562, loss bulk: 79810.875000, loss surf: 4.850335\n",
            "updating the parameters\n",
            "Train Epoch: 513, Loss: 79793.281250, loss bulk: 79788.414062, loss surf: 4.864354\n",
            "updating the parameters\n",
            "Train Epoch: 514, Loss: 79770.828125, loss bulk: 79765.953125, loss surf: 4.878385\n",
            "updating the parameters\n",
            "Train Epoch: 515, Loss: 79748.437500, loss bulk: 79743.546875, loss surf: 4.892426\n",
            "updating the parameters\n",
            "Train Epoch: 516, Loss: 79726.132812, loss bulk: 79721.226562, loss surf: 4.906477\n",
            "updating the parameters\n",
            "Train Epoch: 517, Loss: 79703.812500, loss bulk: 79698.890625, loss surf: 4.920538\n",
            "updating the parameters\n",
            "Train Epoch: 518, Loss: 79681.578125, loss bulk: 79676.640625, loss surf: 4.934608\n",
            "updating the parameters\n",
            "Train Epoch: 519, Loss: 79659.335938, loss bulk: 79654.390625, loss surf: 4.948689\n",
            "updating the parameters\n",
            "Train Epoch: 520, Loss: 79637.156250, loss bulk: 79632.195312, loss surf: 4.962779\n",
            "updating the parameters\n",
            "Train Epoch: 521, Loss: 79615.000000, loss bulk: 79610.023438, loss surf: 4.976882\n",
            "updating the parameters\n",
            "Train Epoch: 522, Loss: 79592.890625, loss bulk: 79587.898438, loss surf: 4.990996\n",
            "updating the parameters\n",
            "Train Epoch: 523, Loss: 79570.812500, loss bulk: 79565.804688, loss surf: 5.005124\n",
            "updating the parameters\n",
            "Train Epoch: 524, Loss: 79548.796875, loss bulk: 79543.781250, loss surf: 5.019266\n",
            "updating the parameters\n",
            "Train Epoch: 525, Loss: 79526.765625, loss bulk: 79521.734375, loss surf: 5.033422\n",
            "updating the parameters\n",
            "Train Epoch: 526, Loss: 79504.796875, loss bulk: 79499.750000, loss surf: 5.047591\n",
            "updating the parameters\n",
            "Train Epoch: 527, Loss: 79482.882812, loss bulk: 79477.820312, loss surf: 5.061773\n",
            "updating the parameters\n",
            "Train Epoch: 528, Loss: 79460.976562, loss bulk: 79455.898438, loss surf: 5.075969\n",
            "updating the parameters\n",
            "Train Epoch: 529, Loss: 79439.085938, loss bulk: 79433.992188, loss surf: 5.090177\n",
            "updating the parameters\n",
            "Train Epoch: 530, Loss: 79417.257812, loss bulk: 79412.156250, loss surf: 5.104396\n",
            "updating the parameters\n",
            "Train Epoch: 531, Loss: 79395.468750, loss bulk: 79390.351562, loss surf: 5.118628\n",
            "updating the parameters\n",
            "Train Epoch: 532, Loss: 79373.687500, loss bulk: 79368.554688, loss surf: 5.132870\n",
            "updating the parameters\n",
            "Train Epoch: 533, Loss: 79351.929688, loss bulk: 79346.781250, loss surf: 5.147126\n",
            "updating the parameters\n",
            "Train Epoch: 534, Loss: 79330.250000, loss bulk: 79325.085938, loss surf: 5.161393\n",
            "updating the parameters\n",
            "Train Epoch: 535, Loss: 79308.593750, loss bulk: 79303.421875, loss surf: 5.175673\n",
            "updating the parameters\n",
            "Train Epoch: 536, Loss: 79286.921875, loss bulk: 79281.734375, loss surf: 5.189964\n",
            "updating the parameters\n",
            "Train Epoch: 537, Loss: 79265.312500, loss bulk: 79260.109375, loss surf: 5.204268\n",
            "updating the parameters\n",
            "Train Epoch: 538, Loss: 79243.742188, loss bulk: 79238.523438, loss surf: 5.218584\n",
            "updating the parameters\n",
            "Train Epoch: 539, Loss: 79222.179688, loss bulk: 79216.945312, loss surf: 5.232911\n",
            "updating the parameters\n",
            "Train Epoch: 540, Loss: 79200.632812, loss bulk: 79195.382812, loss surf: 5.247249\n",
            "updating the parameters\n",
            "Train Epoch: 541, Loss: 79179.171875, loss bulk: 79173.914062, loss surf: 5.261596\n",
            "updating the parameters\n",
            "Train Epoch: 542, Loss: 79157.695312, loss bulk: 79152.421875, loss surf: 5.275951\n",
            "updating the parameters\n",
            "Train Epoch: 543, Loss: 79136.257812, loss bulk: 79130.968750, loss surf: 5.290316\n",
            "updating the parameters\n",
            "Train Epoch: 544, Loss: 79114.843750, loss bulk: 79109.539062, loss surf: 5.304688\n"
          ]
        }
      ],
      "source": [
        "tic()\n",
        "local_min = 10**308\n",
        "for j in range(epochs):\n",
        "    loss_bulk = torch.zeros(1)\n",
        "    loss_surf = torch.zeros(1)\n",
        "\n",
        "    for point in bulk_set:\n",
        "        x = torch.tensor([point+ 0.5*h]).double()\n",
        "        loss_bulk += h*model.loss_function_bulk(x,h)\n",
        "\n",
        "    for point in surf_set:\n",
        "        x = torch.tensor([point]).double()\n",
        "        loss_surf += model.loss_function_surf(x)\n",
        "\n",
        "    loss_bulk_record.append(loss_bulk.data[0])\n",
        "    loss_surf_record.append(loss_surf.data[0])\n",
        "\n",
        "    loss = loss_bulk + loss_surf\n",
        "    print('Train Epoch: {}, Loss: {:.6f}, loss bulk: {:.6f}, loss surf: {:.6f}'.format(j, loss.item(), loss_bulk.item(), loss_surf.item()))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    exp_lr_scheduler(optimizer, j)\n",
        "\n",
        "    if loss.item() < local_min:\n",
        "        print('updating the parameters')\n",
        "        local_min = loss.item()\n",
        "        torch.save(model.state_dict(),'./diff_sigmoid')\n",
        "\n",
        "    optimizer.step()\n",
        "toc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO1QBPgI3EVs"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('./diff_sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GXq0toj3EVt"
      },
      "outputs": [],
      "source": [
        "mu_err_h1 = torch.zeros(1)\n",
        "sigma_err_h1 = torch.zeros(1)\n",
        "bdd_err = torch.zeros(1)\n",
        "mu_err_l2 = torch.zeros(1)\n",
        "sigma_err_l2 = torch.zeros(1)\n",
        "G_relative = torch.zeros(1)\n",
        "mu_err_semi = torch.zeros(1)\n",
        "\n",
        "for point in test_set:\n",
        "\n",
        "    x = torch.tensor([point+ 0.5*dx])\n",
        "    mu, sigma = model(x)\n",
        "    mu_grad, sigma_grad = model.net_grad(x,dx)\n",
        "\n",
        "    # esitmate H1 norm error\n",
        "    mu_diff = (mu - u_exact(x))**2  + (mu_grad + sigma_exact(x))**2\n",
        "\n",
        "    if x.item() < 0.5:\n",
        "        mu_diff_simi = (mu_grad + sigma_exact(x))**2\n",
        "    else:\n",
        "        mu_diff_simi = (mu_grad + sigma_exact(x)/k)**2\n",
        "\n",
        "    sigma_diff = (sigma - sigma_exact(x))**2 + (sigma_grad - f(x))**2\n",
        "\n",
        "    mu_err_h1 += dx*mu_diff\n",
        "    sigma_err_h1 += dx*sigma_diff\n",
        "\n",
        "    # estimate L2 norm error\n",
        "    mu_err_l2 += dx*(mu - u_exact(x))**2\n",
        "    sigma_err_l2 += dx*(sigma - sigma_exact(x))**2\n",
        "\n",
        "    # estimate H1 semi norm  error\n",
        "    mu_err_semi += dx*mu_diff_simi\n",
        "    sigma_err_semi = sigma_err_h1 - sigma_err_l2\n",
        "\n",
        "H1_err = mu_err_h1 + sigma_err_h1\n",
        "H1_err_relative = ((H1_err)**(1/2)) / ((u_h1 + sigma_h1)**(1/2))\n",
        "L2_err = mu_err_l2 + sigma_err_l2\n",
        "L2_err_relative = ((L2_err)**(1/2)) / ((u_l2 + sigma_l2)**(1/2))\n",
        "mu_err_h1_relative = (mu_err_h1/u_h1)**(1/2)\n",
        "mu_err_l2_relative = (mu_err_l2/u_l2)**(1/2)\n",
        "mu_err_semi_relative = (mu_err_semi/(sigma_l2))**(1/2)\n",
        "sigma_err_h1_relative = (sigma_err_h1/sigma_h1)**(1/2)\n",
        "sigma_err_l2_relative = (sigma_err_l2/sigma_l2)**(1/2)\n",
        "sigma_err_semi_relative = (sigma_err_semi/(sigma_h1 - sigma_l2))**(1/2)\n",
        "bdd_err += abs(mu - g(x))\n",
        "\n",
        "G_rel = (local_min)**(1/2)/((u_h1 + sigma_h1)**(1/2))\n",
        "\n",
        "print('u: L2_rel: {:.6f}, H1_semi_rel: {:.6f}'.format( mu_err_l2_relative.item(), mu_err_semi_relative.item()))\n",
        "print('sigma: L2_rel: {:.6f}\\n'.format(sigma_err_l2_relative.item()))\n",
        "print('G_rel: {:.6f}\\n'.format(G_rel.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gJ0Q3ay3EVt"
      },
      "outputs": [],
      "source": [
        "points = test_set\n",
        "yt = np.zeros_like(points)\n",
        "y_diff = np.zeros_like(points)\n",
        "ymu = np.zeros_like(points)\n",
        "ysig = np.zeros_like(points)\n",
        "for i in range(len(points)):\n",
        "    yt[i] = u_exact(points[i])\n",
        "    y_diff[i] =  sigma_exact(points[i])\n",
        "    ymu[i], ysig[i] = model(torch.tensor([points[i]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyFV0aWG3EVu"
      },
      "outputs": [],
      "source": [
        "plt.plot(points, yt, color = 'b', label = 'u_true')\n",
        "plt.plot(points, ymu, color = 'r', label = 'u_approximation')\n",
        "plt.legend()\n",
        "plt.savefig(f'{folder}/u_plot_mesh_{int(1/h)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn-y_gyV3EVu"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "plt.plot(points, y_diff, color = 'b', label = 'sigma_true')\n",
        "plt.plot(points, ysig, color = 'r', label = 'sigma_approximation')\n",
        "plt.legend()\n",
        "plt.savefig(f'{folder}/sig_plot_mesh_{int(1/h)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "52SDhEG-3EVv"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "num = np.arange(1, len(loss_bulk_record)+1, 1)\n",
        "plt.plot(num, loss_bulk_record)\n",
        "plt.plot(num, loss_surf_record)\n",
        "plt.plot(num, np.add(loss_bulk_record , loss_surf_record))\n",
        "plt.savefig(f'{folder}/loss_plot_mesh_{int(1/h)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DRAZpGk43EVw"
      },
      "outputs": [],
      "source": [
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sxFLFOvG3EVw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}